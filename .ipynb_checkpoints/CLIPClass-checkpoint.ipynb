{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/javiimo/ImageClassificationAssignment/blob/main/CLIPClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIg1G6AD6fyQ",
    "outputId": "ec7ad4fe-ae62-4ba5-fac5-d737547a332b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy\n",
      "  Using cached ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.2)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.13)\n",
      "Using cached ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
      "Installing collected packages: ftfy\n",
      "Successfully installed ftfy-6.2.0\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-jtvx5ae1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-jtvx5ae1\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (6.2.0)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.0.0.post200)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.15.2a0+072ec57)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369499 sha256=09c22bd1c3e52fe5800e48036112490a8211f960b88cae0621689ad81814f83a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_j2q6k45/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git\n",
    "! pip install datasets transformers\n",
    "\n",
    "# Used for CLIP:\n",
    "import clip\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "#Used for testing:\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIPModel Class Method Explanations\n",
    "\n",
    "## Constructor\n",
    "- **`__init__(self, model_name='ViT-B/32', device=None)`**\n",
    "  - Initializes the model with a specified model name and device. Defaults to using CUDA if available. It sets up the optimizer and default parameters for the model. And casts it to float 32 to use ADAM. Also sets a `changeseed` to be able to have different random generations in a loop for Entropy Boosting.\n",
    "\n",
    "## Optimizer Methods\n",
    "- **`use_ADAM(self)`**\n",
    "  - Changes the optimizer of the model to ADAM.\n",
    "- **`use_SGD(self)`**\n",
    "  - Switches the optimizer to SGD.\n",
    "- **`convert_model_parameters_to_float32(self, model)`**\n",
    "    - This is for ADAM to work. Converts all model parameters to float32 data type.\n",
    "- **`grad_descent_step(self, loss)`**\n",
    "    - Performs a single gradient descent step using the computed loss, updating the model parameters accordingly. This is what modifies the clip model stored in the object.\n",
    "\n",
    "## Gradient Handling\n",
    "- **`require_CLIP_gradients(self, state=True)`**\n",
    "  - Enables or disables the calculation of gradients for model parameters based on the specified state.\n",
    "\n",
    "## Data Handling\n",
    "- **`load_data(self)`**\n",
    "  - This is just to have some data easy to access to test the methods on some image. Loads the CIFAR100 dataset which is useful for testing or validating the model performance on image data.\n",
    " \n",
    "## Tokenization of labels\n",
    "- **`tokenize_labels(self, classes)`**\n",
    "    - Converts a list of class labels into text tokens using CLIP's tokenizer and encodes them into text features. This is used for matching text descriptions with images. For now this is just used with a photo of a {class} and then stored as an attribute so that we can reuse it for every method.\n",
    " \n",
    "## Transformations\n",
    "- **`augment_image(self, image, num_augmentations=100, transformations=None)`**\n",
    "  - Applies specified transformations to an image (or a random transformation if none given) to generate multiple augmented versions.\n",
    "\n",
    "## Feature and Probability Computations\n",
    "- **`cos_sim(self, image_features, text_features)`**\n",
    "  - Calculates the cosine similarity between image and text features.\n",
    "- **`logits(self, image_features, text_features)`**\n",
    "  - Computes the logits by applying a scale factor (temperature already learned by CLIP) to the cosine similarities.\n",
    "- **`class_probabilities(self, image_features, text_features)`**\n",
    "  - Converts logits to class probabilities using the softmax function.\n",
    "\n",
    "## Entropy and Loss Calculations\n",
    "- **`compute_entropy(self, x)`**\n",
    "  - Computes the Shannon entropy of a probability distribution, measuring uncertainty.\n",
    "- **`marginal_entropy(self, logits)`**\n",
    "    - Calculates the marginal entropy of the logits, which quantifies the uncertainty or spread of the predicted probabilities across different classes.\n",
    "- **`entropy_loss_MEMO(self, batch_features, text_features=None)`**\n",
    "  - Calculates the entropy loss for the MEMO strategy using marginal entropy.\n",
    "- **`entropy_loss_TPT(self, batch_features, text_features=None)`**\n",
    "    - Calculates and returns the entropy loss for TPT.\n",
    "\n",
    "## Predictions with the CLIP model out of the box\n",
    "- **`forward(self, image)`**\n",
    "    - Processes an image through the model, encoding it into image features, normalizing these features, and then computing the class probabilities.\n",
    "- **`predict(self, image)`**\n",
    "    - Completes the prediction process including preprocessing the image, computing probabilities, and determining the most probable class along with its entropy. This is what you can use to compute CLIP out of the box predictions.\n",
    "\n",
    "## Methods used to get predictions\n",
    "- **`MEMO(self, image, num_augmentations=100)`**\n",
    "  - Implements MEMO strategy by optimizing model parameters to minimize entropy across predictions.\n",
    "- **`TPT(self, image, num_augmentations=100)`**\n",
    "    - Applies the TPT strategy, focusing on refining the model's ability to predict with high certainty by averaging probabilities across \"good\" augmentations.\n",
    "- **`entropyboosting(self, image, num_augmentations=100, num_candidates=10, top_aug_num=5)`**\n",
    "  - Uses \"Entropy Boosting\" to enhance model confidence by iterating over augmentation cycles to stabilize and improve prediction certainty.\n",
    "\n",
    "## Extra for TPT\n",
    "- **`confidence_selection(self, probs_matrix, percentile=0.8)`**\n",
    "    - Selects predictions with entropy below a specified percentile, filtering out less confident (high entropy) predictions to focus on more certain outcomes.\n",
    "\n",
    "# Extra for Entropy Boosting\n",
    "- **`pick_candidates(self, tensor, classifier, top_num)`**\n",
    "    - Selects top candidates based on a classifier score.\n",
    "- **`expand_tensor(self, tensor, top_indices, n)`**\n",
    "    - Expands a smaller tensor into a larger one based on specified indices.\n",
    "- **`generate_augmentations_similarities(self, image, image_features, txt_candidates, num_augmentations, top_aug_num)`**\n",
    "    - Generates and evaluates the similarity of augmented images to text candidates, aiding in the identification of most relevant image features.\n",
    "- **`boost_augmentations(self, image, image_features, prob, num_augmentations, num_candidates, top_aug_num)`**\n",
    "    - Boosts the confidence in model predictions by blending probabilities from original and augmented images, focusing on candidates with high initial probabilities.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to split the CLass:\n",
    "We have a parent class CLIP and 3 children (for now) which are MEMO, TPT and EB (Entropy Boost). Somehow, those children should be able to interact and use the same CLIP object )(access its attributes, methods as if they where the same object) without creating a copy of it. Not sure how to do it, but ideally it would work like one single class but splitting the code accross multiple classes so that we can structure the code better.\n",
    "## Attributes:\n",
    "- device (CLIP)\n",
    "- model (CLIP)\n",
    "- preprocess (CLIP)\n",
    "- optimizer (CLIP)\n",
    "- text_features (CLIP)\n",
    "- requiring_grads (CLIP)\n",
    "- logit_scale (CLIP)\n",
    "- changeseed \n",
    "\n",
    "## Methods:\n",
    "- __init__ (CLIP has exactly the one written)\n",
    "- use_ADAM (CLIP)\n",
    "- use_SGD (CLIP)\n",
    "- require_CLIP_gradients (CLIP)\n",
    "- convert_model_parameters_to_float32 (CLIP)\n",
    "- load_data (this one does not matter, won't be in the final version)\n",
    "- tokenize_labels (CLIP)\n",
    "- augment_image (CLIP)\n",
    "- cos_sim (CLIP)\n",
    "- logits (CLIP)\n",
    "- class_probabilities (CLIP)\n",
    "- marginal_entropy (CLIP)\n",
    "- compute_entropy (CLIP)\n",
    "- confidence_selection (TPT)\n",
    "- entropy_loss_MEMO (MEMO)\n",
    "- entropy_loss_TPT (TPT)\n",
    "- forward (CLIP)\n",
    "- predict (CLIP)\n",
    "- grad_descent_step (CLIP)\n",
    "- MEMO (MEMO)\n",
    "- TPT (TPT)\n",
    "- pick_candidates (EB)\n",
    "- expand_tensor (EB)\n",
    "- generate_augmentations_similarities (EB)\n",
    "- boost_augmentations (EB)\n",
    "- entropyboosting (EB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XGJSO0FnKTp",
    "outputId": "5f97a486-48f4-48ca-e713-83c576ebcd03"
   },
   "outputs": [],
   "source": [
    "class CLIPModel:\n",
    "\n",
    "    def __init__(self, model_name='ViT-B/32', device=None):\n",
    "        self.device = device if device else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model, self.preprocess = clip.load(model_name, self.device)\n",
    "        self.model = self.convert_model_parameters_to_float32(self.model)\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
    "        self.text_features = None\n",
    "        self.requiring_grads = None\n",
    "        self.logit_scale = self.model.logit_scale #temperature parameter learned by CLIP\n",
    "        self.changeseed = 0 #This is to be able to do diverse random transforms but replicable\n",
    "    \n",
    "    def use_ADAM(self):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-5)\n",
    "\n",
    "    def use_SGD(self):\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
    "\n",
    "    def require_CLIP_gradients(self, state = True):\n",
    "        if self.requiring_grads is None or state != self.requiring_grads: #don't change if the state is already OK\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = state\n",
    "            self.requiring_grads = state\n",
    "\n",
    "    def convert_model_parameters_to_float32(self, model):\n",
    "        for param in model.parameters():\n",
    "            param.data = param.data.to(torch.float32)\n",
    "        return model\n",
    "\n",
    "    def load_data(self):\n",
    "        cifar100 = torchvision.datasets.CIFAR100(root='./data', download=True, train=False)\n",
    "        return cifar100\n",
    "\n",
    "    #This are heuristic labels\n",
    "    def tokenize_labels(self, classes):\n",
    "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classes]).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            self.text_features = self.model.encode_text(text_inputs)\n",
    "            self.text_features /= self.text_features.norm(dim=-1, p=2, keepdim=True)\n",
    "\n",
    "    def augment_image(self, image, num_augmentations=100, transformations=None):\n",
    "        if transformations==None:\n",
    "            torch.manual_seed(33)#Set a seed for reproducibility of the random augmentations\n",
    "            augmentations = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=30),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "                transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
    "            ])\n",
    "        augmented_images = [self.preprocess(image).unsqueeze(0).to(self.device)] #Add the original image to the batch of augmentations\n",
    "        for _ in range(num_augmentations):\n",
    "            augmented_images.append(self.preprocess(augmentations(image)).unsqueeze(0).to(self.device))\n",
    "        batch = torch.vstack(augmented_images)\n",
    "        return batch #(num_augumentations + 1, 3, 224, 224)\n",
    "\n",
    "    def cos_sim(self, image_features, text_features):\n",
    "        return  image_features @ text_features.T\n",
    "\n",
    "    def logits(self, image_features, text_features):\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        return logit_scale * self.cos_sim(image_features, text_features)\n",
    "\n",
    "    def class_probabilities(self, image_features, text_features):\n",
    "        #Compute cosine similarities\n",
    "        return  self.logits(image_features, text_features).softmax(dim=-1)\n",
    "\n",
    "    def marginal_entropy(self, logits):\n",
    "        z = logits - logits.logsumexp(dim = -1, keepdim=True) # compute z_ij\n",
    "        marginal_logp = z.logsumexp(dim=0) - np.log(z.shape[0])   # compute marginal log probabilities\n",
    "\n",
    "        min_real = torch.finfo(marginal_logp.dtype).min # for numerical stability,\n",
    "        # the smallest representable number given the dtype of logits.\n",
    "        avg_logits = torch.clamp(marginal_logp, min = min_real)  # put a threshold to avoid underflow\n",
    "\n",
    "        return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
    "\n",
    "    def compute_entropy(self, x): #Shanon entropy in bits\n",
    "        #This computes the Shanon entropy\n",
    "        log_x = torch.log2(x.clamp_min(1e-20))\n",
    "        entropy = -torch.sum(x * log_x)\n",
    "        return entropy\n",
    "\n",
    "    def confidence_selection(self, probs_matrix, percentile=0.8):\n",
    "        # Compute entropies for each row in the probability matrix\n",
    "        entropies = torch.tensor([self.compute_entropy(row) for row in probs_matrix])\n",
    "\n",
    "        # Find the threshold for the desired percentile\n",
    "        threshold = torch.quantile(entropies, percentile, interpolation = 'linear')\n",
    "\n",
    "        # Create a boolean mask where entropies below the threshold are selected\n",
    "        boolean_mask = entropies < threshold\n",
    "\n",
    "        # Assuming similarities is intended to be probs_matrix, return filtered matrix\n",
    "        return probs_matrix[boolean_mask]\n",
    "\n",
    "    def entropy_loss_MEMO(self, batch_features, text_features = None):\n",
    "        if text_features is None:\n",
    "            text_features = self.text_features\n",
    "        #Logits (unnormalized probabilities)\n",
    "        logits = self.logits(batch_features, text_features)\n",
    "        # Compute the entropy of every text caption accross all augmentations\n",
    "        marginal_entropy = self.marginal_entropy(logits)\n",
    "        return marginal_entropy\n",
    "\n",
    "    def entropy_loss_TPT(self, batch_features, text_features = None):\n",
    "        if text_features is None:\n",
    "            text_features = self.text_features\n",
    "        probs_matrix = self.class_probabilities(batch_features, text_features)\n",
    "        # Confidence selection for the augmented views:\n",
    "        probs_matrix = self.confidence_selection(probs_matrix)\n",
    "        # Average the caption probabilities across all augmentations\n",
    "        avg_probs = torch.tensor([row.mean() for row in probs_matrix.T])\n",
    "        # Compute the entropy of the averaged probability distribution\n",
    "        return self.compute_entropy(avg_probs), avg_probs\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.model.encode_image(image)\n",
    "        norms = image_features.norm(dim=-1, p=2,  keepdim=True)\n",
    "        if (norms == 0).any():\n",
    "            print(\"Zero norm found in image features\")\n",
    "        image_features = image_features / norms.clamp_min(1e-10)\n",
    "        return self.class_probabilities(image_features, self.text_features)\n",
    "\n",
    "    def predict(self, image):\n",
    "        self.model.eval()\n",
    "        image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            probs = self.forward(image)\n",
    "\n",
    "        prediction = torch.argmax(probs).item()\n",
    "        entropy = float(self.compute_entropy(probs))\n",
    "        return prediction, probs.squeeze(), entropy\n",
    "\n",
    "    def grad_descent_step(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def MEMO(self, image, num_augmentations=100):\n",
    "        # Save original parameters\n",
    "        original_params = {name: param.clone() for name, param in self.model.named_parameters()}\n",
    "\n",
    "        # Require gradients to update the CLIP parameters\n",
    "        self.require_CLIP_gradients(state = True)\n",
    "\n",
    "        try:\n",
    "            self.model.train()\n",
    "            batch = self.augment_image(image, num_augmentations)\n",
    "            batch_features = self.model.encode_image(batch)\n",
    "            norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
    "            if (norms == 0).any():\n",
    "                print(\"Zero norm found in image features\")\n",
    "            batch_features = batch_features / norms.clamp_min(1e-10)\n",
    "            loss = self.entropy_loss_MEMO(batch_features)\n",
    "            self.grad_descent_step(loss)\n",
    "\n",
    "            if any(torch.isnan(param).any() for param in self.model.parameters()):\n",
    "                print(\"nan values detected in model parameters after updating\")\n",
    "            # Predict using the updated model\n",
    "            prediction, probs, entropy = self.predict(image)\n",
    "        finally:\n",
    "            # Restore original parameters\n",
    "            with torch.no_grad():\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    param.copy_(original_params[name])\n",
    "        return prediction, probs.squeeze(), entropy\n",
    "\n",
    "    def TPT(self, image, num_augmentations=100):\n",
    "        self.model.eval()\n",
    "        self.require_CLIP_gradients(False)\n",
    "        batch = self.augment_image(image, num_augmentations)\n",
    "        batch_features = self.model.encode_image(batch)\n",
    "        norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
    "        if (norms == 0).any():\n",
    "            print(\"Zero norm found in image features\")\n",
    "        batch_features = batch_features / norms.clamp_min(1e-10)\n",
    "\n",
    "        entropy, avg_probs = self.entropy_loss_TPT(batch_features)\n",
    "        prediction = torch.argmax(avg_probs).item()\n",
    "        return prediction, avg_probs.squeeze(), float(entropy)\n",
    "\n",
    "    #Implementing the Entropy Boost stuff:\n",
    "    def pick_candidates(self, tensor, classifier, top_num):\n",
    "        # to select a subset of \"candidates\" from a given tensor based on scores provided by a classifier\n",
    "        _, top_indices = torch.topk(classifier, top_num)\n",
    "        candidates = torch.squeeze(tensor[top_indices])\n",
    "        \n",
    "        return candidates, top_indices\n",
    "\n",
    "    def expand_tensor(self, tensor, top_indices, n):\n",
    "    \n",
    "        exp_tensor = torch.zeros(n).to(self.device)\n",
    "        for i in range(top_indices.shape[0]): exp_tensor[top_indices[i]] = tensor[i]\n",
    "    \n",
    "        return exp_tensor\n",
    "\n",
    "    def generate_augmentations_similarities(self, image, image_features, txt_candidates, num_augmentations, top_aug_num):\n",
    "        # augment\n",
    "        torch.manual_seed(33+self.changeseed)\n",
    "        augmentations = transforms.Compose([\n",
    "                                transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
    "                                ])\n",
    "        batch = self.augment_image(image, num_augmentations)\n",
    "        batch_features = self.model.encode_image(batch)\n",
    "        norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
    "        if (norms == 0).any():\n",
    "            print(\"Zero norm found in image features\")\n",
    "        batch_features = batch_features / norms.clamp_min(1e-10)\n",
    "\n",
    "    \n",
    "        # pick the ones closest to the original image\n",
    "        probs_matrix = self.class_probabilities(image_features, batch_features).squeeze()\n",
    "        candidates_features, top_indices_aug = self.pick_candidates(batch_features, probs_matrix, top_num = top_aug_num)\n",
    "\n",
    "        candidates_probs_matrix = self.class_probabilities(candidates_features, txt_candidates).squeeze()\n",
    "    \n",
    "        del batch # avoid Cuda to run out of memory?\n",
    "        return torch.mean(candidates_probs_matrix, dim=0)\n",
    "    \n",
    "    def boost_augmentations(self, image, image_features, prob, num_augmentations, num_candidates, top_aug_num):\n",
    "    \n",
    "        _, top_indices = torch.topk(prob, num_candidates)\n",
    "        text_candidates = self.text_features[top_indices]\n",
    "    \n",
    "        n = prob.shape[0]\n",
    "\n",
    "        #Candidates probabilities in the original image\n",
    "        candidates_prob_og = self.class_probabilities(image_features, text_candidates).squeeze()\n",
    "        candidates_prob_og = self.expand_tensor(candidates_prob_og, top_indices, n)\n",
    "\n",
    "        #Candidates averaged probability in the augmentations\n",
    "        candidates_avg_prob = self.generate_augmentations_similarities(image, image_features, text_candidates, num_augmentations, top_aug_num)\n",
    "        candidates_avg_prob = self.expand_tensor(candidates_avg_prob, top_indices, n)\n",
    "    \n",
    "        return candidates_prob_og, candidates_avg_prob\n",
    "\n",
    "    def entropyboosting(self, image, num_augmentations = 100, num_candidates = 10, top_aug_num = 5):\n",
    "        self.model.eval()\n",
    "        self.require_CLIP_gradients(False)\n",
    "        if num_candidates <= 1:\n",
    "            print(\"If num_candidates=0 this is just CLIP\")\n",
    "            return\n",
    "        image_prepro = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        image_features = self.model.encode_image(image_prepro)\n",
    "        norms = image_features.norm(dim=-1, p=2,  keepdim=True)\n",
    "        image_features = image_features / norms.clamp_min(1e-10)\n",
    "\n",
    "        #Compute out of the box CLIP probability distribution and prediction\n",
    "        clip_probs = self.class_probabilities(image_features, self.text_features).squeeze()\n",
    "        clip_prediction = torch.argmax(clip_probs).item()\n",
    "\n",
    "        phi = 2 / (1 + np.sqrt(5)) # Aura section - math fetish but it seems to work so...\n",
    "        \n",
    "        # Defining loop variables\n",
    "        aug_prob = None #Should this be an input of the method??\n",
    "        output_prob = clip_probs.clone() #Is this necessary??\n",
    "        max_iter = 4\n",
    "        iter = 0\n",
    "        while iter < max_iter:\n",
    "            self.changeseed = self.changeseed + 1 #To make different random transformations at every iter but replicable\n",
    "            candidates_prob_og, candidates_avg_prob = self.boost_augmentations(image, image_features, output_prob, num_augmentations, num_candidates, top_aug_num)\n",
    "            \n",
    "            clip_probs = phi * clip_probs + (1 - phi) * candidates_prob_og #Why are we doing this to the clip probs?\n",
    "            \n",
    "            if aug_prob == None: aug_prob = candidates_avg_prob\n",
    "            else:                aug_prob = 0.5 * aug_prob + 0.5 * candidates_avg_prob\n",
    "            \n",
    "            output_prob = 0.6 * clip_probs + 0.4 * aug_prob # give a little more weight to the probability of the image.\n",
    "            EB_prediction = torch.argmax(output_prob).item()\n",
    "            \n",
    "            # if clip_prediction != EB_prediction then it changed its mind: the class with highest probability has changed - better do further checks.\n",
    "            if clip_prediction == EB_prediction: break\n",
    "\n",
    "            iter += 1\n",
    "        self.changeseed = 0\n",
    "        entropy = self.compute_entropy(output_prob)\n",
    "        return EB_prediction, output_prob.squeeze(), float(entropy)\n",
    "\n",
    "\n",
    "# Preparing the class for usage\n",
    "clip_model = CLIPModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yYC2YqhknDk"
   },
   "source": [
    "1 IMAGE TRIES ON CLIP, MEMO, TPT and EB :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B52IQTi6OyfW",
    "outputId": "d8e52605-ea10-48f4-e56e-776e5a2b1c21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading CIFAR100 for one image tries_\n",
    "cifar100 = clip_model.load_data()\n",
    "clip_model.tokenize_labels(cifar100.classes)\n",
    "image, class_id = cifar100[3637]\n",
    "len(cifar100.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "2.208749294281006\n",
      "tensor([4.4409e-04, 2.0257e-03, 1.7850e-03, 6.7053e-05, 6.8302e-05, 1.2799e-04,\n",
      "        2.1407e-04, 8.0405e-03, 3.6831e-05, 1.3982e-04, 3.8572e-04, 3.0443e-04,\n",
      "        1.2713e-05, 3.9793e-05, 7.2395e-04, 5.2543e-05, 6.3892e-04, 1.7170e-05,\n",
      "        1.5631e-03, 3.5367e-04, 1.0214e-04, 4.6957e-04, 1.1729e-04, 1.9360e-05,\n",
      "        2.2551e-04, 6.2725e-05, 2.7089e-03, 4.3508e-02, 2.1955e-04, 1.7460e-03,\n",
      "        4.6290e-06, 1.6038e-04, 1.2927e-02, 3.8837e-05, 5.5391e-06, 2.1283e-04,\n",
      "        3.4115e-04, 4.3326e-05, 3.8308e-05, 9.3167e-05, 4.4401e-04, 2.5278e-03,\n",
      "        1.5629e-02, 3.2312e-04, 4.5569e-02, 5.2967e-05, 1.3752e-04, 4.2287e-05,\n",
      "        3.7950e-05, 1.7290e-04, 2.4531e-03, 1.0843e-03, 2.3604e-04, 2.8239e-04,\n",
      "        4.5561e-05, 5.3201e-05, 3.1678e-04, 5.2178e-05, 9.5018e-05, 5.7305e-04,\n",
      "        3.0544e-04, 6.4293e-04, 2.1116e-04, 1.2552e-04, 1.8514e-04, 2.5836e-04,\n",
      "        1.7865e-04, 2.3634e-04, 2.7643e-06, 4.6131e-06, 6.4169e-05, 2.6901e-05,\n",
      "        4.1291e-04, 1.9324e-05, 4.9106e-04, 9.3546e-05, 1.0680e-05, 1.4651e-02,\n",
      "        5.8744e-01, 1.0724e-03, 3.9147e-05, 2.2297e-05, 2.9193e-04, 3.2322e-02,\n",
      "        7.7673e-05, 2.4090e-03, 5.8943e-04, 9.7330e-05, 8.7041e-04, 4.2988e-05,\n",
      "        6.9078e-05, 1.6199e-04, 5.6775e-05, 1.9426e-01, 2.7203e-05, 6.5766e-06,\n",
      "        4.0043e-05, 2.9092e-05, 1.3313e-04, 1.1812e-02], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Try entropyboosting\n",
    "prediction, probs, entropy = clip_model.entropyboosting(image)\n",
    "print(prediction)\n",
    "print(entropy)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "087FNIiqpDUb",
    "outputId": "157fb454-8aae-497a-b5c6-aecf4b3d4569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "2.3266210556030273\n",
      "tensor([[1.1976e-03, 5.4628e-03, 4.8136e-03, 1.8082e-04, 1.8419e-04, 3.4514e-04,\n",
      "         5.7728e-04, 8.2105e-03, 9.9322e-05, 3.7705e-04, 1.0402e-03, 8.2096e-04,\n",
      "         3.4284e-05, 1.0731e-04, 1.9523e-03, 1.4169e-04, 1.7230e-03, 4.6304e-05,\n",
      "         4.2152e-03, 9.5374e-04, 2.7545e-04, 1.2663e-03, 3.1629e-04, 5.2210e-05,\n",
      "         6.0814e-04, 1.6915e-04, 7.3052e-03, 1.7485e-02, 5.9205e-04, 4.7085e-03,\n",
      "         1.2483e-05, 4.3251e-04, 8.4594e-03, 1.0473e-04, 1.4937e-05, 5.7395e-04,\n",
      "         9.1999e-04, 1.1684e-04, 1.0331e-04, 2.5125e-04, 1.1974e-03, 6.8169e-03,\n",
      "         1.7164e-02, 8.7137e-04, 1.8750e-02, 1.4284e-04, 3.7087e-04, 1.1404e-04,\n",
      "         1.0234e-04, 4.6626e-04, 6.6154e-03, 2.9242e-03, 6.3653e-04, 7.6153e-04,\n",
      "         1.2287e-04, 1.4347e-04, 8.5428e-04, 1.4071e-04, 2.5624e-04, 1.5454e-03,\n",
      "         8.2368e-04, 1.7338e-03, 5.6945e-04, 3.3850e-04, 4.9928e-04, 6.9673e-04,\n",
      "         4.8177e-04, 6.3735e-04, 7.4547e-06, 1.2440e-05, 1.7305e-04, 7.2545e-05,\n",
      "         1.1135e-03, 5.2111e-05, 1.3243e-03, 2.5227e-04, 2.8802e-05, 1.5945e-02,\n",
      "         6.5313e-01, 2.8920e-03, 1.0557e-04, 6.0129e-05, 7.8726e-04, 3.8252e-02,\n",
      "         2.0946e-04, 6.4964e-03, 1.5895e-03, 2.6247e-04, 2.3473e-03, 1.1593e-04,\n",
      "         1.8628e-04, 4.3683e-04, 1.5311e-04, 1.2288e-01, 7.3359e-05, 1.7735e-05,\n",
      "         1.0799e-04, 7.8453e-05, 3.5900e-04, 8.4517e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Prediction using CLIP out of the box\n",
    "prediction1, probs1, entropy1 = clip_model.predict(image)\n",
    "print(prediction1)\n",
    "print(entropy1)\n",
    "print(probs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "_RheDQxnoeB7",
    "outputId": "7b03df7f-81bc-4555-c017-377d44e5b3c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "2.077364206314087\n",
      "tensor([[9.8620e-04, 4.7828e-03, 4.6798e-03, 1.5572e-04, 1.5197e-04, 3.0867e-04,\n",
      "         4.7689e-04, 6.2922e-03, 8.2343e-05, 3.0923e-04, 8.7553e-04, 8.1575e-04,\n",
      "         2.9356e-05, 8.9023e-05, 1.6763e-03, 1.1114e-04, 1.4614e-03, 3.6086e-05,\n",
      "         3.8958e-03, 8.0583e-04, 2.2591e-04, 1.1260e-03, 2.5036e-04, 4.4503e-05,\n",
      "         4.7512e-04, 1.3567e-04, 5.4395e-03, 1.5579e-02, 5.2545e-04, 4.0048e-03,\n",
      "         1.0843e-05, 3.6359e-04, 7.3783e-03, 9.3484e-05, 1.3365e-05, 5.7313e-04,\n",
      "         7.7305e-04, 1.0464e-04, 9.2250e-05, 1.8495e-04, 9.6642e-04, 6.5645e-03,\n",
      "         1.4847e-02, 7.2444e-04, 1.7678e-02, 1.1078e-04, 3.5591e-04, 1.0298e-04,\n",
      "         7.9667e-05, 3.7379e-04, 6.0870e-03, 2.3019e-03, 5.8701e-04, 6.6893e-04,\n",
      "         1.1238e-04, 1.3150e-04, 7.1440e-04, 1.1645e-04, 2.0226e-04, 1.4091e-03,\n",
      "         7.9140e-04, 1.5631e-03, 4.4642e-04, 2.7754e-04, 4.8233e-04, 6.4062e-04,\n",
      "         4.2063e-04, 5.6923e-04, 6.2323e-06, 9.5952e-06, 1.4697e-04, 6.2266e-05,\n",
      "         9.2542e-04, 4.5426e-05, 1.2508e-03, 2.6994e-04, 2.5094e-05, 1.3129e-02,\n",
      "         7.0722e-01, 2.4452e-03, 1.0118e-04, 5.2856e-05, 6.6586e-04, 3.6857e-02,\n",
      "         1.8823e-04, 4.7386e-03, 1.3757e-03, 2.2036e-04, 2.0671e-03, 8.7088e-05,\n",
      "         1.4390e-04, 3.7851e-04, 1.2681e-04, 9.3765e-02, 6.8134e-05, 1.4196e-05,\n",
      "         1.0045e-04, 6.6866e-05, 3.4894e-04, 8.3519e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Prediction using MEMO with SGD\n",
    "clip_model.use_SGD()\n",
    "prediction2, probs2, entropy2 = clip_model.MEMO(image, num_augmentations=10)\n",
    "print(prediction2)\n",
    "print(entropy2)\n",
    "print(probs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mlTDDcr_IRYf",
    "outputId": "8a58029d-da53-4ed3-96a2-5a039d91d2ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "0.3890056312084198\n",
      "tensor([[7.0291e-05, 1.2202e-03, 2.0966e-03, 2.8387e-05, 4.4688e-06, 2.8498e-05,\n",
      "         3.2856e-05, 6.2306e-05, 1.0148e-05, 4.9512e-05, 2.7819e-04, 5.3239e-04,\n",
      "         6.4909e-06, 6.8183e-06, 9.8106e-05, 1.4464e-06, 4.3893e-04, 2.5695e-06,\n",
      "         4.9707e-04, 5.9817e-05, 5.5518e-06, 6.2155e-05, 4.5031e-06, 1.0935e-05,\n",
      "         5.5434e-06, 4.1206e-06, 2.6603e-05, 4.4461e-04, 3.8518e-04, 2.5397e-04,\n",
      "         1.2974e-06, 1.2624e-05, 4.2113e-05, 4.5163e-05, 2.7994e-06, 4.2294e-04,\n",
      "         7.2896e-05, 4.4254e-05, 1.0838e-05, 1.8192e-06, 3.2608e-05, 2.0624e-03,\n",
      "         5.1893e-04, 3.0729e-05, 3.5040e-03, 1.8276e-06, 1.2709e-04, 2.3962e-05,\n",
      "         5.4501e-06, 1.4678e-05, 1.1937e-03, 3.1151e-05, 1.7511e-04, 5.6660e-05,\n",
      "         2.8462e-05, 1.3722e-05, 4.6307e-05, 2.8664e-06, 9.7663e-06, 8.9210e-04,\n",
      "         2.8619e-04, 3.3893e-04, 2.8193e-06, 5.9115e-06, 1.3063e-04, 8.1566e-05,\n",
      "         3.1865e-05, 1.4796e-05, 5.0939e-07, 1.6153e-06, 1.7927e-05, 1.1527e-05,\n",
      "         8.4458e-06, 4.9185e-06, 1.6388e-04, 6.4259e-04, 8.2300e-06, 1.8448e-04,\n",
      "         9.5956e-01, 1.1634e-04, 1.8333e-05, 6.7579e-06, 2.8254e-05, 1.6869e-02,\n",
      "         4.2036e-05, 1.1157e-04, 7.6165e-05, 1.6402e-05, 2.6695e-04, 5.7434e-06,\n",
      "         6.3032e-06, 7.7760e-05, 5.5851e-06, 1.5060e-04, 1.2123e-05, 7.9850e-07,\n",
      "         7.1923e-05, 1.1627e-05, 1.2605e-04, 4.3244e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Prediction using MEMO with ADAM\n",
    "clip_model.use_ADAM()\n",
    "clip_model.tokenize_labels(cifar100.classes) #You have to tokenize the labels again for the change in precision\n",
    "prediction3, probs3, entropy3 = clip_model.MEMO(image, num_augmentations=10)\n",
    "print(prediction3)\n",
    "print(entropy3)\n",
    "print(probs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nrxX4QrwsCbv",
    "outputId": "828b2345-5334-418b-b40b-444910986132"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "5.611426830291748\n",
      "tensor([0.0064, 0.0020, 0.0094, 0.0159, 0.0024, 0.0015, 0.0036, 0.0047, 0.0037,\n",
      "        0.0079, 0.0029, 0.0115, 0.0021, 0.0032, 0.0041, 0.0024, 0.0094, 0.0036,\n",
      "        0.0035, 0.0114, 0.0044, 0.0107, 0.0029, 0.0023, 0.0009, 0.0038, 0.0035,\n",
      "        0.0242, 0.0036, 0.0294, 0.0014, 0.0065, 0.0031, 0.0114, 0.0015, 0.0182,\n",
      "        0.0032, 0.0038, 0.0027, 0.0075, 0.0080, 0.0273, 0.0121, 0.0070, 0.0103,\n",
      "        0.0014, 0.0164, 0.0213, 0.0034, 0.0102, 0.0039, 0.0064, 0.0574, 0.0015,\n",
      "        0.0009, 0.0005, 0.0391, 0.0023, 0.0041, 0.1248, 0.0108, 0.0077, 0.0039,\n",
      "        0.0030, 0.0048, 0.0042, 0.0057, 0.0013, 0.0009, 0.0124, 0.0019, 0.0014,\n",
      "        0.0031, 0.0034, 0.0014, 0.0026, 0.0052, 0.0044, 0.1003, 0.0022, 0.0020,\n",
      "        0.0020, 0.0149, 0.0114, 0.0022, 0.0200, 0.0051, 0.0162, 0.0090, 0.0066,\n",
      "        0.0070, 0.0077, 0.0038, 0.0256, 0.0022, 0.0030, 0.0308, 0.0060, 0.0206,\n",
      "        0.0081])\n"
     ]
    }
   ],
   "source": [
    "# Prediction using TPT\n",
    "prediction4,prob_avg, entropy4 = clip_model.TPT(image, num_augmentations=10)\n",
    "print(prediction4)\n",
    "print(entropy4)\n",
    "print(prob_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkg1nIszau2y"
   },
   "source": [
    "TESTSSSS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_YTyv3q9V3Ky"
   },
   "outputs": [],
   "source": [
    "# Set the class names for imagenet-A\n",
    "def classnames_imagenetA():\n",
    "    # Define the path to the words file\n",
    "    file_path = '/content/drive/MyDrive/Petaloso Project/Code/Datasets/words_imageneta.txt'\n",
    "\n",
    "    # Initialize an empty list to store the class names\n",
    "    class_names = []\n",
    "\n",
    "    # Open and read the file line by line\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Split each line into wnid and class name, and strip to remove any leading/trailing whitespace\n",
    "            parts = line.strip().split(' ', 1)\n",
    "            if len(parts) > 1:\n",
    "                # Append only the class name (second part) to the list\n",
    "                class_names.append(parts[1])\n",
    "    return class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGUDzNCle_E3"
   },
   "outputs": [],
   "source": [
    "#Implementing subsets\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "def create_stratified_subset(dataset, num_samples_per_class=5):\n",
    "    # Fix the random seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Determine class indices\n",
    "    targets = np.array([s[1] for s in dataset.samples])\n",
    "    classes, class_indices = np.unique(targets, return_inverse=True)\n",
    "\n",
    "    # Select samples from each class\n",
    "    indices = []\n",
    "    for c in classes:\n",
    "        class_idx = np.where(class_indices == c)[0]\n",
    "        if len(class_idx) >= num_samples_per_class:\n",
    "            selected_indices = np.random.choice(class_idx, num_samples_per_class, replace=False)\n",
    "            indices.extend(selected_indices)\n",
    "        else:\n",
    "            # If a class has fewer than the desired number, take all\n",
    "            indices.extend(class_idx)\n",
    "\n",
    "    # Create subset\n",
    "    subset = Subset(dataset, indices)\n",
    "    return subset\n",
    "\n",
    "subset = create_stratified_subset(imageneta, num_samples_per_class=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UPSff5cJs0J",
    "outputId": "e0a08239-6974-4e8d-960b-bf2aab10de19"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [00:18<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 31.26%\n",
      "Average entropy across all predictions: 7.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "imageneta = datasets.ImageFolder(root='/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenet-a')\n",
    "\n",
    "def testing(dataset, model,method='CLIP', batch_size=32, num_aug=100):\n",
    "    model.tokenize_labels(classnames_imagenetA())\n",
    "\n",
    "    def custom_collate_fn(batch):\n",
    "        # Extract images and labels from the batch\n",
    "        images = [item[0] for item in batch]  # PIL images\n",
    "        labels = [item[1] for item in batch]  # Corresponding labels\n",
    "        return images, labels\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "    # Initialize lists to store results\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    entropies = []\n",
    "    confidences = []\n",
    "\n",
    "    # Evaluation loop\n",
    "    for images, labels in tqdm(dataloader):\n",
    "        for image, label in zip(images, labels):\n",
    "            try:\n",
    "                # We choose how the test time predictions are made\n",
    "                if method == 'CLIP':\n",
    "                    prediction, probs, entropy = model.predict(image)\n",
    "                elif method == 'MEMO':\n",
    "                    prediction, probs, entropy = model.MEMO(image, num_augmentations=num_aug)\n",
    "                elif method == 'MEMO_CONF':\n",
    "                    prediction, probs, entropy = model.MEMO(image, num_augmentations=num_aug, conf_sel = False)\n",
    "                elif method == 'TPT':\n",
    "                    prediction, probs, entropy = model.TPT(image, num_augmentations=num_aug)\n",
    "                else:\n",
    "                    print('Enter a valid method for testing.')\n",
    "\n",
    "                if int(prediction) == int(label):\n",
    "                    correct_predictions += 1\n",
    "                total_predictions += 1\n",
    "                entropies.append(entropy)\n",
    "                confidences.append(torch.max(probs).item())\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # Post evaluation statistics or processing\n",
    "    accuracy = (correct_predictions / total_predictions) * 100\n",
    "    average_entropy = sum(entropies) / len(entropies)\n",
    "    average_confidence = sum(confidences) / len(confidences)\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Average entropy across all predictions: {average_entropy:.2f}')\n",
    "\n",
    "\n",
    "testing(subset,clip_model, method = 'CLIP', batch_size=35)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS in AWS Bucket DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: OfficeHomeDataset_10072016/\n",
      "  Subfolder: OfficeHomeDataset_10072016/Art/\n",
      "  Subfolder: OfficeHomeDataset_10072016/Clipart/\n",
      "  Subfolder: OfficeHomeDataset_10072016/Product/\n",
      "  ... and more subfolders\n",
      "\n",
      "Folder: imagenet-a/\n",
      "  Subfolder: imagenet-a/n01498041/\n",
      "  Subfolder: imagenet-a/n01531178/\n",
      "  Subfolder: imagenet-a/n01534433/\n",
      "  ... and more subfolders\n",
      "\n",
      "Folder: imagenetv2-matched-frequency-format-val/\n",
      "  Subfolder: imagenetv2-matched-frequency-format-val/0/\n",
      "  Subfolder: imagenetv2-matched-frequency-format-val/1/\n",
      "  Subfolder: imagenetv2-matched-frequency-format-val/10/\n",
      "  ... and more subfolders\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This is to list the s3_bucket folders and the first 3 subfolders\n",
    "def list_datasets_with_subfolders(s3_bucket, s3_region):\n",
    "    s3_client = boto3.client(\"s3\", region_name=s3_region, verify=True)\n",
    "    response = s3_client.list_objects_v2(Bucket=s3_bucket, Delimiter='/')\n",
    "    \n",
    "    # Use a set to keep track of unique folder names\n",
    "    folders = set()\n",
    "    \n",
    "    # Append the common prefixes which are the folder names\n",
    "    for prefix in response.get('CommonPrefixes', []):\n",
    "        folders.add(prefix.get('Prefix'))\n",
    "    \n",
    "    # Print out the available folders and their first 3 subfolders in the bucket\n",
    "    for folder in sorted(folders):\n",
    "        print(f\"Folder: {folder}\")\n",
    "        \n",
    "        # List objects within the folder to find subfolders\n",
    "        sub_response = s3_client.list_objects_v2(Bucket=s3_bucket, Prefix=folder, Delimiter='/')\n",
    "        subfolders = []\n",
    "        \n",
    "        for sub_prefix in sub_response.get('CommonPrefixes', []):\n",
    "            subfolders.append(sub_prefix.get('Prefix'))\n",
    "        \n",
    "        # Print the first 3 subfolders\n",
    "        for subfolder in sorted(subfolders)[:3]:\n",
    "            print(f\"  Subfolder: {subfolder}\")\n",
    "        if len(subfolders) > 3:\n",
    "            print(\"  ... and more subfolders\")\n",
    "        print()\n",
    "\n",
    "# Example usage\n",
    "s3_bucket = \"deeplearning2024-datasets\"\n",
    "s3_region = \"eu-west-1\"\n",
    "list_datasets_with_subfolders(s3_bucket, s3_region)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a custom ImageFolder class\n",
    "class S3ImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None, imageneta=True):\n",
    "        self.s3_bucket = \"deeplearning2024-datasets\"\n",
    "        self.s3_region = \"eu-west-1\"\n",
    "        self.s3_client = boto3.client(\"s3\", region_name=self.s3_region, verify=True)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get list of objects in the bucket\n",
    "        response = self.s3_client.list_objects_v2(Bucket=self.s3_bucket, Prefix=root)\n",
    "        objects = response.get(\"Contents\", [])\n",
    "        while response.get(\"NextContinuationToken\"):\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Prefix=root,\n",
    "                ContinuationToken=response[\"NextContinuationToken\"]\n",
    "            )\n",
    "            objects.extend(response.get(\"Contents\", []))\n",
    "\n",
    "        # Iterate and keep valid files only\n",
    "        self.instances = []\n",
    "        for ds_idx, item in enumerate(objects):\n",
    "            key = item[\"Key\"]\n",
    "            path = Path(key)\n",
    "            \n",
    "            # Check if file is valid\n",
    "            if path.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\"):\n",
    "                continue\n",
    "\n",
    "            # Get label\n",
    "            label = path.parent.name\n",
    "\n",
    "            # Keep track of valid instances\n",
    "            self.instances.append((label, key))\n",
    "\n",
    "        # Sort classes in alphabetical order (as in ImageFolder)\n",
    "        self.classes = sorted(set(label for label, _ in self.instances))\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        # Get the labels that will be used for the CLIP text_features\n",
    "        if imageneta:\n",
    "            self.class_to_name = self.map_classnames_imagenetA()\n",
    "            self.class_names = [self.class_to_name[cls] for cls in self.classes if cls in self.class_to_name]\n",
    "\n",
    "    \n",
    "    def map_classnames_imagenetA(self):\n",
    "        # Define the path to the words file\n",
    "        file_path = 'List_ClassNames_imageneta.txt'\n",
    "    \n",
    "        # Initialize an empty dictionary to store the class names with their corresponding IDs\n",
    "        class_dict = {}\n",
    "    \n",
    "        # Open and read the file line by line\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                # Split each line into WordNet ID and class name, and strip to remove any leading/trailing whitespace\n",
    "                parts = line.strip().split(' ', 1)\n",
    "                if len(parts) > 1:\n",
    "                    # Use the WordNet ID as the key and the class name as the value in the dictionary\n",
    "                    class_dict[parts[0]] = parts[1]\n",
    "    \n",
    "        return class_dict\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label, key = self.instances[idx]\n",
    "        response = self.s3_client.get_object(Bucket=self.s3_bucket, Key=key)\n",
    "        img = Image.open(response['Body'])\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, self.class_to_idx[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the imagenet-a\n",
    "s3_rootA = 'imagenet-a'\n",
    "imageneta = S3ImageFolder(root=s3_rootA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stingray', 'goldfinch', 'junco', 'American robin', 'jay', 'bald eagle', 'vulture', 'newt', 'American bullfrog', 'box turtle', 'green iguana', 'agama', 'chameleon', 'American alligator', 'garter snake', 'harvestman', 'scorpion', 'tarantula', 'centipede', 'sulphur-crested cockatoo', 'lorikeet', 'hummingbird', 'toucan', 'duck', 'goose', 'koala', 'jellyfish', 'sea anemone', 'flatworm', 'snail', 'crayfish', 'hermit crab', 'flamingo', 'great egret', 'oystercatcher', 'pelican', 'sea lion', 'Chihuahua', 'Golden Retriever', 'Rottweiler', 'German Shepherd Dog', 'pug', 'red fox', 'Persian cat', 'lynx', 'lion', 'American black bear', 'mongoose', 'ladybug', 'rhinoceros beetle', 'weevil', 'fly', 'bee', 'ant', 'grasshopper', 'stick insect', 'cockroach', 'mantis', 'leafhopper', 'dragonfly', 'monarch butterfly', 'small white', 'gossamer-winged butterfly', 'starfish', 'cottontail rabbit', 'porcupine', 'fox squirrel', 'marmot', 'bison', 'skunk', 'armadillo', 'baboon', 'white-headed capuchin', 'African bush elephant', 'pufferfish', 'academic gown', 'accordion', 'acoustic guitar', 'airliner', 'ambulance', 'apron', 'balance beam', 'balloon', 'banjo', 'barn', 'wheelbarrow', 'basketball', 'lighthouse', 'beaker', 'bikini', 'bow', 'bow tie', 'breastplate', 'broom', 'candle', 'canoe', 'castle', 'cello', 'chain', 'chest', 'Christmas stocking', 'cowboy boot', 'cradle', 'rotary dial telephone', 'digital clock', 'doormat', 'drumstick', 'dumbbell', 'envelope', 'feather boa', 'flagpole', 'forklift', 'fountain', 'garbage truck', 'goblet', 'go-kart', 'golf cart', 'grand piano', 'hair dryer', 'clothes iron', \"jack-o'-lantern\", 'jeep', 'kimono', 'lighter', 'limousine', 'manhole cover', 'maraca', 'marimba', 'mask', 'mitten', 'mosque', 'nail', 'obelisk', 'ocarina', 'organ', 'parachute', 'parking meter', 'piggy bank', 'billiard table', 'hockey puck', 'quill', 'racket', 'reel', 'revolver', 'rocking chair', 'rugby ball', 'salt shaker', 'sandal', 'saxophone', 'school bus', 'schooner', 'sewing machine', 'shovel', 'sleeping bag', 'snowmobile', 'snowplow', 'soap dispenser', 'spatula', 'spider web', 'steam locomotive', 'stethoscope', 'couch', 'submarine', 'sundial', 'suspension bridge', 'syringe', 'tank', 'teddy bear', 'toaster', 'torch', 'tricycle', 'umbrella', 'unicycle', 'viaduct', 'volleyball', 'washing machine', 'water tower', 'wine bottle', 'shipwreck', 'guacamole', 'pretzel', 'cheeseburger', 'hot dog', 'broccoli', 'cucumber', 'bell pepper', 'mushroom', 'lemon', 'banana', 'custard apple', 'pomegranate', 'carbonara', 'bubble', 'cliff', 'volcano', 'baseball player', 'rapeseed', \"yellow lady's slipper\", 'corn', 'acorn']\n"
     ]
    }
   ],
   "source": [
    "#Print the class names\n",
    "print(imageneta.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the imagenet-v2\n",
    "s3_rootV2 = 'imagenetv2-matched-frequency-format-val'\n",
    "imagenetv2 = S3ImageFolder(root=s3_rootV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the testing function\n",
    "def testing(dataset, model, method='CLIP', batch_size=32, num_aug=100):\n",
    "    model.tokenize_labels(dataset.classes)\n",
    "\n",
    "    def custom_collate_fn(batch):\n",
    "        # Extract images and labels from the batch\n",
    "        images = [item[0] for item in batch]  # PIL images\n",
    "        labels = [item[1] for item in batch]  # Corresponding labels\n",
    "        return images, labels\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn, num_workers=4)\n",
    "\n",
    "    # Initialize lists to store results\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    entropies = []\n",
    "    confidences = []\n",
    "\n",
    "    # Evaluation loop\n",
    "    for images, labels in tqdm(dataloader):\n",
    "        for image, label in zip(images, labels):\n",
    "            try:\n",
    "                # We choose how the test time predictions are made\n",
    "                if method == 'CLIP':\n",
    "                    prediction, probs, entropy = model.predict(image)\n",
    "                elif method == 'MEMO':\n",
    "                    prediction, probs, entropy = model.MEMO(image, num_augmentations=num_aug)\n",
    "                elif method == 'TPT':\n",
    "                    prediction, probs, entropy = model.TPT(image, num_augmentations=num_aug)\n",
    "                elif method == 'EB':\n",
    "                    prediction, probs, entropy = model.entropyboosting(image, num_augmentations=num_aug)\n",
    "                else:\n",
    "                    print('Enter a valid method for testing.')\n",
    "\n",
    "                if int(prediction) == int(label):\n",
    "                    correct_predictions += 1\n",
    "                total_predictions += 1\n",
    "                entropies.append(entropy)\n",
    "                confidences.append(torch.max(probs).item())\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # Post evaluation statistics or processing\n",
    "    accuracy = (correct_predictions / total_predictions) * 100\n",
    "    average_entropy = sum(entropies) / len(entropies)\n",
    "    average_confidence = sum(confidences) / len(confidences)\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Average entropy across all predictions: {average_entropy:.2f}')\n",
    "    print(f'Average confidence across all predictions: {average_confidence:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with the whole Imagenet-A\n",
    "testing(imageneta, clip_model, method='CLIP', batch_size=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBSETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compatible with subsets\n",
    "class S3ImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.s3_bucket = \"deeplearning2024-datasets\"\n",
    "        self.s3_region = \"eu-west-1\"\n",
    "        self.s3_client = boto3.client(\"s3\", region_name=self.s3_region, verify=True)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get list of objects in the bucket\n",
    "        response = self.s3_client.list_objects_v2(Bucket=self.s3_bucket, Prefix=root)\n",
    "        objects = response.get(\"Contents\", [])\n",
    "        while response.get(\"NextContinuationToken\"):\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Prefix=root,\n",
    "                ContinuationToken=response[\"NextContinuationToken\"]\n",
    "            )\n",
    "            objects.extend(response.get(\"Contents\", []))\n",
    "\n",
    "        # Iterate and keep valid files only\n",
    "        self.instances = []\n",
    "        self.samples = []  # Add samples attribute\n",
    "        for item in objects:\n",
    "            key = item[\"Key\"]\n",
    "            path = Path(key)\n",
    "            \n",
    "            # Check if file is valid\n",
    "            if path.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\"):\n",
    "                continue\n",
    "\n",
    "            # Get label\n",
    "            label = path.parent.name\n",
    "\n",
    "            # Keep track of valid instances\n",
    "            self.instances.append((label, key))\n",
    "            self.samples.append((key, label))  # Add to samples\n",
    "\n",
    "        # Sort classes in alphabetical order (as in ImageFolder)\n",
    "        self.classes = sorted(set(label for label, _ in self.instances)) #This contains keys of the form n0349583...\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        # Get the labels that will be used for the CLIP text_features\n",
    "        if imageneta:\n",
    "            self.class_to_name = self.map_classnames_imagenetA()\n",
    "            self.class_names = [self.class_to_name[cls] for cls in self.classes if cls in self.class_to_name]\n",
    "\n",
    "    \n",
    "    def map_classnames_imagenetA(self):\n",
    "        # Define the path to the words file\n",
    "        file_path = 'List_ClassNames_imageneta.txt'\n",
    "    \n",
    "        # Initialize an empty dictionary to store the class names with their corresponding IDs\n",
    "        class_dict = {}\n",
    "    \n",
    "        # Open and read the file line by line\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                # Split each line into WordNet ID and class name, and strip to remove any leading/trailing whitespace\n",
    "                parts = line.strip().split(' ', 1)\n",
    "                if len(parts) > 1:\n",
    "                    # Use the WordNet ID as the key and the class name as the value in the dictionary\n",
    "                    class_dict[parts[0]] = parts[1]\n",
    "    \n",
    "        return class_dict\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, key = self.instances[idx]\n",
    "        response = self.s3_client.get_object(Bucket=self.s3_bucket, Key=key)\n",
    "        img = Image.open(response['Body'])\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        filename = os.path.basename(key)  # Extract the filename from the key\n",
    "\n",
    "        return img, self.class_to_idx[label], filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the imagenet-a\n",
    "s3_rootA = 'imagenet-a'\n",
    "imageneta = S3ImageFolder(root=s3_rootA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create a subset of Imagenet-A with 5 images per class\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Create a subclass of Subset to include additional attributes\n",
    "class SubsetWithAttributes(Subset):\n",
    "    def __init__(self, dataset, indices):\n",
    "        super().__init__(dataset, indices)\n",
    "        self.classes = dataset.classes\n",
    "        self.class_to_idx = dataset.class_to_idx\n",
    "\n",
    "#Function for getting the subset\n",
    "def create_stratified_subset(dataset, num_samples_per_class=5):\n",
    "    # Fix the random seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Determine class indices\n",
    "    targets = np.array([dataset.class_to_idx[s[1]] for s in dataset.samples])\n",
    "    classes, class_indices = np.unique(targets, return_inverse=True)\n",
    "\n",
    "    # Select samples from each class\n",
    "    indices = []\n",
    "    for c in classes:\n",
    "        class_idx = np.where(class_indices == c)[0]\n",
    "        if len(class_idx) >= num_samples_per_class:\n",
    "            selected_indices = np.random.choice(class_idx, num_samples_per_class, replace=False)\n",
    "            indices.extend(selected_indices)\n",
    "        else:\n",
    "            # If a class has fewer than the desired number, take all\n",
    "            indices.extend(class_idx)\n",
    "\n",
    "    # Create subset\n",
    "    subset = SubsetWithAttributes(dataset, indices)\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29/29 [04:29<00:00,  9.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.40%\n",
      "Average entropy across all predictions: 7.53\n",
      "Average confidence across all predictions: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage of a subset of Imagenet-A with 5 samples per class\n",
    "subset = create_stratified_subset(imageneta, num_samples_per_class=5)\n",
    "testing(subset, clip_model, method='CLIP', batch_size=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SERIALIZING OBJECTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an object to store the result of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class PredictionResult:\n",
    "    image_id: str  # Name of the image file\n",
    "    top_indices: List[int]\n",
    "    top_probabilities: List[float]\n",
    "    true_label_index: int\n",
    "    true_label_probability: float\n",
    "    prediction_entropy: float\n",
    "\n",
    "    def __init__(self, name_img: str, prediction: int, probs: torch.Tensor, entropy: float, num_top: int = 5, decimal_places: int = 6):\n",
    "        self.image_id = name_img\n",
    "        # Get the top 'num_top' indices and probabilities\n",
    "        top_probs, top_indices = torch.topk(probs, num_top)\n",
    "        self.top_indices = top_indices.tolist()\n",
    "        self.top_probabilities = [round(prob.item(), decimal_places) for prob in top_probs]\n",
    "        # True label information\n",
    "        self.true_label_index = prediction\n",
    "        self.true_label_probability = round(probs[prediction].item(), decimal_places)\n",
    "        # Prediction entropy\n",
    "        self.prediction_entropy = entropy\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 'example_image.jpg', 'top_indices': [2, 1, 3], 'top_probabilities': [0.4, 0.3, 0.2], 'true_label_index': 2, 'true_label_probability': 0.4, 'prediction_entropy': 0.97}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "name_img = 'example_image.jpg'\n",
    "prediction = 2\n",
    "probs = torch.tensor([0.1, 0.3, 0.4, 0.2])  # example tensor\n",
    "entropy = 0.97\n",
    "num_top = 3\n",
    "\n",
    "pred_result = PredictionResult(name_img, prediction, probs, entropy, num_top)\n",
    "print(pred_result.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a function to serialize a list of prediction objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serializing a list of Prediction objects\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "# Function to serialize a list of PredictionResult objects\n",
    "def serialize_results(prediction_results: List[PredictionResult], file_path: str):\n",
    "    # Convert each PredictionResult object to a dictionary\n",
    "    results_as_dicts = [result.to_dict() for result in prediction_results]\n",
    "    \n",
    "    # Serialize the list of dictionaries to a JSON string\n",
    "    json_string = json.dumps(results_as_dicts, indent=4)\n",
    "    \n",
    "    # Write the JSON string to a file\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "prediction_results = [\n",
    "    PredictionResult('example_image1.jpg', 2, torch.tensor([0.1, 0.3, 0.4, 0.2]), 0.97, 3),\n",
    "    PredictionResult('example_image2.jpg', 0, torch.tensor([0.6, 0.2, 0.1, 0.1]), 0.85, 3)\n",
    "]\n",
    "\n",
    "serialize_results(prediction_results, 'prediction_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a function to load the serialized list of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to deserialize a list of PredictionResult objects from a file\n",
    "def deserialize_results(file_path: str) -> List[PredictionResult]:\n",
    "    # Read the JSON string from the file\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_string = f.read()\n",
    "    \n",
    "    # Deserialize the JSON string to a list of dictionaries\n",
    "    results_as_dicts = json.loads(json_string)\n",
    "    \n",
    "    # Convert each dictionary back to a PredictionResult object\n",
    "    prediction_results = [\n",
    "        PredictionResult(\n",
    "            name_img=d['image_id'],\n",
    "            prediction=d['true_label_index'],\n",
    "            probs=torch.tensor(d['top_probabilities'] + [0]*(len(d['top_indices']) - len(d['top_probabilities']))),  # dummy tensor for conversion\n",
    "            entropy=d['prediction_entropy'],\n",
    "            num_top=len(d['top_indices'])\n",
    "        ) for d in results_as_dicts\n",
    "    ]\n",
    "    \n",
    "    return prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 'example_image1.jpg', 'top_indices': [0, 1, 2], 'top_probabilities': [0.4, 0.3, 0.2], 'true_label_index': 2, 'true_label_probability': 0.2, 'prediction_entropy': 0.97}\n",
      "{'image_id': 'example_image2.jpg', 'top_indices': [0, 1, 2], 'top_probabilities': [0.6, 0.2, 0.1], 'true_label_index': 0, 'true_label_probability': 0.6, 'prediction_entropy': 0.85}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "deserialized_results = deserialize_results('prediction_results.json')\n",
    "for result in deserialized_results:\n",
    "    print(result.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing on testing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefine for getting the imagefile name as well\n",
    "class S3ImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None, imageneta=True):\n",
    "        self.s3_bucket = \"deeplearning2024-datasets\"\n",
    "        self.s3_region = \"eu-west-1\"\n",
    "        self.s3_client = boto3.client(\"s3\", region_name=self.s3_region, verify=True)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get list of objects in the bucket\n",
    "        response = self.s3_client.list_objects_v2(Bucket=self.s3_bucket, Prefix=root)\n",
    "        objects = response.get(\"Contents\", [])\n",
    "        while response.get(\"NextContinuationToken\"):\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Prefix=root,\n",
    "                ContinuationToken=response[\"NextContinuationToken\"]\n",
    "            )\n",
    "            objects.extend(response.get(\"Contents\", []))\n",
    "\n",
    "        # Iterate and keep valid files only\n",
    "        self.instances = []\n",
    "        self.samples = []  # Add samples attribute\n",
    "        for item in objects:\n",
    "            key = item[\"Key\"]\n",
    "            path = Path(key)\n",
    "            \n",
    "            # Check if file is valid\n",
    "            if path.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\"):\n",
    "                continue\n",
    "\n",
    "            # Get label\n",
    "            label = path.parent.name\n",
    "\n",
    "            # Keep track of valid instances\n",
    "            self.instances.append((label, key))\n",
    "            self.samples.append((key, label))  # Add to samples\n",
    "\n",
    "        # Sort classes in alphabetical order (as in ImageFolder)\n",
    "        self.classes = sorted(set(label for label, _ in self.instances)) #This contains keys of the form n0349583...\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        # Get the labels that will be used for the CLIP text_features\n",
    "        if imageneta:\n",
    "            self.class_to_name = self.map_classnames_imagenetA()\n",
    "            self.class_names = [self.class_to_name[cls] for cls in self.classes if cls in self.class_to_name]\n",
    "\n",
    "    \n",
    "    def map_classnames_imagenetA(self):\n",
    "        # Define the path to the words file\n",
    "        file_path = 'List_ClassNames_imageneta.txt'\n",
    "    \n",
    "        # Initialize an empty dictionary to store the class names with their corresponding IDs\n",
    "        class_dict = {}\n",
    "    \n",
    "        # Open and read the file line by line\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                # Split each line into WordNet ID and class name, and strip to remove any leading/trailing whitespace\n",
    "                parts = line.strip().split(' ', 1)\n",
    "                if len(parts) > 1:\n",
    "                    # Use the WordNet ID as the key and the class name as the value in the dictionary\n",
    "                    class_dict[parts[0]] = parts[1]\n",
    "    \n",
    "        return class_dict\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, key = self.instances[idx]\n",
    "        response = self.s3_client.get_object(Bucket=self.s3_bucket, Key=key)\n",
    "        img = Image.open(response['Body'])\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        filename = os.path.basename(key)  # Extract the filename from the key\n",
    "\n",
    "        return img, self.class_to_idx[label], filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the testing function\n",
    "def testing(dataset, model, method='CLIP', batch_size=32, num_aug=20):\n",
    "    model.tokenize_labels(dataset.class_names)\n",
    "\n",
    "    def custom_collate_fn(batch):\n",
    "        images = [item[0] for item in batch]  # PIL images\n",
    "        labels = [item[1] for item in batch]  # Corresponding labels\n",
    "        filenames = [item[2] for item in batch]    # Corresponding keys/IDs\n",
    "        return images, labels, filenames\n",
    "    \n",
    "    #We use num_workers=4 but this only works well when using the gpu, not the cpu.\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn, num_workers=4)\n",
    "\n",
    "    # Initialize lists to store results\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    entropies = []\n",
    "    confidences = []\n",
    "    results = []\n",
    "\n",
    "    # Evaluation loop\n",
    "    for images, labels, filenames in tqdm(dataloader):  \n",
    "        for image, label, filename in zip(images, labels, filenames): \n",
    "            try:\n",
    "                # We choose how the test time predictions are made\n",
    "                if method == 'CLIP':\n",
    "                    prediction, probs, entropy = model.predict(image)\n",
    "                    resultsfilename = 'CLIP_Base_results.json'\n",
    "                elif method == 'MEMO':\n",
    "                    prediction, probs, entropy = model.MEMO(image, num_augmentations=num_aug)\n",
    "                    resultsfilename = 'CLIP_MEMO_results.json'\n",
    "                elif method == 'TPT':\n",
    "                    prediction, probs, entropy = model.TPT(image, num_augmentations=num_aug)\n",
    "                    resultsfilename = 'CLIP_TPT_results.json'\n",
    "                elif method == 'EB':\n",
    "                    prediction, probs, entropy = model.entropyboosting(image, num_augmentations=num_aug)\n",
    "                    resultsfilename = 'CLIP_EB_results.json'\n",
    "                else:\n",
    "                    print('Enter a valid method for testing.')\n",
    "\n",
    "                #Store the results of each image:\n",
    "                results.append(PredictionResult(filename, prediction, probs, entropy))\n",
    "\n",
    "                #Some computations for getting average results at the end\n",
    "                if int(prediction) == int(label):\n",
    "                    correct_predictions += 1\n",
    "                total_predictions += 1\n",
    "                entropies.append(entropy)\n",
    "                confidences.append(torch.max(probs).item())\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "    #Serializing results for each image:\n",
    "    serialize_results(results, resultsfilename)\n",
    "    \n",
    "    # Post evaluation general statistics\n",
    "    accuracy = (correct_predictions / total_predictions) * 100\n",
    "    average_entropy = sum(entropies) / len(entropies)\n",
    "    average_confidence = sum(confidences) / len(confidences)\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Average entropy across all predictions: {average_entropy:.2f}')\n",
    "    print(f'Average confidence across all predictions: {average_confidence:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the imagenet-a\n",
    "s3_rootA = 'imagenet-a'\n",
    "imageneta = S3ImageFolder(root=s3_rootA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stingray', 'goldfinch', 'junco', 'American robin', 'jay', 'bald eagle', 'vulture', 'newt', 'American bullfrog', 'box turtle', 'green iguana', 'agama', 'chameleon', 'American alligator', 'garter snake', 'harvestman', 'scorpion', 'tarantula', 'centipede', 'sulphur-crested cockatoo', 'lorikeet', 'hummingbird', 'toucan', 'duck', 'goose', 'koala', 'jellyfish', 'sea anemone', 'flatworm', 'snail', 'crayfish', 'hermit crab', 'flamingo', 'great egret', 'oystercatcher', 'pelican', 'sea lion', 'Chihuahua', 'Golden Retriever', 'Rottweiler', 'German Shepherd Dog', 'pug', 'red fox', 'Persian cat', 'lynx', 'lion', 'American black bear', 'mongoose', 'ladybug', 'rhinoceros beetle', 'weevil', 'fly', 'bee', 'ant', 'grasshopper', 'stick insect', 'cockroach', 'mantis', 'leafhopper', 'dragonfly', 'monarch butterfly', 'small white', 'gossamer-winged butterfly', 'starfish', 'cottontail rabbit', 'porcupine', 'fox squirrel', 'marmot', 'bison', 'skunk', 'armadillo', 'baboon', 'white-headed capuchin', 'African bush elephant', 'pufferfish', 'academic gown', 'accordion', 'acoustic guitar', 'airliner', 'ambulance', 'apron', 'balance beam', 'balloon', 'banjo', 'barn', 'wheelbarrow', 'basketball', 'lighthouse', 'beaker', 'bikini', 'bow', 'bow tie', 'breastplate', 'broom', 'candle', 'canoe', 'castle', 'cello', 'chain', 'chest', 'Christmas stocking', 'cowboy boot', 'cradle', 'rotary dial telephone', 'digital clock', 'doormat', 'drumstick', 'dumbbell', 'envelope', 'feather boa', 'flagpole', 'forklift', 'fountain', 'garbage truck', 'goblet', 'go-kart', 'golf cart', 'grand piano', 'hair dryer', 'clothes iron', \"jack-o'-lantern\", 'jeep', 'kimono', 'lighter', 'limousine', 'manhole cover', 'maraca', 'marimba', 'mask', 'mitten', 'mosque', 'nail', 'obelisk', 'ocarina', 'organ', 'parachute', 'parking meter', 'piggy bank', 'billiard table', 'hockey puck', 'quill', 'racket', 'reel', 'revolver', 'rocking chair', 'rugby ball', 'salt shaker', 'sandal', 'saxophone', 'school bus', 'schooner', 'sewing machine', 'shovel', 'sleeping bag', 'snowmobile', 'snowplow', 'soap dispenser', 'spatula', 'spider web', 'steam locomotive', 'stethoscope', 'couch', 'submarine', 'sundial', 'suspension bridge', 'syringe', 'tank', 'teddy bear', 'toaster', 'torch', 'tricycle', 'umbrella', 'unicycle', 'viaduct', 'volleyball', 'washing machine', 'water tower', 'wine bottle', 'shipwreck', 'guacamole', 'pretzel', 'cheeseburger', 'hot dog', 'broccoli', 'cucumber', 'bell pepper', 'mushroom', 'lemon', 'banana', 'custard apple', 'pomegranate', 'carbonara', 'bubble', 'cliff', 'volcano', 'baseball player', 'rapeseed', \"yellow lady's slipper\", 'corn', 'acorn']\n"
     ]
    }
   ],
   "source": [
    "print(imageneta.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [01:47<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 29.85%\n",
      "Average entropy across all predictions: 3.16\n",
      "Average confidence across all predictions: 0.46\n"
     ]
    }
   ],
   "source": [
    "#subset = create_stratified_subset(imageneta, num_samples_per_class=1)\n",
    "testing(imageneta, clip_model, method='CLIP', batch_size=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 21/215 [08:26<1:22:28, 25.51s/it]"
     ]
    }
   ],
   "source": [
    "testing(imageneta, clip_model, method='EB', batch_size=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [1:16:49<00:00, 21.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.93%\n",
      "Average entropy across all predictions: 2.67\n",
      "Average confidence across all predictions: 0.53\n"
     ]
    }
   ],
   "source": [
    "testing(imageneta, clip_model, method='MEMO', batch_size=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing(imageneta, clip_model, method='TPT', batch_size=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other attempt (ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Tuple\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "@dataclass\n",
    "class PredictionResult:\n",
    "    image_id: str  # or use some other identifier\n",
    "    top_indices: List[int]\n",
    "    top_probabilities: List[float]\n",
    "    true_label_index: int\n",
    "    true_label_probability: float\n",
    "    prediction_entropy: float\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "def serialize_results(results: List[PredictionResult], file_path: str):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump([result.to_dict() for result in results], f, indent=4)\n",
    "\n",
    "def testing(dataset, model, method='CLIP', batch_size=32):\n",
    "    results = []  # List to store PredictionResult objects\n",
    "\n",
    "    def custom_collate_fn(batch):\n",
    "        images = [item[0] for item in batch]  # PIL images\n",
    "        labels = [item[1] for item in batch]  # Corresponding labels\n",
    "        return images, labels\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "    # Evaluation loop\n",
    "    for images, labels in tqdm(dataloader):\n",
    "        for image, label in zip(images, labels):\n",
    "            try:\n",
    "                if method == 'CLIP':\n",
    "                    prediction, probs, entropy = model.predict(image)  # Ensure model.predict returns probs as a tensor\n",
    "                    probs = probs.squeeze()\n",
    "                    # Ensure probs is a tensor and contains probabilities\n",
    "                    if not isinstance(probs, torch.Tensor) or probs.ndim != 1:\n",
    "                        print(probs.shape)\n",
    "                        raise ValueError(\"Probs must be a 1-dimensional tensor\")\n",
    "                    \n",
    "                    # Get top 5 predictions\n",
    "                    top_probs, top_indices = torch.topk(probs, 5)\n",
    "                    top_probs = top_probs.tolist()\n",
    "                    top_indices = top_indices.tolist()\n",
    "\n",
    "                    # Ensure the label is within the valid range\n",
    "                    if label >= len(probs):\n",
    "                        raise IndexError(f\"Label index {label} is out of bounds for probabilities of size {len(probs)}\")\n",
    "\n",
    "                    # True label probability\n",
    "                    true_label_prob = probs[label].item()\n",
    "\n",
    "                    # Create a PredictionResult instance and append to results\n",
    "                    result = PredictionResult(\n",
    "                        image_id=image.filename,  # Make sure images have a filename or another unique identifier\n",
    "                        top_indices=top_indices,\n",
    "                        top_probabilities=top_probs,\n",
    "                        true_label_index=label,\n",
    "                        true_label_probability=true_label_prob,\n",
    "                        prediction_entropy=entropy\n",
    "                    )\n",
    "                    results.append(result)\n",
    "                else:\n",
    "                    print('Enter a valid method for testing.')\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "    # Serialize results to a file after the loop\n",
    "    serialize_results(results, 'clip_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:56<00:00,  9.34s/it]\n"
     ]
    }
   ],
   "source": [
    "# Example usage of a subset of Imagenet-A with 5 samples per class\n",
    "subset = create_stratified_subset(imageneta, num_samples_per_class=1)\n",
    "testing(subset, clip_model, method='CLIP', batch_size=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
