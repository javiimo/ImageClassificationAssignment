{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javiimo/ImageClassificationAssignment/blob/main/CLIPClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIg1G6AD6fyQ",
        "outputId": "2eb9da39-0590-4c1b-bbb6-e4a145681d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m902.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.2.0\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-bx2p7_kk\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-bx2p7_kk\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.17.1+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369499 sha256=59d1b7abccc4dbd6baf843ab6a293d59eef06851833cbc2160a9be4c978c6d4a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fg3gnmja/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n",
            "Successfully installed clip-1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "! pip install datasets transformers\n",
        "\n",
        "\n",
        "import clip\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6WJzQYEtTxi",
        "outputId": "0322bbd5-d299-43c7-ed63-405241409065"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPModel:\n",
        "    def __init__(self, model_name='ViT-B/32', device=None):\n",
        "        self.device = device if device else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model, self.preprocess = clip.load(model_name, self.device)\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
        "        self.text_features = None\n",
        "        self.requiring_grads = None\n",
        "\n",
        "    def use_ADAM(self):\n",
        "        self.model = self.convert_model_parameters_to_float32(self.model)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-5) #Numerical inestability\n",
        "\n",
        "    def use_SGD(self):\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
        "    def require_CLIP_gradients(self, state = True):\n",
        "        if self.requiring_grads is None or state != self.requiring_grads: #don't change if the state is already OK\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = state\n",
        "            self.requiring_grads = state\n",
        "\n",
        "    def convert_model_parameters_to_float32(self, model):\n",
        "        for param in model.parameters():\n",
        "            param.data = param.data.to(torch.float32)\n",
        "        return model\n",
        "\n",
        "    def load_data(self):\n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', download=True, train=False)\n",
        "        return cifar100\n",
        "\n",
        "    #This are heuristic labels\n",
        "    def tokenize_labels(self, classes):\n",
        "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classes]).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            self.text_features = self.model.encode_text(text_inputs)\n",
        "            self.text_features /= self.text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    def augment_image(self, image, num_augmentations=100, transformations=None):\n",
        "        if transformations==None:\n",
        "            augmentations = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomVerticalFlip(p=0.5),\n",
        "                transforms.RandomRotation(degrees=30),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "                transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
        "            ])\n",
        "        augmented_images = [self.preprocess(image).unsqueeze(0).to(self.device)] #Add the original image to the batch of augmentations\n",
        "        for _ in range(num_augmentations):\n",
        "            augmented_images.append(self.preprocess(augmentations(image)).unsqueeze(0).to(self.device))\n",
        "        batch = torch.vstack(augmented_images)\n",
        "        return batch #(num_augumentations + 1, 3, 224, 224)\n",
        "\n",
        "    def marginal_entropy(self, logits):\n",
        "        z = logits - logits.logsumexp(dim = -1, keepdim=True) # compute z_ij\n",
        "        marginal_logp = z.logsumexp(dim=0) - np.log(z.shape[0])   # compute marginal log probabilities\n",
        "\n",
        "        min_real = torch.finfo(marginal_logp.dtype).min # for numerical stability,\n",
        "        # the smallest representable number given the dtype of logits.\n",
        "        avg_logits = torch.clamp(marginal_logp, min = min_real)  # put a threshold to avoid underflow\n",
        "\n",
        "        return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
        "\n",
        "    def compute_entropy(self, x): #Shanon entropy in bits\n",
        "        #This computes the Shanon entropy\n",
        "        log_x = torch.log2(x.clamp_min(1e-20))\n",
        "        entropy = -torch.sum(x * log_x)\n",
        "        return entropy\n",
        "\n",
        "    def class_probabilities(self, text_features, image_features):\n",
        "        #Compute cosine similarities\n",
        "        return  (100 * self.cos_sim(image_features, text_features)).softmax(dim=-1)\n",
        "\n",
        "    def cos_sim(self, image_features, text_features):\n",
        "        return  image_features @ text_features.T\n",
        "\n",
        "    def logits(self, text_features, image_features):\n",
        "        #Compute cosine similarities\n",
        "        return 100 * self.cos_sim(image_features, text_features)\n",
        "\n",
        "    def confidence_selection(self, probs_matrix, percentile=0.8):\n",
        "        # Compute entropies for each row in the probability matrix\n",
        "        entropies = torch.tensor([self.compute_entropy(row) for row in probs_matrix])\n",
        "\n",
        "        # Sort entropies and find the threshold for the desired percentile\n",
        "        sorted_entropies, _ = torch.sort(entropies, descending=True)\n",
        "        threshold = sorted_entropies[int(len(sorted_entropies) * percentile)]\n",
        "\n",
        "        # Create a boolean mask where entropies below the threshold are selected\n",
        "        boolean_mask = entropies < threshold\n",
        "\n",
        "        # Assuming similarities is intended to be probs_matrix, return filtered matrix\n",
        "        return probs_matrix[boolean_mask]\n",
        "\n",
        "\n",
        "    def entropy_loss_MEMO(self, batch_features, text_features = None):\n",
        "        if text_features is None:\n",
        "            text_features = self.text_features\n",
        "        #Logits (unnormalized probabilities)\n",
        "        logits = self.logits(text_features, batch_features)\n",
        "        # Compute the entropy of every text caption accross all augmentations\n",
        "        marginal_entropy = self.marginal_entropy(logits)\n",
        "        return marginal_entropy\n",
        "\n",
        "    def entropy_loss_TPT(self, batch_features, text_features = None):\n",
        "        if text_features is None:\n",
        "            text_features = self.text_features\n",
        "        probs_matrix = self.class_probabilities(text_features, batch_features)\n",
        "        # Confidence selection for the augmented views:\n",
        "        probs_matrix = self.confidence_selection(probs_matrix)\n",
        "        # Average the caption probabilities across all augmentations\n",
        "        avg_probs = torch.tensor([row.mean() for row in probs_matrix.T])\n",
        "        # Compute the entropy of the averaged probability distribution\n",
        "        return self.compute_entropy(avg_probs), avg_probs\n",
        "\n",
        "    def grad_descent_step(self, loss):\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def predict(self, image):\n",
        "        self.model.eval()\n",
        "        image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            image_features = self.model.encode_image(image)\n",
        "            norms = image_features.norm(dim=-1, keepdim=True)\n",
        "            if (norms == 0).any():\n",
        "                print(\"Zero norm found in image features\")\n",
        "            image_features = image_features / norms.clamp_min(1e-10)\n",
        "\n",
        "        probs = self.class_probabilities(self.text_features, image_features)\n",
        "        prediction = torch.argmax(probs).item()\n",
        "        entropy = float(self.compute_entropy(probs))\n",
        "        return prediction, probs, entropy\n",
        "\n",
        "    def MEMO(self, image, num_augmentations=100, conf_sel = False):\n",
        "        # Save original parameters\n",
        "        original_params = {name: param.clone() for name, param in self.model.named_parameters()}\n",
        "\n",
        "        # Require gradients to update the CLIP parameters\n",
        "        self.require_CLIP_gradients(state = True)\n",
        "        try:\n",
        "            self.model.train() #DO WE WANT BATCH NORMALIZ AND DROPOUT?\n",
        "            batch = self.augment_image(image, num_augmentations)\n",
        "            batch_features = self.model.encode_image(batch)\n",
        "            norms = batch_features.norm(dim=-1, keepdim=True)\n",
        "            if (norms == 0).any():\n",
        "                print(\"Zero norm found in image features\")\n",
        "            batch_features = batch_features / norms.clamp_min(1e-10)\n",
        "\n",
        "            loss = self.entropy_loss_MEMO(batch_features, conf_sel= conf_sel)\n",
        "            self.grad_descent_step(loss)\n",
        "\n",
        "            if any(torch.isnan(param).any() for param in self.model.parameters()):\n",
        "                print(\"nan values detected in model parameters after updating\")\n",
        "            # Predict using the updated model\n",
        "            prediction, probs, entropy = self.predict(image)\n",
        "        finally:\n",
        "            # Restore original parameters\n",
        "            with torch.no_grad():\n",
        "                for name, param in self.model.named_parameters():\n",
        "                    param.copy_(original_params[name])\n",
        "        return prediction, probs, entropy\n",
        "\n",
        "    def TPT(self, image, num_augmentations=100):\n",
        "        batch = self.augment_image(image, num_augmentations)\n",
        "        batch_features = self.model.encode_image(batch)\n",
        "        norms = batch_features.norm(dim=-1, keepdim=True)\n",
        "        if (norms == 0).any():\n",
        "            print(\"Zero norm found in image features\")\n",
        "        batch_features = batch_features / norms.clamp_min(1e-10)\n",
        "\n",
        "        entropy, avg_probs = self.entropy_loss_TPT(batch_features)\n",
        "        prediction = torch.argmax(avg_probs).item()\n",
        "        return prediction, avg_probs, float(entropy)\n",
        "\n",
        "    def train_CoOp(self):\n",
        "        #Prevent CLIP parameters from changing\n",
        "        self.require_CLIP_gradients(state=False)\n",
        "\n",
        "\n",
        "# Preparing the class for usage\n",
        "clip_model = CLIPModel()"
      ],
      "metadata": {
        "id": "6XGJSO0FnKTp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 IMAGE TRIES!!"
      ],
      "metadata": {
        "id": "4yYC2YqhknDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading CIFAR100 for one image tries_\n",
        "cifar100 = clip_model.load_data()\n",
        "clip_model.tokenize_labels(cifar100.classes)\n",
        "image, class_id = cifar100[3637]\n",
        "len(cifar100.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B52IQTi6OyfW",
        "outputId": "4684e18c-7845-4b5d-c7ec-58a47a299edd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction using CLIP out of the box\n",
        "prediction1, probs1, entropy1 = clip_model.predict(image)\n",
        "print(probs1, prediction1, entropy1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "087FNIiqpDUb",
        "outputId": "1ef1f635-0ee3-4363-f348-3542322faf49"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.1673e-03, 5.3139e-03, 4.8409e-03, 1.7631e-04, 1.7631e-04, 3.3450e-04,\n",
            "         5.6887e-04, 8.1024e-03, 9.5844e-05, 3.6168e-04, 1.0300e-03, 8.2779e-04,\n",
            "         3.3617e-05, 1.0526e-04, 1.9255e-03, 1.3947e-04, 1.6985e-03, 4.5300e-05,\n",
            "         4.1389e-03, 9.3794e-04, 2.6464e-04, 1.2236e-03, 3.0470e-04, 5.0485e-05,\n",
            "         6.1512e-04, 1.6308e-04, 7.1526e-03, 1.7151e-02, 5.8699e-04, 4.6196e-03,\n",
            "         1.2159e-05, 4.2295e-04, 8.2321e-03, 1.0365e-04, 1.4722e-05, 5.7793e-04,\n",
            "         9.2363e-04, 1.1379e-04, 1.0043e-04, 2.4092e-04, 1.1492e-03, 6.7177e-03,\n",
            "         1.6632e-02, 8.4114e-04, 1.8845e-02, 1.3947e-04, 3.6168e-04, 1.1033e-04,\n",
            "         9.8884e-05, 4.5013e-04, 6.5117e-03, 2.8896e-03, 6.2513e-04, 7.5388e-04,\n",
            "         1.2118e-04, 1.4162e-04, 8.2779e-04, 1.3733e-04, 2.4092e-04, 1.5230e-03,\n",
            "         8.2779e-04, 1.6985e-03, 5.6028e-04, 3.2926e-04, 4.9448e-04, 6.7568e-04,\n",
            "         4.7183e-04, 6.2513e-04, 7.1526e-06, 1.2159e-05, 1.7083e-04, 7.0095e-05,\n",
            "         1.0967e-03, 5.1320e-05, 1.2827e-03, 2.5249e-04, 2.7895e-05, 1.5625e-02,\n",
            "         6.5381e-01, 2.8458e-03, 1.0526e-04, 5.9068e-05, 7.7772e-04, 3.8666e-02,\n",
            "         2.0611e-04, 6.2141e-03, 1.5230e-03, 2.5654e-04, 2.2869e-03, 1.1033e-04,\n",
            "         1.8191e-04, 4.2295e-04, 1.5318e-04, 1.2482e-01, 7.2360e-05, 1.7166e-05,\n",
            "         1.0693e-04, 7.5817e-05, 3.5620e-04, 8.4915e-03]], device='cuda:0',\n",
            "       dtype=torch.float16) 78 2.310546875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction using MEMO with SGD\n",
        "clip_model.use_SGD()\n",
        "clip_model.tokenize_labels(cifar100.classes) #You have to tokenize the labels again for the change in precision\n",
        "prediction2, probs2, entropy2 = clip_model.MEMO(image, num_augmentations=200)\n",
        "print(prediction2)\n",
        "print(entropy2)\n",
        "print(probs2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RheDQxnoeB7",
        "outputId": "d1a5589d-6345-4db7-abf1-8941bae9111f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78\n",
            "2.3203125\n",
            "tensor([[1.1835e-03, 5.3864e-03, 4.7531e-03, 1.7869e-04, 1.7869e-04, 3.3903e-04,\n",
            "         5.5885e-04, 7.9651e-03, 9.5606e-05, 3.6669e-04, 1.0281e-03, 8.1348e-04,\n",
            "         3.3021e-05, 1.0502e-04, 1.9512e-03, 1.4138e-04, 1.6689e-03, 4.5180e-05,\n",
            "         4.1962e-03, 9.5081e-04, 2.6822e-04, 1.2398e-03, 3.0398e-04, 5.0366e-05,\n",
            "         6.0463e-04, 1.6522e-04, 7.1373e-03, 1.7670e-02, 5.8603e-04, 4.6806e-03,\n",
            "         1.2338e-05, 4.2868e-04, 8.7433e-03, 1.0341e-04, 1.4663e-05, 5.6791e-04,\n",
            "         8.9312e-04, 1.1355e-04, 1.0341e-04, 2.4045e-04, 1.1473e-03, 6.9160e-03,\n",
            "         1.6861e-02, 8.5211e-04, 1.9104e-02, 1.3912e-04, 3.6669e-04, 1.1355e-04,\n",
            "         9.8646e-05, 4.5633e-04, 6.4964e-03, 2.9297e-03, 6.3324e-04, 7.4053e-04,\n",
            "         1.2088e-04, 1.4138e-04, 8.5211e-04, 1.4138e-04, 2.4796e-04, 1.5678e-03,\n",
            "         8.2588e-04, 1.7214e-03, 5.6791e-04, 3.2854e-04, 4.8566e-04, 6.7425e-04,\n",
            "         4.7064e-04, 6.4325e-04, 7.1526e-06, 1.2159e-05, 1.6785e-04, 7.2181e-05,\n",
            "         1.0948e-03, 5.1975e-05, 1.3199e-03, 2.5201e-04, 2.7835e-05, 1.5587e-02,\n",
            "         6.5234e-01, 2.8381e-03, 1.0502e-04, 5.8889e-05, 7.7629e-04, 3.8574e-02,\n",
            "         2.0885e-04, 6.2981e-03, 1.5678e-03, 2.5582e-04, 2.3174e-03, 1.1009e-04,\n",
            "         1.7869e-04, 4.3535e-04, 1.5283e-04, 1.2451e-01, 7.3314e-05, 1.7166e-05,\n",
            "         1.0836e-04, 7.6830e-05, 3.4976e-04, 8.3466e-03]], device='cuda:0',\n",
            "       dtype=torch.float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction using MEMO with ADAM\n",
        "clip_model.use_ADAM()\n",
        "clip_model.tokenize_labels(cifar100.classes) #You have to tokenize the labels again for the change in precision\n",
        "prediction3, probs3, entropy3 = clip_model.MEMO(image, num_augmentations=200)\n",
        "print(prediction3)\n",
        "print(entropy3)\n",
        "print(probs3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlTDDcr_IRYf",
        "outputId": "00fac6c6-2ad3-42db-c0a7-af8c0eb5d5e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78\n",
            "0.8373997211456299\n",
            "tensor([[1.6591e-04, 1.0249e-03, 1.0178e-03, 2.8824e-05, 7.7287e-06, 1.1470e-04,\n",
            "         1.2600e-05, 3.6076e-04, 7.8495e-06, 4.7137e-05, 1.7768e-04, 2.8146e-04,\n",
            "         9.0305e-06, 7.7580e-06, 1.7224e-04, 4.9013e-06, 4.1220e-04, 3.3961e-06,\n",
            "         1.0478e-03, 1.0763e-04, 2.0637e-05, 1.0673e-04, 1.1451e-05, 9.0927e-06,\n",
            "         2.1189e-05, 3.2925e-05, 1.5194e-04, 1.1907e-02, 9.1187e-05, 1.7999e-03,\n",
            "         1.2486e-06, 6.3784e-05, 1.0149e-03, 9.9239e-05, 1.8798e-06, 1.7885e-04,\n",
            "         4.6223e-05, 4.1723e-05, 1.8384e-05, 8.6757e-06, 5.0464e-05, 5.3061e-03,\n",
            "         3.0214e-03, 5.7490e-05, 2.0175e-02, 6.9417e-06, 1.0235e-04, 5.4347e-05,\n",
            "         6.1173e-06, 5.7619e-05, 9.9058e-04, 3.6462e-04, 7.8096e-04, 7.8209e-05,\n",
            "         5.3192e-05, 1.2046e-05, 1.1861e-03, 2.5058e-05, 2.6502e-05, 6.1550e-03,\n",
            "         1.8720e-04, 3.5223e-04, 8.4711e-05, 3.9928e-05, 7.5721e-05, 1.0469e-04,\n",
            "         3.6221e-05, 8.8144e-05, 1.2223e-06, 1.3005e-06, 1.0501e-05, 1.7542e-05,\n",
            "         4.0825e-05, 7.7753e-06, 1.7458e-04, 9.3549e-05, 5.8485e-06, 1.6723e-03,\n",
            "         8.9971e-01, 2.5178e-04, 2.9215e-05, 4.8879e-06, 5.2304e-05, 2.7188e-02,\n",
            "         6.9318e-05, 9.5348e-04, 1.3798e-04, 3.2469e-05, 1.7149e-04, 8.8732e-06,\n",
            "         1.5461e-05, 7.2595e-05, 1.0118e-05, 6.6008e-03, 4.6223e-05, 1.3169e-06,\n",
            "         8.1127e-05, 8.8726e-06, 1.0346e-04, 2.3032e-03]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction using TPT\n",
        "prediction4,prob_avg, entropy4 = clip_model.TPT(image, num_augmentations=200)\n",
        "print(prediction4)\n",
        "print(entropy4)\n",
        "print(prob_avg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrxX4QrwsCbv",
        "outputId": "87628f34-ccf0-4068-d761-e11c969887c7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59\n",
            "5.03515625\n",
            "tensor([0.0127, 0.0011, 0.0041, 0.0046, 0.0016, 0.0013, 0.0020, 0.0052, 0.0014,\n",
            "        0.0040, 0.0073, 0.0023, 0.0007, 0.0024, 0.0029, 0.0010, 0.0082, 0.0123,\n",
            "        0.0042, 0.0055, 0.0016, 0.0025, 0.0014, 0.0034, 0.0023, 0.0019, 0.0069,\n",
            "        0.0239, 0.0027, 0.0293, 0.0005, 0.0023, 0.0064, 0.0066, 0.0003, 0.0014,\n",
            "        0.0019, 0.0032, 0.0006, 0.0060, 0.0044, 0.0216, 0.0052, 0.0039, 0.0168,\n",
            "        0.0010, 0.0030, 0.0233, 0.0017, 0.0091, 0.0030, 0.0170, 0.0448, 0.0042,\n",
            "        0.0011, 0.0002, 0.0572, 0.0045, 0.0029, 0.1476, 0.0051, 0.0052, 0.0059,\n",
            "        0.0025, 0.0019, 0.0021, 0.0021, 0.0012, 0.0003, 0.0040, 0.0031, 0.0005,\n",
            "        0.0048, 0.0009, 0.0008, 0.0035, 0.0025, 0.0067, 0.1105, 0.0013, 0.0008,\n",
            "        0.0006, 0.0121, 0.0171, 0.0012, 0.0751, 0.0022, 0.0036, 0.0039, 0.0068,\n",
            "        0.0030, 0.0040, 0.0046, 0.1014, 0.0005, 0.0018, 0.0214, 0.0018, 0.0016,\n",
            "        0.0098], dtype=torch.float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTSSSS\n"
      ],
      "metadata": {
        "id": "vkg1nIszau2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "imagenetv2 = datasets.ImageFolder(root='/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenetv2')\n",
        "imageneta = datasets.ImageFolder(root='/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenet-a')"
      ],
      "metadata": {
        "id": "XHe39gn-PHrR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "1d2c21d8-dbb2-49fe-dc06-8bcd8a0de677"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenetv2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f40f99cb1355>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimagenetv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenetv2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimageneta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenet-a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenetv2'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the class names for imagenet-A\n",
        "def classnames_imagenetA():\n",
        "    # Define the path to the words file\n",
        "    file_path = '/content/drive/MyDrive/Petaloso Project/Code/Datasets/words_imageneta.txt'\n",
        "\n",
        "    # Initialize an empty list to store the class names\n",
        "    class_names = []\n",
        "\n",
        "    # Open and read the file line by line\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            # Split each line into wnid and class name, and strip to remove any leading/trailing whitespace\n",
        "            parts = line.strip().split(' ', 1)\n",
        "            if len(parts) > 1:\n",
        "                # Append only the class name (second part) to the list\n",
        "                class_names.append(parts[1])\n",
        "    return class_names"
      ],
      "metadata": {
        "id": "_YTyv3q9V3Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing subsets\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "def create_stratified_subset(dataset, num_samples_per_class=5):\n",
        "    # Fix the random seed for reproducibility\n",
        "    torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "    # Determine class indices\n",
        "    targets = np.array([s[1] for s in dataset.samples])\n",
        "    classes, class_indices = np.unique(targets, return_inverse=True)\n",
        "\n",
        "    # Select samples from each class\n",
        "    indices = []\n",
        "    for c in classes:\n",
        "        class_idx = np.where(class_indices == c)[0]\n",
        "        if len(class_idx) >= num_samples_per_class:\n",
        "            selected_indices = np.random.choice(class_idx, num_samples_per_class, replace=False)\n",
        "            indices.extend(selected_indices)\n",
        "        else:\n",
        "            # If a class has fewer than the desired number, take all\n",
        "            indices.extend(class_idx)\n",
        "\n",
        "    # Create subset\n",
        "    subset = Subset(dataset, indices)\n",
        "    return subset\n",
        "\n",
        "subset = create_stratified_subset(imageneta, num_samples_per_class=5)"
      ],
      "metadata": {
        "id": "IGUDzNCle_E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "imageneta = datasets.ImageFolder(root='/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenet-a')\n",
        "\n",
        "def testing(dataset, model,method='CLIP', batch_size=32, num_aug=100):\n",
        "    model.tokenize_labels(classnames_imagenetA())\n",
        "\n",
        "    def custom_collate_fn(batch):\n",
        "        # Extract images and labels from the batch\n",
        "        images = [item[0] for item in batch]  # PIL images\n",
        "        labels = [item[1] for item in batch]  # Corresponding labels\n",
        "        return images, labels\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    entropies = []\n",
        "    confidences = []\n",
        "\n",
        "    # Evaluation loop\n",
        "    for images, labels in tqdm(dataloader):\n",
        "        for image, label in zip(images, labels):\n",
        "            try:\n",
        "                # We choose how the test time predictions are made\n",
        "                if method == 'CLIP':\n",
        "                    prediction, probs, entropy = model.predict(image)\n",
        "                elif method == 'MEMO':\n",
        "                    prediction, probs, entropy = model.MEMO(image, num_augmentations=num_aug)\n",
        "                elif method == 'MEMO_CONF':\n",
        "                    prediction, probs, entropy = model.MEMO(image, num_augmentations=num_aug, conf_sel = False)\n",
        "                elif method == 'TPT':\n",
        "                    prediction, probs, entropy = model.TPT(image, num_augmentations=num_aug)\n",
        "                else:\n",
        "                    print('Enter a valid method for testing.')\n",
        "\n",
        "                if int(prediction) == int(label):\n",
        "                    correct_predictions += 1\n",
        "                total_predictions += 1\n",
        "                entropies.append(entropy)\n",
        "                confidences.append(torch.max(probs).item())\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred: {e}\")\n",
        "\n",
        "    # Post evaluation statistics or processing\n",
        "    accuracy = (correct_predictions / total_predictions) * 100\n",
        "    average_entropy = sum(entropies) / len(entropies)\n",
        "    average_confidence = sum(confidences) / len(confidences)\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "    print(f'Average entropy across all predictions: {average_entropy:.2f}')\n",
        "\n",
        "\n",
        "testing(subset,clip_model, method = 'CLIP', batch_size=35)\n"
      ],
      "metadata": {
        "id": "2UPSff5cJs0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a08239-6974-4e8d-960b-bf2aab10de19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29/29 [00:18<00:00,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 31.26%\n",
            "Average entropy across all predictions: 7.64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COOP!!!"
      ],
      "metadata": {
        "id": "JaniK16Skr4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "\n",
        "_tokenizer = _Tokenizer()"
      ],
      "metadata": {
        "id": "g_IxvwpzZqKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        self.text_projection = clip_model.text_projection\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        x = prompts + self.positional_embedding\n",
        "        x = x.permute(1, 0, 2)  # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
        "        x = self.ln_final(x)\n",
        "\n",
        "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "JNC1Hxz1Zq-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
        "        super().__init__()\n",
        "        n_cls = len(classnames)\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        clip_imsize = clip_model.visual.input_resolution\n",
        "\n",
        "        # Use given words to initialize context vectors\n",
        "        # De aquí sacamos el context vector (tokenizado si lo sacamos de un cierto\n",
        "        # texto o directamente un vector random que no sale de tokenizar\n",
        "        # si lo iniciamos en plan random) y el prompt prefix que es el texto con\n",
        "        # el que comenzamos antes de entrenar, que puede ser o texto inicial\n",
        "        # con sentido o una X que no representa nada si no hay texto que inicialize\n",
        "        # solo representa la cantidad de palabras que hay\n",
        "        if ctx_init:\n",
        "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
        "            n_ctx = len(ctx_init.split(\" \"))\n",
        "            prompt = clip.tokenize(ctx_init).to(clip_model.token_embedding.weight.device)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt)\n",
        "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
        "            prompt_prefix = ctx_init\n",
        "        else:\n",
        "            if csc:\n",
        "                print(\"Initializing class-specific contexts\")\n",
        "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim)\n",
        "            else:\n",
        "                print(\"Initializing a generic context\")\n",
        "                ctx_vectors = torch.empty(n_ctx, ctx_dim)\n",
        "\n",
        "            torch.nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "\n",
        "        print(f\"Initial context: '{prompt_prefix}'\")\n",
        "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
        "\n",
        "        # These are the `prompts` we want to optimize\n",
        "        self.ctx = nn.Parameter(ctx_vectors)\n",
        "\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "\n",
        "        # print(\"+++\")\n",
        "        # print(\"Prompts:\")\n",
        "        # for p in prompts:\n",
        "        #     print(p)\n",
        "        # print(\"+++\")\n",
        "\n",
        "        # Aqui está tokenizando a partir de el prompt (las X o lo que le hayamos\n",
        "        # dado) pero no usa el context vector pa nada. Pero ese es el que nos importa\n",
        "        # así que NO ENTIENDO ESTOS PA QUE SON. PARA SACAR EL SOS, CLS y EOS tokens\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(clip_model.token_embedding.weight.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
        "\n",
        "        # These token vectors will be saved when in save_model(),\n",
        "        # but they should be ignored in load_model() as we want to use\n",
        "        # those computed using the current class names.\n",
        "        # Buffer implica que no computas el gradiente para estos tokens. Asi que\n",
        "        # son constantes!\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
        "\n",
        "        self.n_cls = n_cls\n",
        "        self.n_ctx = n_ctx\n",
        "        self.tokenized_prompts = tokenized_prompts\n",
        "        self.name_lens = name_lens\n",
        "        self.class_token_position = class_token_position\n",
        "\n",
        "    def forward(self):\n",
        "        prefix = self.token_prefix\n",
        "        suffix = self.token_suffix\n",
        "        ctx = self.ctx\n",
        "\n",
        "        # If CoOp, expand the ctx for all classes (implying a shared context across all classes)\n",
        "        if ctx.dim() == 2:\n",
        "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "\n",
        "        #Metemos el class token donde toque.\n",
        "        #PERO AQUI NO ESTÁ EL CLASS TOKEN! SII VA EN EL SUFFIX JUNTO CON EOS\n",
        "        if self.class_token_position == \"end\":\n",
        "            prompts = torch.cat(\n",
        "                [\n",
        "                    prefix,  # (n_cls, 1, dim)\n",
        "                    ctx,     # (n_cls, n_ctx, dim)\n",
        "                    suffix,  # (n_cls, *, dim)\n",
        "                ],\n",
        "                dim=1,\n",
        "            )\n",
        "\n",
        "        elif self.class_token_position == \"middle\":\n",
        "            half_n_ctx = self.n_ctx // 2\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
        "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
        "                prompt = torch.cat(\n",
        "                    [\n",
        "                        prefix_i,     # (1, 1, dim)\n",
        "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
        "                        class_i,      # (1, name_len, dim)\n",
        "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
        "                        suffix_i,     # (1, *, dim)\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        elif self.class_token_position == \"front\":\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i = ctx[i : i + 1, :, :]\n",
        "                prompt = torch.cat(\n",
        "                    [\n",
        "                        prefix_i,  # (1, 1, dim)\n",
        "                        class_i,   # (1, name_len, dim)\n",
        "                        ctx_i,     # (1, n_ctx, dim)\n",
        "                        suffix_i,  # (1, *, dim)\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "        return prompts"
      ],
      "metadata": {
        "id": "Sn0AmktyZxxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_coop()"
      ],
      "metadata": {
        "id": "p912NXY-Z8NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CoCoOp:\n",
        "    def __init__(self, CLIP_model, device=None):\n",
        "        self.device = device if device else \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "sT6wAzCrEoL9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}