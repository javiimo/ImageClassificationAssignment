{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javiimo/ImageClassificationAssignment/blob/main/CLIPClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIg1G6AD6fyQ",
        "outputId": "be6938fc-f3b4-4e21-8d5c-48a49e3216ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.2.0\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-meoqy32g\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-meoqy32g\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.17.1+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369499 sha256=1856b118f616de7cfdb5379de4fca3c1a675bbcb30088c3cfbd682800bf5a45f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-g9faqnr8/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n",
            "Successfully installed clip-1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.0-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.0 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "! pip install datasets transformers\n",
        "\n",
        "\n",
        "import clip\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imagenetv2= 'vaishaal/ImageNetV2' #https://github.com/modestyachts/ImageNetV2?tab=readme-ov-file\n",
        "class HFDataset:\n",
        "    def __init__(self, name='barkermrl/imagenet-a', split=None, config=None):\n",
        "        self.name = name\n",
        "        self.dataset = None\n",
        "        self.datasetdict = None #In case there are multiple splits and none is selected\n",
        "        self.split = split\n",
        "\n",
        "\n",
        "    def print_info(self):\n",
        "        dset.inspect()\n",
        "        print('Split types:')\n",
        "        print(dset.split_types())\n",
        "        print('Configurations:')\n",
        "        print(self.config_names())\n",
        "\n",
        "\n",
        "    def inspect(self):\n",
        "        from datasets import load_dataset_builder\n",
        "        ds_builder = load_dataset_builder(self.name)\n",
        "        print('Description:')\n",
        "        print(ds_builder.info.description)\n",
        "        print('Features: ')\n",
        "        print(ds_builder.info.features)\n",
        "\n",
        "    def split_types(self):\n",
        "        from datasets import get_dataset_split_names\n",
        "        return get_dataset_split_names(self.name) #list of split types\n",
        "\n",
        "    def config_names(self):\n",
        "        from datasets import get_dataset_config_names\n",
        "        return get_dataset_config_names(self.name) #list of subdataset\n",
        "\n",
        "    def load(self, split=None, config=None):\n",
        "        # Set config and split to instance attributes if provided\n",
        "        if config is not None:\n",
        "            self.config = config\n",
        "        if split is not None:\n",
        "            self.split = split\n",
        "\n",
        "        # Choose the appropriate dataset loading mechanism\n",
        "        if self.split is None:\n",
        "            # If no split is defined, it implies a full dataset dict might be loaded\n",
        "            if self.config is not None:\n",
        "                print('Setting a DatasetDict')\n",
        "                self.datasetdict = load_dataset(self.name, self.config)\n",
        "            else:\n",
        "                print('Setting a DatasetDict')\n",
        "                self.datasetdict = load_dataset(self.name)\n",
        "        else:\n",
        "            # Load specific split\n",
        "            if self.config is not None:\n",
        "                self.dataset = load_dataset(self.name, self.config, split=self.split)\n",
        "            else:\n",
        "                self.dataset = load_dataset(self.name, split=self.split)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "grgqma5RNJg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dset = HFDataset(name=imagenetv2)\n",
        "dset.print_info()\n",
        "#dset.load(split='train')\n",
        "#dset.dataset[900]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJIXzG84Ltnw",
        "outputId": "e7849bea-c710-4a1f-e822-0afb56c0d367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Description:\n",
            "\n",
            "Features: \n",
            "None\n",
            "Split types:\n",
            "['train']\n",
            "Configurations:\n",
            "['default']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6WJzQYEtTxi",
        "outputId": "31e105ff-b1f5-46bb-ff07-1ca2c52d4a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "imagenetv2 = datasets.ImageFolder(root='/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenetv2')\n",
        "imageneta = datasets.ImageFolder(root='/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenet-a')"
      ],
      "metadata": {
        "id": "XHe39gn-PHrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our data sets\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset_imageneta = load_dataset('barkermrl/imagenet-a',  split=\"test\")\n",
        "#dataset_imagenetv2 = load_dataset('imagenet_v2')\n"
      ],
      "metadata": {
        "id": "IgdgKOGfj-S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CLIPModel:\n",
        "    def __init__(self, model_name='ViT-B/32', device=None):\n",
        "        self.device = device if device else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model, self.preprocess = clip.load(model_name, self.device)\n",
        "        #self.model = self.convert_model_parameters_to_float32(self.model)\n",
        "        #self.optimizer = optim.Adam(self.model.parameters(), lr=0.000000001, weight_decay=2e-4) #Numerical inestability\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=1000, momentum=0.9)\n",
        "        self.text_features = None\n",
        "        self.requiring_grads = None\n",
        "\n",
        "    def require_CLIP_gradients(self, state = True):\n",
        "        if self.requiring_grads is None or state != self.requiring_grads: #don't change if the state is already OK\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = state\n",
        "            self.requiring_grads = state\n",
        "\n",
        "    def convert_model_parameters_to_float32(self, model):\n",
        "        for param in model.parameters():\n",
        "            param.data = param.data.to(torch.float32)\n",
        "        return model\n",
        "\n",
        "    def load_data(self):\n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', download=True, train=False)\n",
        "        return cifar100\n",
        "\n",
        "    #This are heuristic labels\n",
        "    def tokenize_labels(self, classes):\n",
        "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classes]).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            self.text_features = self.model.encode_text(text_inputs)\n",
        "            self.text_features /= self.text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    def augment_image(self, image, num_augmentations=100):\n",
        "        augmentations = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomVerticalFlip(p=0.5),\n",
        "            transforms.RandomRotation(degrees=30),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "            transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
        "        ])\n",
        "        augmented_images = [self.preprocess(image).unsqueeze(0).to(self.device)]\n",
        "        for _ in range(num_augmentations):\n",
        "            augmented_images.append(self.preprocess(augmentations(image)).unsqueeze(0).to(self.device))\n",
        "        batch = torch.vstack(augmented_images)\n",
        "        return batch\n",
        "\n",
        "    def compute_entropy(self, x):\n",
        "        log_x = torch.log2(x.clamp_min(1e-20))\n",
        "        entropy = -torch.sum(x * log_x)\n",
        "        return entropy\n",
        "\n",
        "    def class_probabilities(self, text_features, image_features):\n",
        "        #Compute cosine similarities\n",
        "        return (image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "    def confidence_selection(self, similarities, percentile = 0.8):\n",
        "        entropies = torch.tensor([self.compute_entropy(row) for row in similarities])\n",
        "        sorted_entropies, _ = torch.sort(entropies, descending=True)\n",
        "        threshold = sorted_entropies[int(len(sorted_entropies) * percentile)]\n",
        "        boolean_mask = entropies < threshold\n",
        "        return similarities[boolean_mask]\n",
        "\n",
        "    def entropy_loss_MEMO(self, image_features, text_features = None, conf_sel=False):\n",
        "        if text_features is None:\n",
        "            text_features = self.text_features\n",
        "        similarities = self.class_probabilities(text_features, image_features)\n",
        "        # Apply confidence selection to rule out high entropy augmentations\n",
        "        if conf_sel:\n",
        "            similarities = self.confidence_selection(similarities)\n",
        "        # Compute the entropy of every text caption accross all augmentations\n",
        "        entropies = [self.compute_entropy(row) for row in similarities]\n",
        "        return torch.stack(entropies).mean()\n",
        "\n",
        "    def entropy_loss_TPT(self, image_features, text_features = None):\n",
        "        if text_features is None:\n",
        "            text_features = self.text_features\n",
        "        similarities = self.class_probabilities(text_features, image_features)\n",
        "        # Confidence selection for the augmented views:\n",
        "        similarities = self.confidence_selection(similarities)\n",
        "        # Average the caption probabilities across all augmentations\n",
        "        avg_probs = torch.tensor([row.mean() for row in similarities.T])\n",
        "        # Compute the entropy of the averaged probability distribution\n",
        "        return self.compute_entropy(avg_probs), avg_probs\n",
        "\n",
        "    def grad_descent_step(self, loss):\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def predict(self, image):\n",
        "        self.model.eval()\n",
        "        image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            image_features = self.model.encode_image(image)\n",
        "            norms = image_features.norm(dim=-1, keepdim=True)\n",
        "            if (norms == 0).any():\n",
        "                print(\"Zero norm found in image features\")\n",
        "            image_features = image_features / norms.clamp_min(1e-10)\n",
        "\n",
        "        similarity = self.class_probabilities(self.text_features, image_features)\n",
        "        prediction = torch.argmax(similarity).item()\n",
        "        entropy = float(self.compute_entropy(similarity))\n",
        "        return prediction, similarity, entropy\n",
        "\n",
        "    def MEMO(self, image, num_augmentations=100, conf_sel = False):\n",
        "        # Save original parameters\n",
        "        original_params = {name: param.clone() for name, param in self.model.named_parameters()}\n",
        "\n",
        "        # Require gradients to update the CLIP parameters\n",
        "        self.require_CLIP_gradients(state = True)\n",
        "        try:\n",
        "            self.model.train() #DO WE WANT BATCH NORMALIZ AND DROPOUT?\n",
        "            batch = self.augment_image(image, num_augmentations)\n",
        "            image_features = self.model.encode_image(batch)\n",
        "            norms = image_features.norm(dim=-1, keepdim=True)\n",
        "            if (norms == 0).any():\n",
        "                print(\"Zero norm found in image features\")\n",
        "            image_features = image_features / norms.clamp_min(1e-10)\n",
        "\n",
        "            loss = self.entropy_loss_MEMO(image_features, conf_sel= conf_sel)\n",
        "            self.grad_descent_step(loss)\n",
        "\n",
        "            if any(torch.isnan(param).any() for param in self.model.parameters()):\n",
        "                print(\"nan values detected in model parameters after updating\")\n",
        "            # Predict using the updated model\n",
        "            prediction, similarity, entropy = self.predict(image)\n",
        "        finally:\n",
        "            # Restore original parameters\n",
        "            with torch.no_grad():\n",
        "                for name, param in self.model.named_parameters():\n",
        "                    param.copy_(original_params[name])\n",
        "        return prediction, similarity, entropy\n",
        "\n",
        "    def TPT(self, image, num_augmentations=100):\n",
        "        batch = self.augment_image(image, num_augmentations)\n",
        "        image_features = self.model.encode_image(batch)\n",
        "        norms = image_features.norm(dim=-1, keepdim=True)\n",
        "        if (norms == 0).any():\n",
        "            print(\"Zero norm found in image features\")\n",
        "        image_features = image_features / norms.clamp_min(1e-10)\n",
        "\n",
        "        entropy, avg_probs = self.entropy_loss_TPT(image_features)\n",
        "        prediction = torch.argmax(avg_probs).item()\n",
        "        return prediction, avg_probs, float(entropy)\n",
        "\n",
        "    def train_CoOp(self):\n",
        "        #Prevent CLIP parameters from changing\n",
        "        self.require_CLIP_gradients(state=False)\n",
        "\n",
        "\n",
        "# Preparing the class for usage\n",
        "clip_model = CLIPModel()"
      ],
      "metadata": {
        "id": "6XGJSO0FnKTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading CIFAR100 for one image tries_\n",
        "cifar100 = clip_model.load_data()\n",
        "clip_model.tokenize_labels(cifar100.classes)\n",
        "image, class_id = cifar100[3637]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B52IQTi6OyfW",
        "outputId": "9ba183be-11c4-4327-dffd-56d0d70add09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction using CLIP out of the box\n",
        "prediction1, similarity1, entropy1 = clip_model.predict(image)\n",
        "print(similarity1, prediction1, entropy1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "087FNIiqpDUb",
        "outputId": "d3030e15-9c54-40fe-a1bb-1c3c22588189"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0101, 0.0102, 0.0102, 0.0099, 0.0099, 0.0100, 0.0100, 0.0103, 0.0098,\n",
            "         0.0100, 0.0101, 0.0100, 0.0097, 0.0098, 0.0101, 0.0099, 0.0101, 0.0098,\n",
            "         0.0102, 0.0101, 0.0099, 0.0101, 0.0099, 0.0098, 0.0100, 0.0099, 0.0103,\n",
            "         0.0104, 0.0100, 0.0102, 0.0096, 0.0100, 0.0103, 0.0098, 0.0096, 0.0100,\n",
            "         0.0101, 0.0098, 0.0098, 0.0099, 0.0101, 0.0103, 0.0103, 0.0100, 0.0104,\n",
            "         0.0099, 0.0100, 0.0098, 0.0098, 0.0100, 0.0103, 0.0102, 0.0100, 0.0100,\n",
            "         0.0099, 0.0099, 0.0100, 0.0099, 0.0099, 0.0101, 0.0100, 0.0101, 0.0100,\n",
            "         0.0100, 0.0100, 0.0100, 0.0100, 0.0100, 0.0096, 0.0096, 0.0099, 0.0098,\n",
            "         0.0101, 0.0098, 0.0101, 0.0099, 0.0097, 0.0103, 0.0107, 0.0102, 0.0098,\n",
            "         0.0098, 0.0100, 0.0104, 0.0099, 0.0102, 0.0101, 0.0099, 0.0101, 0.0098,\n",
            "         0.0099, 0.0100, 0.0099, 0.0106, 0.0098, 0.0097, 0.0098, 0.0098, 0.0100,\n",
            "         0.0103]]) 78 6.643564224243164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction using MEMO at test time\n",
        "prediction2, similarity2, entropy2 = clip_model.MEMO(image, num_augmentations=100)\n",
        "print(similarity2, prediction2, entropy2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RheDQxnoeB7",
        "outputId": "73a6ba31-7ba6-493d-aaf0-6423770ccb94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.6438, grad_fn=<MeanBackward0>)\n",
            "tensor([[0.0103, 0.0099, 0.0103, 0.0100, 0.0094, 0.0099, 0.0101, 0.0100, 0.0098,\n",
            "         0.0099, 0.0101, 0.0102, 0.0096, 0.0101, 0.0100, 0.0097, 0.0098, 0.0099,\n",
            "         0.0105, 0.0102, 0.0100, 0.0098, 0.0097, 0.0097, 0.0095, 0.0099, 0.0096,\n",
            "         0.0101, 0.0102, 0.0101, 0.0095, 0.0097, 0.0095, 0.0106, 0.0096, 0.0101,\n",
            "         0.0099, 0.0100, 0.0094, 0.0099, 0.0099, 0.0106, 0.0101, 0.0102, 0.0101,\n",
            "         0.0096, 0.0102, 0.0107, 0.0099, 0.0097, 0.0099, 0.0100, 0.0108, 0.0100,\n",
            "         0.0100, 0.0095, 0.0108, 0.0101, 0.0104, 0.0109, 0.0103, 0.0103, 0.0103,\n",
            "         0.0099, 0.0097, 0.0098, 0.0096, 0.0094, 0.0098, 0.0100, 0.0103, 0.0098,\n",
            "         0.0095, 0.0096, 0.0097, 0.0101, 0.0097, 0.0102, 0.0104, 0.0099, 0.0098,\n",
            "         0.0100, 0.0103, 0.0105, 0.0101, 0.0102, 0.0101, 0.0100, 0.0101, 0.0105,\n",
            "         0.0099, 0.0098, 0.0103, 0.0102, 0.0096, 0.0096, 0.0105, 0.0099, 0.0102,\n",
            "         0.0103]]) 59 6.643089294433594\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction using TPT\n",
        "prediction3,prob_avg, entropy3 = clip_model.TPT(image)\n",
        "print(entropy3)\n",
        "print(prob_avg)\n",
        "print(prediction3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrxX4QrwsCbv",
        "outputId": "af1765b3-3716-471c-a12c-87c2c4cd4f0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.643748760223389\n",
            "tensor([0.0102, 0.0099, 0.0101, 0.0101, 0.0100, 0.0099, 0.0100, 0.0101, 0.0099,\n",
            "        0.0100, 0.0101, 0.0100, 0.0098, 0.0099, 0.0100, 0.0099, 0.0101, 0.0099,\n",
            "        0.0101, 0.0101, 0.0099, 0.0100, 0.0099, 0.0100, 0.0099, 0.0099, 0.0101,\n",
            "        0.0102, 0.0100, 0.0102, 0.0098, 0.0099, 0.0101, 0.0100, 0.0097, 0.0100,\n",
            "        0.0100, 0.0099, 0.0098, 0.0100, 0.0100, 0.0101, 0.0101, 0.0100, 0.0101,\n",
            "        0.0099, 0.0100, 0.0101, 0.0099, 0.0101, 0.0100, 0.0102, 0.0102, 0.0101,\n",
            "        0.0099, 0.0098, 0.0102, 0.0101, 0.0099, 0.0103, 0.0101, 0.0101, 0.0101,\n",
            "        0.0100, 0.0100, 0.0100, 0.0100, 0.0099, 0.0097, 0.0100, 0.0100, 0.0098,\n",
            "        0.0101, 0.0099, 0.0099, 0.0100, 0.0098, 0.0101, 0.0102, 0.0099, 0.0099,\n",
            "        0.0098, 0.0101, 0.0103, 0.0099, 0.0101, 0.0099, 0.0099, 0.0100, 0.0100,\n",
            "        0.0099, 0.0101, 0.0100, 0.0103, 0.0097, 0.0100, 0.0101, 0.0100, 0.0100,\n",
            "        0.0102])\n",
            "59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction using MEMO with confidence selection at test time\n",
        "prediction4, similarity4, entropy4 = clip_model.MEMO(image, num_augmentations=100, conf_sel=True)\n",
        "print(similarity4, prediction4, entropy4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6EOTB7t1kZG",
        "outputId": "872411cc-bc83-4a77-9306-c5f055a98604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.6437, grad_fn=<MeanBackward0>)\n",
            "tensor([[0.0105, 0.0097, 0.0105, 0.0099, 0.0095, 0.0098, 0.0101, 0.0100, 0.0096,\n",
            "         0.0099, 0.0104, 0.0105, 0.0093, 0.0096, 0.0101, 0.0097, 0.0099, 0.0095,\n",
            "         0.0105, 0.0101, 0.0096, 0.0098, 0.0097, 0.0098, 0.0098, 0.0095, 0.0099,\n",
            "         0.0101, 0.0102, 0.0103, 0.0098, 0.0097, 0.0100, 0.0100, 0.0096, 0.0103,\n",
            "         0.0102, 0.0097, 0.0096, 0.0096, 0.0098, 0.0101, 0.0100, 0.0102, 0.0103,\n",
            "         0.0098, 0.0104, 0.0103, 0.0097, 0.0096, 0.0101, 0.0104, 0.0103, 0.0104,\n",
            "         0.0098, 0.0097, 0.0104, 0.0104, 0.0099, 0.0106, 0.0104, 0.0104, 0.0104,\n",
            "         0.0098, 0.0098, 0.0099, 0.0094, 0.0098, 0.0094, 0.0100, 0.0105, 0.0100,\n",
            "         0.0098, 0.0098, 0.0100, 0.0102, 0.0095, 0.0106, 0.0106, 0.0099, 0.0099,\n",
            "         0.0097, 0.0104, 0.0106, 0.0101, 0.0099, 0.0100, 0.0097, 0.0101, 0.0099,\n",
            "         0.0095, 0.0100, 0.0104, 0.0103, 0.0093, 0.0099, 0.0100, 0.0098, 0.0104,\n",
            "         0.0105]]) 83 6.64306640625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the similarities after the two MEMO implementations (with confidence selection and without it) are the same or not\n",
        "s1 = similarity1.to(torch.float16)\n",
        "s2 = similarity2.to(torch.float16)\n",
        "s4 = similarity4.to(torch.float16)\n",
        "print('Clip vs Memo without conf sel')\n",
        "print(s1==s2)\n",
        "print('Clip vs Memo with conf sel')\n",
        "print(s1==s4)\n",
        "print('Memo with conf sel vs Memo without conf sel')\n",
        "print(s2==s4)"
      ],
      "metadata": {
        "id": "WF8GfLAGiUr5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9fcf9e-b030-4200-8587-1fb47d97d795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clip vs Memo without conf sel\n",
            "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False]])\n",
            "Clip vs Memo with conf sel\n",
            "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True, False, False, False,  True, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False]])\n",
            "Memo with conf sel vs Memo without conf sel\n",
            "tensor([[False, False, False, False, False, False, False,  True, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "\n",
        "_tokenizer = _Tokenizer()"
      ],
      "metadata": {
        "id": "g_IxvwpzZqKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        self.text_projection = clip_model.text_projection\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        x = prompts + self.positional_embedding\n",
        "        x = x.permute(1, 0, 2)  # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
        "        x = self.ln_final(x)\n",
        "\n",
        "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "JNC1Hxz1Zq-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
        "        super().__init__()\n",
        "        n_cls = len(classnames)\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        clip_imsize = clip_model.visual.input_resolution\n",
        "\n",
        "        # Use given words to initialize context vectors\n",
        "        # De aquí sacamos el context vector (tokenizado si lo sacamos de un cierto\n",
        "        # texto o directamente un vector random que no sale de tokenizar\n",
        "        # si lo iniciamos en plan random) y el prompt prefix que es el texto con\n",
        "        # el que comenzamos antes de entrenar, que puede ser o texto inicial\n",
        "        # con sentido o una X que no representa nada si no hay texto que inicialize\n",
        "        # solo representa la cantidad de palabras que hay\n",
        "        if ctx_init:\n",
        "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
        "            n_ctx = len(ctx_init.split(\" \"))\n",
        "            prompt = clip.tokenize(ctx_init).to(clip_model.token_embedding.weight.device)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt)\n",
        "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
        "            prompt_prefix = ctx_init\n",
        "        else:\n",
        "            if csc:\n",
        "                print(\"Initializing class-specific contexts\")\n",
        "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim)\n",
        "            else:\n",
        "                print(\"Initializing a generic context\")\n",
        "                ctx_vectors = torch.empty(n_ctx, ctx_dim)\n",
        "\n",
        "            torch.nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "\n",
        "        print(f\"Initial context: '{prompt_prefix}'\")\n",
        "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
        "\n",
        "        # These are the `prompts` we want to optimize\n",
        "        self.ctx = nn.Parameter(ctx_vectors)\n",
        "\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "\n",
        "        # print(\"+++\")\n",
        "        # print(\"Prompts:\")\n",
        "        # for p in prompts:\n",
        "        #     print(p)\n",
        "        # print(\"+++\")\n",
        "\n",
        "        # Aqui está tokenizando a partir de el prompt (las X o lo que le hayamos\n",
        "        # dado) pero no usa el context vector pa nada. Pero ese es el que nos importa\n",
        "        # así que NO ENTIENDO ESTOS PA QUE SON. PARA SACAR EL SOS, CLS y EOS tokens\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(clip_model.token_embedding.weight.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
        "\n",
        "        # These token vectors will be saved when in save_model(),\n",
        "        # but they should be ignored in load_model() as we want to use\n",
        "        # those computed using the current class names.\n",
        "        # Buffer implica que no computas el gradiente para estos tokens. Asi que\n",
        "        # son constantes!\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
        "\n",
        "        self.n_cls = n_cls\n",
        "        self.n_ctx = n_ctx\n",
        "        self.tokenized_prompts = tokenized_prompts\n",
        "        self.name_lens = name_lens\n",
        "        self.class_token_position = class_token_position\n",
        "\n",
        "    def forward(self):\n",
        "        prefix = self.token_prefix\n",
        "        suffix = self.token_suffix\n",
        "        ctx = self.ctx\n",
        "\n",
        "        # If CoOp, expand the ctx for all classes (implying a shared context across all classes)\n",
        "        if ctx.dim() == 2:\n",
        "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "\n",
        "        #Metemos el class token donde toque.\n",
        "        #PERO AQUI NO ESTÁ EL CLASS TOKEN! SII VA EN EL SUFFIX JUNTO CON EOS\n",
        "        if self.class_token_position == \"end\":\n",
        "            prompts = torch.cat(\n",
        "                [\n",
        "                    prefix,  # (n_cls, 1, dim)\n",
        "                    ctx,     # (n_cls, n_ctx, dim)\n",
        "                    suffix,  # (n_cls, *, dim)\n",
        "                ],\n",
        "                dim=1,\n",
        "            )\n",
        "\n",
        "        elif self.class_token_position == \"middle\":\n",
        "            half_n_ctx = self.n_ctx // 2\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
        "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
        "                prompt = torch.cat(\n",
        "                    [\n",
        "                        prefix_i,     # (1, 1, dim)\n",
        "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
        "                        class_i,      # (1, name_len, dim)\n",
        "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
        "                        suffix_i,     # (1, *, dim)\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        elif self.class_token_position == \"front\":\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i = ctx[i : i + 1, :, :]\n",
        "                prompt = torch.cat(\n",
        "                    [\n",
        "                        prefix_i,  # (1, 1, dim)\n",
        "                        class_i,   # (1, name_len, dim)\n",
        "                        ctx_i,     # (1, n_ctx, dim)\n",
        "                        suffix_i,  # (1, *, dim)\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "        return prompts"
      ],
      "metadata": {
        "id": "Sn0AmktyZxxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_coop()"
      ],
      "metadata": {
        "id": "p912NXY-Z8NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CoCoOp:\n",
        "    def __init__(self, CLIP_model, device=None):\n",
        "        self.device = device if device else \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "sT6wAzCrEoL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTSSSS\n"
      ],
      "metadata": {
        "id": "vkg1nIszau2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the class names for imagenet-A\n",
        "def classnames_imagenetA():\n",
        "    # Define the path to the words file\n",
        "    file_path = '/content/drive/MyDrive/Petaloso Project/Code/Datasets/words_imageneta.txt'\n",
        "\n",
        "    # Initialize an empty list to store the class names\n",
        "    class_names = []\n",
        "\n",
        "    # Open and read the file line by line\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            # Split each line into wnid and class name, and strip to remove any leading/trailing whitespace\n",
        "            parts = line.strip().split(' ', 1)\n",
        "            if len(parts) > 1:\n",
        "                # Append only the class name (second part) to the list\n",
        "                class_names.append(parts[1])\n",
        "    return class_names"
      ],
      "metadata": {
        "id": "_YTyv3q9V3Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing subsets\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "def create_stratified_subset(dataset, num_samples_per_class=5):\n",
        "    # Fix the random seed for reproducibility\n",
        "    torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "    # Determine class indices\n",
        "    targets = np.array([s[1] for s in dataset.samples])\n",
        "    classes, class_indices = np.unique(targets, return_inverse=True)\n",
        "\n",
        "    # Select samples from each class\n",
        "    indices = []\n",
        "    for c in classes:\n",
        "        class_idx = np.where(class_indices == c)[0]\n",
        "        if len(class_idx) >= num_samples_per_class:\n",
        "            selected_indices = np.random.choice(class_idx, num_samples_per_class, replace=False)\n",
        "            indices.extend(selected_indices)\n",
        "        else:\n",
        "            # If a class has fewer than the desired number, take all\n",
        "            indices.extend(class_idx)\n",
        "\n",
        "    # Create subset\n",
        "    subset = Subset(dataset, indices)\n",
        "    return subset\n",
        "\n",
        "subset = create_stratified_subset(imageneta, num_samples_per_class=5)"
      ],
      "metadata": {
        "id": "IGUDzNCle_E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "imageneta = datasets.ImageFolder(root='/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenet-a')\n",
        "\n",
        "def testing(dataset, model,method='CLIP', batch_size=32, num_aug=100):\n",
        "    model.tokenize_labels(classnames_imagenetA())\n",
        "\n",
        "    def custom_collate_fn(batch):\n",
        "        # Extract images and labels from the batch\n",
        "        images = [item[0] for item in batch]  # PIL images\n",
        "        labels = [item[1] for item in batch]  # Corresponding labels\n",
        "        return images, labels\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    entropies = []\n",
        "    confidences = []\n",
        "\n",
        "    # Evaluation loop\n",
        "    for images, labels in tqdm(dataloader):\n",
        "        for image, label in zip(images, labels):\n",
        "            try:\n",
        "                # We choose how the test time predictions are made\n",
        "                if method == 'CLIP':\n",
        "                    prediction, probs, entropy = model.predict(image)\n",
        "                elif method == 'MEMO':\n",
        "                    prediction, probs, entropy = model.MEMO(image, num_augmentations=num_aug)\n",
        "                elif method == 'MEMO_CONF':\n",
        "                    prediction, probs, entropy = model.MEMO(image, num_augmentations=num_aug, conf_sel = False)\n",
        "                elif method == 'TPT':\n",
        "                    prediction, probs, entropy = model.TPT(image, num_augmentations=num_aug)\n",
        "                else:\n",
        "                    print('Enter a valid method for testing.')\n",
        "\n",
        "                if int(prediction) == int(label):\n",
        "                    correct_predictions += 1\n",
        "                total_predictions += 1\n",
        "                entropies.append(entropy)\n",
        "                confidences.append(torch.max(probs).item())\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred: {e}\")\n",
        "\n",
        "    # Post evaluation statistics or processing\n",
        "    accuracy = (correct_predictions / total_predictions) * 100\n",
        "    average_entropy = sum(entropies) / len(entropies)\n",
        "    average_confidence = sum(confidences) / len(confidences)\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "    print(f'Average entropy across all predictions: {average_entropy:.2f}')\n",
        "\n",
        "\n",
        "testing(subset,clip_model, method = 'CLIP', batch_size=35)\n"
      ],
      "metadata": {
        "id": "2UPSff5cJs0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a08239-6974-4e8d-960b-bf2aab10de19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29/29 [00:18<00:00,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 31.26%\n",
            "Average entropy across all predictions: 7.64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}