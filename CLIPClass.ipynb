{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javiimo/ImageClassificationAssignment/blob/main/CLIPClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIg1G6AD6fyQ",
        "outputId": "72d35543-78c0-4981-f79c-b3b54fab6d24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-urt6hl6q\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-urt6hl6q\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.17.1+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "! pip install datasets transformers\n",
        "\n",
        "\n",
        "import clip\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6WJzQYEtTxi",
        "outputId": "0322bbd5-d299-43c7-ed63-405241409065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPModel:\n",
        "    def __init__(self, model_name='ViT-B/32', device=None):\n",
        "        self.device = device if device else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model, self.preprocess = clip.load(model_name, self.device)\n",
        "        self.model = self.convert_model_parameters_to_float32(self.model)\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
        "        self.text_features = None\n",
        "        self.requiring_grads = None\n",
        "\n",
        "    def use_ADAM(self):\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-5) #Numerical inestability\n",
        "\n",
        "    def use_SGD(self):\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
        "    def require_CLIP_gradients(self, state = True):\n",
        "        if self.requiring_grads is None or state != self.requiring_grads: #don't change if the state is already OK\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = state\n",
        "            self.requiring_grads = state\n",
        "\n",
        "    def convert_model_parameters_to_float32(self, model):\n",
        "        for param in model.parameters():\n",
        "            param.data = param.data.to(torch.float32)\n",
        "        return model\n",
        "\n",
        "    def load_data(self):\n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', download=True, train=False)\n",
        "        return cifar100\n",
        "\n",
        "    #This are heuristic labels\n",
        "    def tokenize_labels(self, classes):\n",
        "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classes]).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            self.text_features = self.model.encode_text(text_inputs)\n",
        "            self.text_features /= self.text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    def augment_image(self, image, num_augmentations=100, transformations=None):\n",
        "        torch.manual_seed(33)#Set a seed for reproducibility of the random augmentations\n",
        "        if transformations==None:\n",
        "            augmentations = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomVerticalFlip(p=0.5),\n",
        "                transforms.RandomRotation(degrees=30),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "                transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
        "            ])\n",
        "        augmented_images = [self.preprocess(image).unsqueeze(0).to(self.device)] #Add the original image to the batch of augmentations\n",
        "        for _ in range(num_augmentations):\n",
        "            augmented_images.append(self.preprocess(augmentations(image)).unsqueeze(0).to(self.device))\n",
        "        batch = torch.vstack(augmented_images)\n",
        "        return batch #(num_augumentations + 1, 3, 224, 224)\n",
        "\n",
        "    def marginal_entropy(self, logits):\n",
        "        z = logits - logits.logsumexp(dim = -1, keepdim=True) # compute z_ij\n",
        "        marginal_logp = z.logsumexp(dim=0) - np.log(z.shape[0])   # compute marginal log probabilities\n",
        "\n",
        "        min_real = torch.finfo(marginal_logp.dtype).min # for numerical stability,\n",
        "        # the smallest representable number given the dtype of logits.\n",
        "        avg_logits = torch.clamp(marginal_logp, min = min_real)  # put a threshold to avoid underflow\n",
        "\n",
        "        return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
        "\n",
        "    def compute_entropy(self, x): #Shanon entropy in bits\n",
        "        #This computes the Shanon entropy\n",
        "        log_x = torch.log2(x.clamp_min(1e-20))\n",
        "        entropy = -torch.sum(x * log_x)\n",
        "        return entropy\n",
        "\n",
        "    def class_probabilities(self, text_features, image_features):\n",
        "        #Compute cosine similarities\n",
        "        return  (100 * self.cos_sim(image_features, text_features)).softmax(dim=-1)\n",
        "\n",
        "    def cos_sim(self, image_features, text_features):\n",
        "        return  image_features @ text_features.T\n",
        "\n",
        "    def logits(self, text_features, image_features):\n",
        "        #Compute cosine similarities\n",
        "        return 100 * self.cos_sim(image_features, text_features)\n",
        "\n",
        "    def confidence_selection(self, probs_matrix, percentile=0.8):\n",
        "        # Compute entropies for each row in the probability matrix\n",
        "        entropies = torch.tensor([self.compute_entropy(row) for row in probs_matrix])\n",
        "\n",
        "        # Sort entropies and find the threshold for the desired percentile\n",
        "        sorted_entropies, _ = torch.sort(entropies, descending=True)\n",
        "        threshold = sorted_entropies[int(len(sorted_entropies) * percentile)]\n",
        "\n",
        "        # Create a boolean mask where entropies below the threshold are selected\n",
        "        boolean_mask = entropies < threshold\n",
        "\n",
        "        # Assuming similarities is intended to be probs_matrix, return filtered matrix\n",
        "        return probs_matrix[boolean_mask]\n",
        "\n",
        "\n",
        "    def entropy_loss_MEMO(self, batch_features, text_features = None):\n",
        "        if text_features is None:\n",
        "            text_features = self.text_features\n",
        "        #Logits (unnormalized probabilities)\n",
        "        logits = self.logits(text_features, batch_features)\n",
        "        # Compute the entropy of every text caption accross all augmentations\n",
        "        marginal_entropy = self.marginal_entropy(logits)\n",
        "        return marginal_entropy\n",
        "\n",
        "    def entropy_loss_TPT(self, batch_features, text_features = None):\n",
        "        if text_features is None:\n",
        "            text_features = self.text_features\n",
        "        probs_matrix = self.class_probabilities(text_features, batch_features)\n",
        "        # Confidence selection for the augmented views:\n",
        "        probs_matrix = self.confidence_selection(probs_matrix)\n",
        "        # Average the caption probabilities across all augmentations\n",
        "        avg_probs = torch.tensor([row.mean() for row in probs_matrix.T])\n",
        "        # Compute the entropy of the averaged probability distribution\n",
        "        return self.compute_entropy(avg_probs), avg_probs\n",
        "\n",
        "    def grad_descent_step(self, loss):\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def predict(self, image):\n",
        "        self.model.eval()\n",
        "        image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            image_features = self.model.encode_image(image)\n",
        "            norms = image_features.norm(dim=-1, keepdim=True)\n",
        "            if (norms == 0).any():\n",
        "                print(\"Zero norm found in image features\")\n",
        "            image_features = image_features / norms.clamp_min(1e-10)\n",
        "\n",
        "        probs = self.class_probabilities(self.text_features, image_features)\n",
        "        prediction = torch.argmax(probs).item()\n",
        "        entropy = float(self.compute_entropy(probs))\n",
        "        return prediction, probs, entropy\n",
        "\n",
        "    def MEMO(self, image, num_augmentations=100, conf_sel = False):\n",
        "        # Save original parameters\n",
        "        original_params = {name: param.clone() for name, param in self.model.named_parameters()}\n",
        "\n",
        "        # Require gradients to update the CLIP parameters\n",
        "        self.require_CLIP_gradients(state = True)\n",
        "        try:\n",
        "            self.model.train() #DO WE WANT BATCH NORMALIZ AND DROPOUT?\n",
        "            batch = self.augment_image(image, num_augmentations)\n",
        "            batch_features = self.model.encode_image(batch)\n",
        "            norms = batch_features.norm(dim=-1, keepdim=True)\n",
        "            if (norms == 0).any():\n",
        "                print(\"Zero norm found in image features\")\n",
        "            batch_features = batch_features / norms.clamp_min(1e-10)\n",
        "\n",
        "            loss = self.entropy_loss_MEMO(batch_features)\n",
        "            self.grad_descent_step(loss)\n",
        "\n",
        "            if any(torch.isnan(param).any() for param in self.model.parameters()):\n",
        "                print(\"nan values detected in model parameters after updating\")\n",
        "            # Predict using the updated model\n",
        "            prediction, probs, entropy = self.predict(image)\n",
        "        finally:\n",
        "            # Restore original parameters\n",
        "            with torch.no_grad():\n",
        "                for name, param in self.model.named_parameters():\n",
        "                    param.copy_(original_params[name])\n",
        "        return prediction, probs, entropy\n",
        "\n",
        "    def TPT(self, image, num_augmentations=100):\n",
        "        batch = self.augment_image(image, num_augmentations)\n",
        "        batch_features = self.model.encode_image(batch)\n",
        "        norms = batch_features.norm(dim=-1, keepdim=True)\n",
        "        if (norms == 0).any():\n",
        "            print(\"Zero norm found in image features\")\n",
        "        batch_features = batch_features / norms.clamp_min(1e-10)\n",
        "\n",
        "        entropy, avg_probs = self.entropy_loss_TPT(batch_features)\n",
        "        prediction = torch.argmax(avg_probs).item()\n",
        "        return prediction, avg_probs, float(entropy)\n",
        "\n",
        "    def train_CoOp(self):\n",
        "        #Prevent CLIP parameters from changing\n",
        "        self.require_CLIP_gradients(state=False)\n",
        "\n",
        "\n",
        "# Preparing the class for usage\n",
        "clip_model = CLIPModel()"
      ],
      "metadata": {
        "id": "6XGJSO0FnKTp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 IMAGE TRIES!!"
      ],
      "metadata": {
        "id": "4yYC2YqhknDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading CIFAR100 for one image tries_\n",
        "cifar100 = clip_model.load_data()\n",
        "clip_model.tokenize_labels(cifar100.classes)\n",
        "image, class_id = cifar100[3637]\n",
        "len(cifar100.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B52IQTi6OyfW",
        "outputId": "1d2de153-942e-4cb7-b7c8-514f2cddaedb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction using CLIP out of the box\n",
        "prediction1, probs1, entropy1 = clip_model.predict(image)\n",
        "print(prediction1)\n",
        "print(entropy1)\n",
        "print(probs1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "087FNIiqpDUb",
        "outputId": "790d5c95-3d18-456a-ca05-91ee48d2d970"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78\n",
            "2.32662034034729\n",
            "tensor([[1.1976e-03, 5.4627e-03, 4.8137e-03, 1.8082e-04, 1.8419e-04, 3.4514e-04,\n",
            "         5.7728e-04, 8.2105e-03, 9.9322e-05, 3.7705e-04, 1.0402e-03, 8.2096e-04,\n",
            "         3.4284e-05, 1.0731e-04, 1.9523e-03, 1.4169e-04, 1.7230e-03, 4.6304e-05,\n",
            "         4.2152e-03, 9.5375e-04, 2.7545e-04, 1.2663e-03, 3.1629e-04, 5.2210e-05,\n",
            "         6.0814e-04, 1.6915e-04, 7.3052e-03, 1.7485e-02, 5.9205e-04, 4.7084e-03,\n",
            "         1.2483e-05, 4.3251e-04, 8.4594e-03, 1.0473e-04, 1.4937e-05, 5.7395e-04,\n",
            "         9.1999e-04, 1.1684e-04, 1.0331e-04, 2.5125e-04, 1.1974e-03, 6.8169e-03,\n",
            "         1.7165e-02, 8.7137e-04, 1.8750e-02, 1.4284e-04, 3.7087e-04, 1.1404e-04,\n",
            "         1.0234e-04, 4.6626e-04, 6.6154e-03, 2.9242e-03, 6.3653e-04, 7.6152e-04,\n",
            "         1.2287e-04, 1.4347e-04, 8.5429e-04, 1.4071e-04, 2.5624e-04, 1.5454e-03,\n",
            "         8.2368e-04, 1.7338e-03, 5.6945e-04, 3.3850e-04, 4.9928e-04, 6.9674e-04,\n",
            "         4.8177e-04, 6.3735e-04, 7.4546e-06, 1.2440e-05, 1.7305e-04, 7.2546e-05,\n",
            "         1.1135e-03, 5.2111e-05, 1.3243e-03, 2.5227e-04, 2.8802e-05, 1.5945e-02,\n",
            "         6.5313e-01, 2.8920e-03, 1.0557e-04, 6.0129e-05, 7.8726e-04, 3.8253e-02,\n",
            "         2.0946e-04, 6.4963e-03, 1.5895e-03, 2.6247e-04, 2.3473e-03, 1.1592e-04,\n",
            "         1.8628e-04, 4.3683e-04, 1.5311e-04, 1.2288e-01, 7.3359e-05, 1.7735e-05,\n",
            "         1.0799e-04, 7.8453e-05, 3.5900e-04, 8.4518e-03]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction using MEMO with SGD\n",
        "clip_model.use_SGD()\n",
        "clip_model.tokenize_labels(cifar100.classes) #You have to tokenize the labels again for the change in precision\n",
        "prediction2, probs2, entropy2 = clip_model.MEMO(image, num_augmentations=200)\n",
        "print(prediction2)\n",
        "print(entropy2)\n",
        "print(probs2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RheDQxnoeB7",
        "outputId": "e884bc27-5e6c-480b-d57f-99819b66f9d8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78\n",
            "2.3031229972839355\n",
            "tensor([[1.1555e-03, 5.5266e-03, 4.7070e-03, 1.7656e-04, 1.8022e-04, 3.4120e-04,\n",
            "         5.4227e-04, 7.7798e-03, 9.5200e-05, 3.6872e-04, 9.9392e-04, 8.0828e-04,\n",
            "         3.3263e-05, 1.0290e-04, 1.8887e-03, 1.3996e-04, 1.6361e-03, 4.4184e-05,\n",
            "         4.1856e-03, 9.4392e-04, 2.7112e-04, 1.2482e-03, 3.0046e-04, 5.0310e-05,\n",
            "         5.6624e-04, 1.6562e-04, 7.0130e-03, 1.7856e-02, 5.7408e-04, 4.6373e-03,\n",
            "         1.2445e-05, 4.3009e-04, 8.9465e-03, 1.0432e-04, 1.4850e-05, 5.6357e-04,\n",
            "         8.8066e-04, 1.1357e-04, 1.0451e-04, 2.3661e-04, 1.1586e-03, 7.0127e-03,\n",
            "         1.7192e-02, 8.4787e-04, 1.8738e-02, 1.3629e-04, 3.6719e-04, 1.1561e-04,\n",
            "         9.8545e-05, 4.4902e-04, 6.5243e-03, 2.8773e-03, 6.4266e-04, 7.2155e-04,\n",
            "         1.2300e-04, 1.4284e-04, 8.6557e-04, 1.3902e-04, 2.5246e-04, 1.5767e-03,\n",
            "         8.2791e-04, 1.7084e-03, 5.5092e-04, 3.3196e-04, 4.9347e-04, 6.8558e-04,\n",
            "         4.6992e-04, 6.4473e-04, 7.2626e-06, 1.1809e-05, 1.6351e-04, 7.2358e-05,\n",
            "         1.0745e-03, 5.2223e-05, 1.3701e-03, 2.5394e-04, 2.7736e-05, 1.5431e-02,\n",
            "         6.5828e-01, 2.7842e-03, 1.0541e-04, 5.8276e-05, 7.6055e-04, 3.7457e-02,\n",
            "         2.0571e-04, 6.2784e-03, 1.5655e-03, 2.5646e-04, 2.3018e-03, 1.0993e-04,\n",
            "         1.7401e-04, 4.4382e-04, 1.4787e-04, 1.2018e-01, 7.3260e-05, 1.7269e-05,\n",
            "         1.1033e-04, 7.6754e-05, 3.5496e-04, 8.3659e-03]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction using MEMO with ADAM\n",
        "clip_model.use_ADAM()\n",
        "clip_model.tokenize_labels(cifar100.classes) #You have to tokenize the labels again for the change in precision\n",
        "prediction3, probs3, entropy3 = clip_model.MEMO(image, num_augmentations=200)\n",
        "print(prediction3)\n",
        "print(entropy3)\n",
        "print(probs3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlTDDcr_IRYf",
        "outputId": "0c18c812-0569-45f3-e7c6-43c6d1a75660"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78\n",
            "0.8575076460838318\n",
            "tensor([[1.0673e-04, 1.2476e-03, 1.7046e-03, 1.8541e-05, 6.2944e-06, 1.0282e-04,\n",
            "         2.5938e-05, 3.0161e-04, 1.0533e-05, 4.5671e-05, 1.8914e-04, 5.4465e-04,\n",
            "         1.5232e-05, 9.3350e-06, 1.3823e-04, 3.0655e-06, 3.9640e-04, 4.6186e-06,\n",
            "         7.5120e-04, 8.1243e-05, 1.8175e-05, 3.6007e-05, 1.2801e-05, 1.1251e-05,\n",
            "         3.9845e-05, 2.3166e-05, 1.6623e-04, 1.3365e-02, 1.1611e-04, 1.9594e-03,\n",
            "         1.3046e-06, 3.7131e-05, 7.8169e-04, 7.8888e-05, 1.3339e-06, 3.4440e-04,\n",
            "         4.5947e-05, 6.3079e-05, 1.2233e-05, 7.9210e-06, 8.8096e-05, 4.9567e-03,\n",
            "         2.9440e-03, 3.7596e-05, 2.8697e-02, 5.8353e-06, 1.6224e-04, 3.3654e-05,\n",
            "         6.6215e-06, 8.0918e-05, 9.4958e-04, 1.9897e-04, 5.0031e-04, 1.1566e-04,\n",
            "         4.9600e-05, 7.9399e-06, 5.1438e-04, 2.3661e-05, 1.9431e-05, 3.4405e-03,\n",
            "         2.5177e-04, 3.9546e-04, 3.7581e-05, 3.4556e-05, 1.0131e-04, 7.1997e-05,\n",
            "         3.6074e-05, 1.2981e-04, 1.4308e-06, 1.8119e-06, 1.0959e-05, 1.6219e-05,\n",
            "         2.6474e-05, 6.8046e-06, 8.9177e-05, 6.8747e-05, 1.0270e-05, 1.3046e-03,\n",
            "         8.9377e-01, 2.8651e-04, 1.5518e-05, 7.4475e-06, 4.2068e-05, 2.6469e-02,\n",
            "         6.7609e-05, 5.6605e-04, 1.1523e-04, 5.4380e-05, 1.2813e-04, 5.8091e-06,\n",
            "         2.1696e-05, 7.9598e-05, 9.6397e-06, 7.7099e-03, 3.9824e-05, 1.3988e-06,\n",
            "         8.2370e-05, 5.2500e-06, 1.8346e-04, 2.0813e-03]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction using TPT\n",
        "prediction4,prob_avg, entropy4 = clip_model.TPT(image, num_augmentations=200)\n",
        "print(prediction4)\n",
        "print(entropy4)\n",
        "print(prob_avg)"
      ],
      "metadata": {
        "id": "nrxX4QrwsCbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTSSSS\n"
      ],
      "metadata": {
        "id": "vkg1nIszau2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "imagenetv2 = datasets.ImageFolder(root='/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenetv2')\n",
        "imageneta = datasets.ImageFolder(root='/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenet-a')"
      ],
      "metadata": {
        "id": "XHe39gn-PHrR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "1d2c21d8-dbb2-49fe-dc06-8bcd8a0de677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenetv2'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f40f99cb1355>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimagenetv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenetv2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mimageneta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenet-a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenetv2'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the class names for imagenet-A\n",
        "def classnames_imagenetA():\n",
        "    # Define the path to the words file\n",
        "    file_path = '/content/drive/MyDrive/Petaloso Project/Code/Datasets/words_imageneta.txt'\n",
        "\n",
        "    # Initialize an empty list to store the class names\n",
        "    class_names = []\n",
        "\n",
        "    # Open and read the file line by line\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            # Split each line into wnid and class name, and strip to remove any leading/trailing whitespace\n",
        "            parts = line.strip().split(' ', 1)\n",
        "            if len(parts) > 1:\n",
        "                # Append only the class name (second part) to the list\n",
        "                class_names.append(parts[1])\n",
        "    return class_names"
      ],
      "metadata": {
        "id": "_YTyv3q9V3Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing subsets\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "def create_stratified_subset(dataset, num_samples_per_class=5):\n",
        "    # Fix the random seed for reproducibility\n",
        "    torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "    # Determine class indices\n",
        "    targets = np.array([s[1] for s in dataset.samples])\n",
        "    classes, class_indices = np.unique(targets, return_inverse=True)\n",
        "\n",
        "    # Select samples from each class\n",
        "    indices = []\n",
        "    for c in classes:\n",
        "        class_idx = np.where(class_indices == c)[0]\n",
        "        if len(class_idx) >= num_samples_per_class:\n",
        "            selected_indices = np.random.choice(class_idx, num_samples_per_class, replace=False)\n",
        "            indices.extend(selected_indices)\n",
        "        else:\n",
        "            # If a class has fewer than the desired number, take all\n",
        "            indices.extend(class_idx)\n",
        "\n",
        "    # Create subset\n",
        "    subset = Subset(dataset, indices)\n",
        "    return subset\n",
        "\n",
        "subset = create_stratified_subset(imageneta, num_samples_per_class=5)"
      ],
      "metadata": {
        "id": "IGUDzNCle_E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "imageneta = datasets.ImageFolder(root='/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenet-a')\n",
        "\n",
        "def testing(dataset, model,method='CLIP', batch_size=32, num_aug=100):\n",
        "    model.tokenize_labels(classnames_imagenetA())\n",
        "\n",
        "    def custom_collate_fn(batch):\n",
        "        # Extract images and labels from the batch\n",
        "        images = [item[0] for item in batch]  # PIL images\n",
        "        labels = [item[1] for item in batch]  # Corresponding labels\n",
        "        return images, labels\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    entropies = []\n",
        "    confidences = []\n",
        "\n",
        "    # Evaluation loop\n",
        "    for images, labels in tqdm(dataloader):\n",
        "        for image, label in zip(images, labels):\n",
        "            try:\n",
        "                # We choose how the test time predictions are made\n",
        "                if method == 'CLIP':\n",
        "                    prediction, probs, entropy = model.predict(image)\n",
        "                elif method == 'MEMO':\n",
        "                    prediction, probs, entropy = model.MEMO(image, num_augmentations=num_aug)\n",
        "                elif method == 'MEMO_CONF':\n",
        "                    prediction, probs, entropy = model.MEMO(image, num_augmentations=num_aug, conf_sel = False)\n",
        "                elif method == 'TPT':\n",
        "                    prediction, probs, entropy = model.TPT(image, num_augmentations=num_aug)\n",
        "                else:\n",
        "                    print('Enter a valid method for testing.')\n",
        "\n",
        "                if int(prediction) == int(label):\n",
        "                    correct_predictions += 1\n",
        "                total_predictions += 1\n",
        "                entropies.append(entropy)\n",
        "                confidences.append(torch.max(probs).item())\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred: {e}\")\n",
        "\n",
        "    # Post evaluation statistics or processing\n",
        "    accuracy = (correct_predictions / total_predictions) * 100\n",
        "    average_entropy = sum(entropies) / len(entropies)\n",
        "    average_confidence = sum(confidences) / len(confidences)\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "    print(f'Average entropy across all predictions: {average_entropy:.2f}')\n",
        "\n",
        "\n",
        "testing(subset,clip_model, method = 'CLIP', batch_size=35)\n"
      ],
      "metadata": {
        "id": "2UPSff5cJs0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a08239-6974-4e8d-960b-bf2aab10de19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29/29 [00:18<00:00,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 31.26%\n",
            "Average entropy across all predictions: 7.64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COOP!!!"
      ],
      "metadata": {
        "id": "JaniK16Skr4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "\n",
        "_tokenizer = _Tokenizer()"
      ],
      "metadata": {
        "id": "g_IxvwpzZqKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        self.text_projection = clip_model.text_projection\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        x = prompts + self.positional_embedding\n",
        "        x = x.permute(1, 0, 2)  # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
        "        x = self.ln_final(x)\n",
        "\n",
        "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "JNC1Hxz1Zq-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
        "        super().__init__()\n",
        "        n_cls = len(classnames)\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        clip_imsize = clip_model.visual.input_resolution\n",
        "\n",
        "        # Use given words to initialize context vectors\n",
        "        # De aquí sacamos el context vector (tokenizado si lo sacamos de un cierto\n",
        "        # texto o directamente un vector random que no sale de tokenizar\n",
        "        # si lo iniciamos en plan random) y el prompt prefix que es el texto con\n",
        "        # el que comenzamos antes de entrenar, que puede ser o texto inicial\n",
        "        # con sentido o una X que no representa nada si no hay texto que inicialize\n",
        "        # solo representa la cantidad de palabras que hay\n",
        "        if ctx_init:\n",
        "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
        "            n_ctx = len(ctx_init.split(\" \"))\n",
        "            prompt = clip.tokenize(ctx_init).to(clip_model.token_embedding.weight.device)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt)\n",
        "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
        "            prompt_prefix = ctx_init\n",
        "        else:\n",
        "            if csc:\n",
        "                print(\"Initializing class-specific contexts\")\n",
        "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim)\n",
        "            else:\n",
        "                print(\"Initializing a generic context\")\n",
        "                ctx_vectors = torch.empty(n_ctx, ctx_dim)\n",
        "\n",
        "            torch.nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "\n",
        "        print(f\"Initial context: '{prompt_prefix}'\")\n",
        "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
        "\n",
        "        # These are the `prompts` we want to optimize\n",
        "        self.ctx = nn.Parameter(ctx_vectors)\n",
        "\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "\n",
        "        # print(\"+++\")\n",
        "        # print(\"Prompts:\")\n",
        "        # for p in prompts:\n",
        "        #     print(p)\n",
        "        # print(\"+++\")\n",
        "\n",
        "        # Aqui está tokenizando a partir de el prompt (las X o lo que le hayamos\n",
        "        # dado) pero no usa el context vector pa nada. Pero ese es el que nos importa\n",
        "        # así que NO ENTIENDO ESTOS PA QUE SON. PARA SACAR EL SOS, CLS y EOS tokens\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(clip_model.token_embedding.weight.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
        "\n",
        "        # These token vectors will be saved when in save_model(),\n",
        "        # but they should be ignored in load_model() as we want to use\n",
        "        # those computed using the current class names.\n",
        "        # Buffer implica que no computas el gradiente para estos tokens. Asi que\n",
        "        # son constantes!\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
        "\n",
        "        self.n_cls = n_cls\n",
        "        self.n_ctx = n_ctx\n",
        "        self.tokenized_prompts = tokenized_prompts\n",
        "        self.name_lens = name_lens\n",
        "        self.class_token_position = class_token_position\n",
        "\n",
        "    def forward(self):\n",
        "        prefix = self.token_prefix\n",
        "        suffix = self.token_suffix\n",
        "        ctx = self.ctx\n",
        "\n",
        "        # If CoOp, expand the ctx for all classes (implying a shared context across all classes)\n",
        "        if ctx.dim() == 2:\n",
        "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "\n",
        "        #Metemos el class token donde toque.\n",
        "        #PERO AQUI NO ESTÁ EL CLASS TOKEN! SII VA EN EL SUFFIX JUNTO CON EOS\n",
        "        if self.class_token_position == \"end\":\n",
        "            prompts = torch.cat(\n",
        "                [\n",
        "                    prefix,  # (n_cls, 1, dim)\n",
        "                    ctx,     # (n_cls, n_ctx, dim)\n",
        "                    suffix,  # (n_cls, *, dim)\n",
        "                ],\n",
        "                dim=1,\n",
        "            )\n",
        "\n",
        "        elif self.class_token_position == \"middle\":\n",
        "            half_n_ctx = self.n_ctx // 2\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
        "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
        "                prompt = torch.cat(\n",
        "                    [\n",
        "                        prefix_i,     # (1, 1, dim)\n",
        "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
        "                        class_i,      # (1, name_len, dim)\n",
        "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
        "                        suffix_i,     # (1, *, dim)\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        elif self.class_token_position == \"front\":\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i = ctx[i : i + 1, :, :]\n",
        "                prompt = torch.cat(\n",
        "                    [\n",
        "                        prefix_i,  # (1, 1, dim)\n",
        "                        class_i,   # (1, name_len, dim)\n",
        "                        ctx_i,     # (1, n_ctx, dim)\n",
        "                        suffix_i,  # (1, *, dim)\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "        return prompts"
      ],
      "metadata": {
        "id": "Sn0AmktyZxxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_coop()"
      ],
      "metadata": {
        "id": "p912NXY-Z8NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CoCoOp:\n",
        "    def __init__(self, CLIP_model, device=None):\n",
        "        self.device = device if device else \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "sT6wAzCrEoL9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}