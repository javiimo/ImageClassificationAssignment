{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javiimo/ImageClassificationAssignment/blob/main/CLIPClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIg1G6AD6fyQ",
        "outputId": "ec7ad4fe-ae62-4ba5-fac5-d737547a332b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m51.2/54.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.2.0\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-q5o7ts7l\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-q5o7ts7l\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.17.1+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369499 sha256=16e4a8e57136eb41ea445a29377982d4387598ab0d20f792958f610fda46e14c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-rpim_ifw/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n",
            "Successfully installed clip-1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "! pip install datasets transformers\n",
        "\n",
        "\n",
        "import clip\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6WJzQYEtTxi",
        "outputId": "0322bbd5-d299-43c7-ed63-405241409065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler"
      ],
      "metadata": {
        "id": "6uvFyM8Pr7sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPModel:\n",
        "\n",
        "    def __init__(self, model_name='ViT-B/32', device=None):\n",
        "        self.device = device if device else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model, self.preprocess = clip.load(model_name, self.device)\n",
        "        #self.model = self.convert_model_parameters_to_float32(self.model)\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
        "        self.text_features = None\n",
        "        self.requiring_grads = None\n",
        "        self.logit_scale = self.model.logit_scale\n",
        "\n",
        "    def use_ADAM(self):\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-5)\n",
        "\n",
        "    def use_SGD(self):\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
        "\n",
        "    def require_CLIP_gradients(self, state = True):\n",
        "        if self.requiring_grads is None or state != self.requiring_grads: #don't change if the state is already OK\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = state\n",
        "            self.requiring_grads = state\n",
        "\n",
        "    def convert_model_parameters_to_float32(self, model):\n",
        "        for param in model.parameters():\n",
        "            param.data = param.data.to(torch.float32)\n",
        "        return model\n",
        "\n",
        "    def load_data(self):\n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', download=True, train=False)\n",
        "        return cifar100\n",
        "\n",
        "    #This are heuristic labels\n",
        "    def tokenize_labels(self, classes):\n",
        "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classes]).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            self.text_features = self.model.encode_text(text_inputs)\n",
        "            self.text_features /= self.text_features.norm(dim=-1, p=2, keepdim=True)\n",
        "\n",
        "    def augment_image(self, image, num_augmentations=100, transformations=None):\n",
        "        torch.manual_seed(33)#Set a seed for reproducibility of the random augmentations\n",
        "        if transformations==None:\n",
        "            augmentations = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomVerticalFlip(p=0.5),\n",
        "                transforms.RandomRotation(degrees=30),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "                transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
        "            ])\n",
        "        augmented_images = [self.preprocess(image).unsqueeze(0).to(self.device)] #Add the original image to the batch of augmentations\n",
        "        for _ in range(num_augmentations):\n",
        "            augmented_images.append(self.preprocess(augmentations(image)).unsqueeze(0).to(self.device))\n",
        "        batch = torch.vstack(augmented_images)\n",
        "        return batch #(num_augumentations + 1, 3, 224, 224)\n",
        "\n",
        "    def cos_sim(self, image_features, text_features):\n",
        "        return  image_features @ text_features.T\n",
        "\n",
        "    def logits(self, text_features, image_features):\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        return logit_scale * self.cos_sim(image_features, text_features)\n",
        "\n",
        "    def class_probabilities(self, text_features, image_features):\n",
        "        #Compute cosine similarities\n",
        "        return  self.logits(text_features, image_features).softmax(dim=-1)\n",
        "\n",
        "    def marginal_entropy(self, logits):\n",
        "        z = logits - logits.logsumexp(dim = -1, keepdim=True) # compute z_ij\n",
        "        marginal_logp = z.logsumexp(dim=0) - np.log(z.shape[0])   # compute marginal log probabilities\n",
        "\n",
        "        min_real = torch.finfo(marginal_logp.dtype).min # for numerical stability,\n",
        "        # the smallest representable number given the dtype of logits.\n",
        "        avg_logits = torch.clamp(marginal_logp, min = min_real)  # put a threshold to avoid underflow\n",
        "\n",
        "        return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
        "\n",
        "    def compute_entropy(self, x): #Shanon entropy in bits\n",
        "        #This computes the Shanon entropy\n",
        "        log_x = torch.log2(x.clamp_min(1e-20))\n",
        "        entropy = -torch.sum(x * log_x)\n",
        "        return entropy\n",
        "\n",
        "    def confidence_selection(self, probs_matrix, percentile=0.8):\n",
        "        # Compute entropies for each row in the probability matrix\n",
        "        entropies = torch.tensor([self.compute_entropy(row) for row in probs_matrix])\n",
        "\n",
        "        # Find the threshold for the desired percentile\n",
        "        threshold = torch.quantile(entropies, percentile, interpolation = 'linear')\n",
        "\n",
        "        # Create a boolean mask where entropies below the threshold are selected\n",
        "        boolean_mask = entropies < threshold\n",
        "\n",
        "        # Assuming similarities is intended to be probs_matrix, return filtered matrix\n",
        "        return probs_matrix[boolean_mask]\n",
        "\n",
        "    def entropy_loss_MEMO(self, batch_features, text_features = None):\n",
        "        if text_features is None:\n",
        "            text_features = self.text_features\n",
        "        #Logits (unnormalized probabilities)\n",
        "        logits = self.logits(text_features, batch_features)\n",
        "        # Compute the entropy of every text caption accross all augmentations\n",
        "        marginal_entropy = self.marginal_entropy(logits)\n",
        "        return marginal_entropy\n",
        "\n",
        "    def entropy_loss_TPT(self, batch_features, text_features = None):\n",
        "        if text_features is None:\n",
        "            text_features = self.text_features\n",
        "        probs_matrix = self.class_probabilities(text_features, batch_features)\n",
        "        # Confidence selection for the augmented views:\n",
        "        probs_matrix = self.confidence_selection(probs_matrix)\n",
        "        # Average the caption probabilities across all augmentations\n",
        "        avg_probs = torch.tensor([row.mean() for row in probs_matrix.T])\n",
        "        # Compute the entropy of the averaged probability distribution\n",
        "        return self.compute_entropy(avg_probs), avg_probs\n",
        "\n",
        "    def forward(self, image):\n",
        "        image_features = self.model.encode_image(image)\n",
        "        norms = image_features.norm(dim=-1, p=2,  keepdim=True)\n",
        "        if (norms == 0).any():\n",
        "            print(\"Zero norm found in image features\")\n",
        "        image_features = image_features / norms.clamp_min(1e-10)\n",
        "        return self.class_probabilities(self.text_features, image_features)\n",
        "\n",
        "    def predict(self, image):\n",
        "        self.model.eval()\n",
        "        image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            probs = self.forward(image)\n",
        "\n",
        "        prediction = torch.argmax(probs).item()\n",
        "        entropy = float(self.compute_entropy(probs))\n",
        "        return prediction, probs, entropy\n",
        "\n",
        "    def grad_descent_step(self, loss):\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def MEMO(self, image, num_augmentations=100):\n",
        "        # Save original parameters\n",
        "        original_params = {name: param.clone() for name, param in self.model.named_parameters()}\n",
        "\n",
        "        # Require gradients to update the CLIP parameters\n",
        "        self.require_CLIP_gradients(state = True)\n",
        "\n",
        "        scaler = GradScaler(growth_interval=1000)  # Initialize GradScaler for mixed precision\n",
        "        try:\n",
        "            self.model.train()\n",
        "            batch = self.augment_image(image, num_augmentations)\n",
        "\n",
        "            with autocast(device_type='cuda', dtype=torch.float16):  # Enable automatic mixed precision\n",
        "                batch_features = self.model.encode_image(batch)\n",
        "                norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
        "                if (norms == 0).any():\n",
        "                    print(\"Zero norm found in image features\")\n",
        "                batch_features = batch_features / norms.clamp_min(1e-10)\n",
        "                loss = self.entropy_loss_MEMO(batch_features)\n",
        "            #self.grad_descent_step(loss)\n",
        "            scaler.scale(loss).backward()  # Scale the loss and compute scaled gradients\n",
        "            scaler.step(self.optimizer)    # Update weights\n",
        "            scaler.update()                # Update the scale for next iteration\n",
        "            self.optimizer.zero_grad()     # Clear gradients after update\n",
        "\n",
        "            if any(torch.isnan(param).any() for param in self.model.parameters()):\n",
        "                print(\"nan values detected in model parameters after updating\")\n",
        "            # Predict using the updated model\n",
        "            prediction, probs, entropy = self.predict(image)\n",
        "        finally:\n",
        "            # Restore original parameters\n",
        "            with torch.no_grad():\n",
        "                for name, param in self.model.named_parameters():\n",
        "                    param.copy_(original_params[name])\n",
        "        return prediction, probs, entropy\n",
        "\n",
        "    def TPT(self, image, num_augmentations=100):\n",
        "        batch = self.augment_image(image, num_augmentations)\n",
        "        batch_features = self.model.encode_image(batch)\n",
        "        norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
        "        if (norms == 0).any():\n",
        "            print(\"Zero norm found in image features\")\n",
        "        batch_features = batch_features / norms.clamp_min(1e-10)\n",
        "\n",
        "        entropy, avg_probs = self.entropy_loss_TPT(batch_features)\n",
        "        prediction = torch.argmax(avg_probs).item()\n",
        "        return prediction, avg_probs, float(entropy)\n",
        "\n",
        "\n",
        "# Preparing the class for usage\n",
        "clip_model = CLIPModel()"
      ],
      "metadata": {
        "id": "6XGJSO0FnKTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f97a486-48f4-48ca-e713-83c576ebcd03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:04<00:00, 74.7MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 IMAGE TRIES!!"
      ],
      "metadata": {
        "id": "4yYC2YqhknDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading CIFAR100 for one image tries_\n",
        "cifar100 = clip_model.load_data()\n",
        "clip_model.tokenize_labels(cifar100.classes)\n",
        "image, class_id = cifar100[3637]\n",
        "len(cifar100.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B52IQTi6OyfW",
        "outputId": "d8e52605-ea10-48f4-e56e-776e5a2b1c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:10<00:00, 15564992.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction using CLIP out of the box\n",
        "prediction1, probs1, entropy1 = clip_model.predict(image)\n",
        "print(prediction1)\n",
        "print(entropy1)\n",
        "print(probs1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "087FNIiqpDUb",
        "outputId": "157fb454-8aae-497a-b5c6-aecf4b3d4569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78\n",
            "2.3266122341156006\n",
            "tensor([[1.1976e-03, 5.4628e-03, 4.8137e-03, 1.8082e-04, 1.8419e-04, 3.4514e-04,\n",
            "         5.7728e-04, 8.2104e-03, 9.9321e-05, 3.7704e-04, 1.0402e-03, 8.2096e-04,\n",
            "         3.4284e-05, 1.0731e-04, 1.9523e-03, 1.4169e-04, 1.7230e-03, 4.6303e-05,\n",
            "         4.2151e-03, 9.5373e-04, 2.7544e-04, 1.2663e-03, 3.1628e-04, 5.2210e-05,\n",
            "         6.0814e-04, 1.6915e-04, 7.3051e-03, 1.7485e-02, 5.9205e-04, 4.7085e-03,\n",
            "         1.2483e-05, 4.3250e-04, 8.4594e-03, 1.0473e-04, 1.4937e-05, 5.7395e-04,\n",
            "         9.1999e-04, 1.1684e-04, 1.0331e-04, 2.5124e-04, 1.1974e-03, 6.8168e-03,\n",
            "         1.7164e-02, 8.7137e-04, 1.8750e-02, 1.4284e-04, 3.7086e-04, 1.1404e-04,\n",
            "         1.0234e-04, 4.6625e-04, 6.6154e-03, 2.9241e-03, 6.3652e-04, 7.6153e-04,\n",
            "         1.2287e-04, 1.4347e-04, 8.5426e-04, 1.4071e-04, 2.5623e-04, 1.5453e-03,\n",
            "         8.2367e-04, 1.7338e-03, 5.6944e-04, 3.3850e-04, 4.9928e-04, 6.9673e-04,\n",
            "         4.8177e-04, 6.3735e-04, 7.4545e-06, 1.2440e-05, 1.7305e-04, 7.2545e-05,\n",
            "         1.1135e-03, 5.2110e-05, 1.3243e-03, 2.5227e-04, 2.8801e-05, 1.5946e-02,\n",
            "         6.5313e-01, 2.8920e-03, 1.0557e-04, 6.0129e-05, 7.8725e-04, 3.8252e-02,\n",
            "         2.0946e-04, 6.4962e-03, 1.5895e-03, 2.6247e-04, 2.3472e-03, 1.1592e-04,\n",
            "         1.8628e-04, 4.3682e-04, 1.5311e-04, 1.2288e-01, 7.3359e-05, 1.7735e-05,\n",
            "         1.0799e-04, 7.8452e-05, 3.5900e-04, 8.4517e-03]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction using MEMO with SGD\n",
        "clip_model.use_SGD()\n",
        "prediction2, probs2, entropy2 = clip_model.MEMO(image, num_augmentations=10)\n",
        "print(prediction2)\n",
        "print(entropy2)\n",
        "print(probs2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "_RheDQxnoeB7",
        "outputId": "7b03df7f-81bc-4555-c017-377d44e5b3c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "autocast.__init__() got an unexpected keyword argument 'device_type'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c70bc6fa73da>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Prediction using MEMO with SGD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_SGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprediction2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMEMO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_augmentations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropy2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-2f5a5ba208a7>\u001b[0m in \u001b[0;36mMEMO\u001b[0;34m(self, image, num_augmentations)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugment_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_augmentations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Enable automatic mixed precision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m                 \u001b[0mbatch_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mnorms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: autocast.__init__() got an unexpected keyword argument 'device_type'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction using MEMO with ADAM\n",
        "clip_model.use_ADAM()\n",
        "clip_model.tokenize_labels(cifar100.classes) #You have to tokenize the labels again for the change in precision\n",
        "prediction3, probs3, entropy3 = clip_model.MEMO(image, num_augmentations=10)\n",
        "print(prediction3)\n",
        "print(entropy3)\n",
        "print(probs3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlTDDcr_IRYf",
        "outputId": "8a58029d-da53-4ed3-96a2-5a039d91d2ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "78\n",
            "0.3890048563480377\n",
            "tensor([[7.0291e-05, 1.2202e-03, 2.0966e-03, 2.8387e-05, 4.4689e-06, 2.8498e-05,\n",
            "         3.2856e-05, 6.2306e-05, 1.0148e-05, 4.9513e-05, 2.7819e-04, 5.3239e-04,\n",
            "         6.4909e-06, 6.8182e-06, 9.8106e-05, 1.4464e-06, 4.3893e-04, 2.5695e-06,\n",
            "         4.9707e-04, 5.9818e-05, 5.5518e-06, 6.2155e-05, 4.5030e-06, 1.0935e-05,\n",
            "         5.5434e-06, 4.1206e-06, 2.6603e-05, 4.4462e-04, 3.8518e-04, 2.5397e-04,\n",
            "         1.2974e-06, 1.2624e-05, 4.2114e-05, 4.5163e-05, 2.7994e-06, 4.2294e-04,\n",
            "         7.2895e-05, 4.4254e-05, 1.0838e-05, 1.8192e-06, 3.2609e-05, 2.0624e-03,\n",
            "         5.1894e-04, 3.0728e-05, 3.5041e-03, 1.8276e-06, 1.2709e-04, 2.3962e-05,\n",
            "         5.4501e-06, 1.4678e-05, 1.1937e-03, 3.1152e-05, 1.7511e-04, 5.6660e-05,\n",
            "         2.8462e-05, 1.3722e-05, 4.6307e-05, 2.8665e-06, 9.7663e-06, 8.9210e-04,\n",
            "         2.8619e-04, 3.3893e-04, 2.8192e-06, 5.9116e-06, 1.3063e-04, 8.1568e-05,\n",
            "         3.1866e-05, 1.4796e-05, 5.0939e-07, 1.6153e-06, 1.7927e-05, 1.1527e-05,\n",
            "         8.4457e-06, 4.9185e-06, 1.6388e-04, 6.4259e-04, 8.2301e-06, 1.8448e-04,\n",
            "         9.5956e-01, 1.1634e-04, 1.8333e-05, 6.7579e-06, 2.8254e-05, 1.6869e-02,\n",
            "         4.2035e-05, 1.1157e-04, 7.6166e-05, 1.6402e-05, 2.6695e-04, 5.7433e-06,\n",
            "         6.3032e-06, 7.7762e-05, 5.5851e-06, 1.5060e-04, 1.2123e-05, 7.9851e-07,\n",
            "         7.1924e-05, 1.1627e-05, 1.2605e-04, 4.3243e-03]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction using TPT\n",
        "prediction4,prob_avg, entropy4 = clip_model.TPT(image, num_augmentations=10)\n",
        "print(prediction4)\n",
        "print(entropy4)\n",
        "print(prob_avg)"
      ],
      "metadata": {
        "id": "nrxX4QrwsCbv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "828b2345-5334-418b-b40b-444910986132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59\n",
            "5.61142635345459\n",
            "tensor([0.0064, 0.0020, 0.0094, 0.0159, 0.0024, 0.0015, 0.0036, 0.0047, 0.0037,\n",
            "        0.0079, 0.0029, 0.0115, 0.0021, 0.0032, 0.0041, 0.0024, 0.0094, 0.0036,\n",
            "        0.0035, 0.0114, 0.0044, 0.0107, 0.0029, 0.0023, 0.0009, 0.0038, 0.0035,\n",
            "        0.0242, 0.0036, 0.0294, 0.0014, 0.0065, 0.0031, 0.0114, 0.0015, 0.0182,\n",
            "        0.0032, 0.0038, 0.0027, 0.0075, 0.0080, 0.0273, 0.0121, 0.0070, 0.0103,\n",
            "        0.0014, 0.0164, 0.0213, 0.0034, 0.0102, 0.0039, 0.0064, 0.0574, 0.0015,\n",
            "        0.0009, 0.0005, 0.0391, 0.0023, 0.0041, 0.1248, 0.0108, 0.0077, 0.0039,\n",
            "        0.0030, 0.0048, 0.0042, 0.0057, 0.0013, 0.0009, 0.0124, 0.0019, 0.0014,\n",
            "        0.0031, 0.0034, 0.0014, 0.0026, 0.0052, 0.0044, 0.1003, 0.0022, 0.0020,\n",
            "        0.0020, 0.0149, 0.0114, 0.0022, 0.0200, 0.0051, 0.0162, 0.0090, 0.0066,\n",
            "        0.0070, 0.0077, 0.0038, 0.0256, 0.0022, 0.0030, 0.0308, 0.0060, 0.0206,\n",
            "        0.0081])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TESTSSSS\n"
      ],
      "metadata": {
        "id": "vkg1nIszau2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "#imagenetv2 = datasets.ImageFolder(root='/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenetv2')\n",
        "#imageneta = datasets.ImageFolder(root='/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenet-a')"
      ],
      "metadata": {
        "id": "XHe39gn-PHrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the class names for imagenet-A\n",
        "def classnames_imagenetA():\n",
        "    # Define the path to the words file\n",
        "    file_path = '/content/drive/MyDrive/Petaloso Project/Code/Datasets/words_imageneta.txt'\n",
        "\n",
        "    # Initialize an empty list to store the class names\n",
        "    class_names = []\n",
        "\n",
        "    # Open and read the file line by line\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            # Split each line into wnid and class name, and strip to remove any leading/trailing whitespace\n",
        "            parts = line.strip().split(' ', 1)\n",
        "            if len(parts) > 1:\n",
        "                # Append only the class name (second part) to the list\n",
        "                class_names.append(parts[1])\n",
        "    return class_names"
      ],
      "metadata": {
        "id": "_YTyv3q9V3Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing subsets\n",
        "import numpy as np\n",
        "from torch.utils.data import Subset\n",
        "def create_stratified_subset(dataset, num_samples_per_class=5):\n",
        "    # Fix the random seed for reproducibility\n",
        "    torch.manual_seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "    # Determine class indices\n",
        "    targets = np.array([s[1] for s in dataset.samples])\n",
        "    classes, class_indices = np.unique(targets, return_inverse=True)\n",
        "\n",
        "    # Select samples from each class\n",
        "    indices = []\n",
        "    for c in classes:\n",
        "        class_idx = np.where(class_indices == c)[0]\n",
        "        if len(class_idx) >= num_samples_per_class:\n",
        "            selected_indices = np.random.choice(class_idx, num_samples_per_class, replace=False)\n",
        "            indices.extend(selected_indices)\n",
        "        else:\n",
        "            # If a class has fewer than the desired number, take all\n",
        "            indices.extend(class_idx)\n",
        "\n",
        "    # Create subset\n",
        "    subset = Subset(dataset, indices)\n",
        "    return subset\n",
        "\n",
        "subset = create_stratified_subset(imageneta, num_samples_per_class=5)"
      ],
      "metadata": {
        "id": "IGUDzNCle_E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "imageneta = datasets.ImageFolder(root='/content/drive/MyDrive/Petaloso Project/Code/Datasets/imagenet-a')\n",
        "\n",
        "def testing(dataset, model,method='CLIP', batch_size=32, num_aug=100):\n",
        "    model.tokenize_labels(classnames_imagenetA())\n",
        "\n",
        "    def custom_collate_fn(batch):\n",
        "        # Extract images and labels from the batch\n",
        "        images = [item[0] for item in batch]  # PIL images\n",
        "        labels = [item[1] for item in batch]  # Corresponding labels\n",
        "        return images, labels\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "    entropies = []\n",
        "    confidences = []\n",
        "\n",
        "    # Evaluation loop\n",
        "    for images, labels in tqdm(dataloader):\n",
        "        for image, label in zip(images, labels):\n",
        "            try:\n",
        "                # We choose how the test time predictions are made\n",
        "                if method == 'CLIP':\n",
        "                    prediction, probs, entropy = model.predict(image)\n",
        "                elif method == 'MEMO':\n",
        "                    prediction, probs, entropy = model.MEMO(image, num_augmentations=num_aug)\n",
        "                elif method == 'MEMO_CONF':\n",
        "                    prediction, probs, entropy = model.MEMO(image, num_augmentations=num_aug, conf_sel = False)\n",
        "                elif method == 'TPT':\n",
        "                    prediction, probs, entropy = model.TPT(image, num_augmentations=num_aug)\n",
        "                else:\n",
        "                    print('Enter a valid method for testing.')\n",
        "\n",
        "                if int(prediction) == int(label):\n",
        "                    correct_predictions += 1\n",
        "                total_predictions += 1\n",
        "                entropies.append(entropy)\n",
        "                confidences.append(torch.max(probs).item())\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred: {e}\")\n",
        "\n",
        "    # Post evaluation statistics or processing\n",
        "    accuracy = (correct_predictions / total_predictions) * 100\n",
        "    average_entropy = sum(entropies) / len(entropies)\n",
        "    average_confidence = sum(confidences) / len(confidences)\n",
        "    print(f'Accuracy: {accuracy:.2f}%')\n",
        "    print(f'Average entropy across all predictions: {average_entropy:.2f}')\n",
        "\n",
        "\n",
        "testing(subset,clip_model, method = 'CLIP', batch_size=35)\n"
      ],
      "metadata": {
        "id": "2UPSff5cJs0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a08239-6974-4e8d-960b-bf2aab10de19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29/29 [00:18<00:00,  1.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 31.26%\n",
            "Average entropy across all predictions: 7.64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COOP!!!"
      ],
      "metadata": {
        "id": "JaniK16Skr4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "\n",
        "_tokenizer = _Tokenizer()"
      ],
      "metadata": {
        "id": "g_IxvwpzZqKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        self.text_projection = clip_model.text_projection\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        x = prompts + self.positional_embedding\n",
        "        x = x.permute(1, 0, 2)  # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
        "        x = self.ln_final(x)\n",
        "\n",
        "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "JNC1Hxz1Zq-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, clip_model, classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
        "        super().__init__()\n",
        "        n_cls = len(classnames)\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        clip_imsize = clip_model.visual.input_resolution\n",
        "\n",
        "        # Use given words to initialize context vectors\n",
        "        # De aquí sacamos el context vector (tokenizado si lo sacamos de un cierto\n",
        "        # texto o directamente un vector random que no sale de tokenizar\n",
        "        # si lo iniciamos en plan random) y el prompt prefix que es el texto con\n",
        "        # el que comenzamos antes de entrenar, que puede ser o texto inicial\n",
        "        # con sentido o una X que no representa nada si no hay texto que inicialize\n",
        "        # solo representa la cantidad de palabras que hay\n",
        "        if ctx_init:\n",
        "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
        "            n_ctx = len(ctx_init.split(\" \"))\n",
        "            prompt = clip.tokenize(ctx_init).to(clip_model.token_embedding.weight.device)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt)\n",
        "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
        "            prompt_prefix = ctx_init\n",
        "        else:\n",
        "            if csc:\n",
        "                print(\"Initializing class-specific contexts\")\n",
        "                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim)\n",
        "            else:\n",
        "                print(\"Initializing a generic context\")\n",
        "                ctx_vectors = torch.empty(n_ctx, ctx_dim)\n",
        "\n",
        "            torch.nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "\n",
        "        print(f\"Initial context: '{prompt_prefix}'\")\n",
        "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
        "\n",
        "        # These are the `prompts` we want to optimize\n",
        "        self.ctx = nn.Parameter(ctx_vectors)\n",
        "\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "\n",
        "        # print(\"+++\")\n",
        "        # print(\"Prompts:\")\n",
        "        # for p in prompts:\n",
        "        #     print(p)\n",
        "        # print(\"+++\")\n",
        "\n",
        "        # Aqui está tokenizando a partir de el prompt (las X o lo que le hayamos\n",
        "        # dado) pero no usa el context vector pa nada. Pero ese es el que nos importa\n",
        "        # así que NO ENTIENDO ESTOS PA QUE SON. PARA SACAR EL SOS, CLS y EOS tokens\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(clip_model.token_embedding.weight.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
        "\n",
        "        # These token vectors will be saved when in save_model(),\n",
        "        # but they should be ignored in load_model() as we want to use\n",
        "        # those computed using the current class names.\n",
        "        # Buffer implica que no computas el gradiente para estos tokens. Asi que\n",
        "        # son constantes!\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
        "\n",
        "        self.n_cls = n_cls\n",
        "        self.n_ctx = n_ctx\n",
        "        self.tokenized_prompts = tokenized_prompts\n",
        "        self.name_lens = name_lens\n",
        "        self.class_token_position = class_token_position\n",
        "\n",
        "    def forward(self):\n",
        "        prefix = self.token_prefix\n",
        "        suffix = self.token_suffix\n",
        "        ctx = self.ctx\n",
        "\n",
        "        # If CoOp, expand the ctx for all classes (implying a shared context across all classes)\n",
        "        if ctx.dim() == 2:\n",
        "            ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "\n",
        "        #Metemos el class token donde toque.\n",
        "        #PERO AQUI NO ESTÁ EL CLASS TOKEN! SII VA EN EL SUFFIX JUNTO CON EOS\n",
        "        if self.class_token_position == \"end\":\n",
        "            prompts = torch.cat(\n",
        "                [\n",
        "                    prefix,  # (n_cls, 1, dim)\n",
        "                    ctx,     # (n_cls, n_ctx, dim)\n",
        "                    suffix,  # (n_cls, *, dim)\n",
        "                ],\n",
        "                dim=1,\n",
        "            )\n",
        "\n",
        "        elif self.class_token_position == \"middle\":\n",
        "            half_n_ctx = self.n_ctx // 2\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]\n",
        "                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]\n",
        "                prompt = torch.cat(\n",
        "                    [\n",
        "                        prefix_i,     # (1, 1, dim)\n",
        "                        ctx_i_half1,  # (1, n_ctx//2, dim)\n",
        "                        class_i,      # (1, name_len, dim)\n",
        "                        ctx_i_half2,  # (1, n_ctx//2, dim)\n",
        "                        suffix_i,     # (1, *, dim)\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        elif self.class_token_position == \"front\":\n",
        "            prompts = []\n",
        "            for i in range(self.n_cls):\n",
        "                name_len = self.name_lens[i]\n",
        "                prefix_i = prefix[i : i + 1, :, :]\n",
        "                class_i = suffix[i : i + 1, :name_len, :]\n",
        "                suffix_i = suffix[i : i + 1, name_len:, :]\n",
        "                ctx_i = ctx[i : i + 1, :, :]\n",
        "                prompt = torch.cat(\n",
        "                    [\n",
        "                        prefix_i,  # (1, 1, dim)\n",
        "                        class_i,   # (1, name_len, dim)\n",
        "                        ctx_i,     # (1, n_ctx, dim)\n",
        "                        suffix_i,  # (1, *, dim)\n",
        "                    ],\n",
        "                    dim=1,\n",
        "                )\n",
        "                prompts.append(prompt)\n",
        "            prompts = torch.cat(prompts, dim=0)\n",
        "\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "        return prompts"
      ],
      "metadata": {
        "id": "Sn0AmktyZxxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_coop()"
      ],
      "metadata": {
        "id": "p912NXY-Z8NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CoCoOp:\n",
        "    def __init__(self, CLIP_model, device=None):\n",
        "        self.device = device if device else \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "sT6wAzCrEoL9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}