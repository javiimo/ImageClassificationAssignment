{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "boYeQ1N3oM32",
        "oB5guLVxuxNA",
        "ic68jtmU0L3r"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Refactoring"
      ],
      "metadata": {
        "id": "er_b3ECBhwbY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Imports"
      ],
      "metadata": {
        "id": "yN7gaByPhwn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "! pip install datasets transformers\n",
        "! pip install seaborn\n",
        "! pip install nltk\n",
        "\n",
        "# Used for CLIP:\n",
        "import clip\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#Used for testing:\n",
        "#from pathlib import Path\n",
        "# from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "# from dataclasses import dataclass, asdict\n",
        "# from typing import List, Dict\n",
        "# import os\n",
        "# import json\n",
        "# from torch.utils.data import Subset\n",
        "import torch.nn.functional as F\n",
        "from torchvision.transforms.functional import to_tensor, to_pil_image\n",
        "import torch.nn as nn\n",
        "\n",
        "# for Dictionary\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "#Used for visualizing results\n",
        "import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "# import seaborn as sns\n",
        "# from ipywidgets import widgets, interactive_output, Dropdown, Output, VBox, Button\n",
        "# from IPython.display import display\n",
        "\n",
        "\n",
        "# for adaptive temperature\n",
        "from scipy.optimize import fsolve\n",
        "from scipy.special import softmax"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZEQ3tC5h5So",
        "outputId": "d12d3f86-5f42-43ec-801a-5eabe5cb574a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.2.0\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-kvlk6aom\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-kvlk6aom\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.18.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=ab836b20e6e3a7929a2e091d6cdf93357233bacbf42dbee063012d8e4feacc0c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t9941ial/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n",
            "Successfully installed clip-1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests>=2.32.2 (from datasets)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.0.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.5. Inizialize dataset and CLIP"
      ],
      "metadata": {
        "id": "ZtzA1-wiDiS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cifar100 = torchvision.datasets.CIFAR100(root= './data', download = True, train = False)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# in order to avoid problems with MEMO backpropagation step, all parameters of the model are set to torch.float32\n",
        "for param in model.parameters(): param.data = param.data.to(torch.float32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn5JUSB1nDLD",
        "outputId": "9d189946-dffc-44e7-c904-29ee836103a8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:04<00:00, 34285558.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:04<00:00, 79.4MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Useful functions\n",
        "\n",
        "Function not specific to a single class, but useful for every environment"
      ],
      "metadata": {
        "id": "hYlt95VGkC8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class model_methods:\n",
        "\n",
        "  def get_entropy(self, logits):\n",
        "\n",
        "    z = logits - logits.logsumexp(dim = -1, keepdim=True)     # compute z_ij\n",
        "    marginal_logp = z.logsumexp(dim=0) - np.log(z.shape[0])   # compute marginal log probabilities\n",
        "\n",
        "    min_real = torch.finfo(marginal_logp.dtype).min           # for numerical stability, the smallest representable number given the dtype of logits.\n",
        "    avg_logits = torch.clamp(marginal_logp, min = min_real)   # put a threshold to avoid underflow\n",
        "\n",
        "    return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
        "\n",
        "\n",
        "  def get_augmentations(self, image, num_augmentations = 50, transformations = None, manual_seed = None):\n",
        "\n",
        "    if transformations == None:\n",
        "        #Set a seed for reproducibility of the random augmentations\n",
        "        if manual_seed != None: torch.manual_seed(manual_seed)\n",
        "\n",
        "        transformations = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomVerticalFlip(p=0.5),\n",
        "            transforms.RandomRotation(degrees=30),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "            transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
        "        ])\n",
        "\n",
        "    augmented_images = [self.preprocess(image).unsqueeze(0).to(self.device)] #Add the original image to the batch of augmentations\n",
        "    for _ in range(num_augmentations):\n",
        "        augmented_images.append(self.preprocess(transformations(image)).unsqueeze(0).to(self.device))\n",
        "\n",
        "    batch = torch.vstack(augmented_images)\n",
        "    return batch #(num_augumentations + 1, 3, 224, 224)\n",
        "\n",
        "\n",
        "  def get_synonyms(self, word):\n",
        "\n",
        "    synonyms = set()\n",
        "\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name().lower())\n",
        "\n",
        "    return synonyms\n",
        "\n",
        "  # Get definitions\n",
        "  def get_definitions(self, word):\n",
        "    definitions = []\n",
        "\n",
        "    for syn in wordnet.synsets(word):\n",
        "      definitions.append(syn.definition())\n",
        "\n",
        "    return definitions\n",
        "\n",
        "\n",
        "  def get_temperature(x, beta = 0.5 + np.log(2 * np.pi * np.exp(1))):\n",
        "\n",
        "    n = x.shape[0]\n",
        "    x_np = x.cpu().numpy()\n",
        "\n",
        "    x_max = np.max(x_np)\n",
        "\n",
        "\n",
        "    def entropy(tau):\n",
        "\n",
        "      y = x_np * tau\n",
        "      p = softmax(y)\n",
        "\n",
        "      den = np.ones(n) + y + y**2 / 2 + y**3 / 6 + y**4 / 24 + y**5/120\n",
        "      num = y * den\n",
        "\n",
        "      H = - np.sum(num) / np.sum(den) + tau * x_max + np.log(n) / 2\n",
        "      return H - beta\n",
        "\n",
        "    initial_guess = 100.0  # Initial guess for tau\n",
        "    root = fsolve(entropy, initial_guess)\n",
        "\n",
        "    return torch.tensor(root).to(device)\n"
      ],
      "metadata": {
        "id": "3W5ov1Dtkx-g"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Clip class"
      ],
      "metadata": {
        "id": "MDSyAqQDiHRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class clip_model(model_methods):\n",
        "\n",
        "  def __init__(self, model, preprocess, device = \"cuda\" if torch.cuda.is_available() else \"cpu\", dataset = None):\n",
        "\n",
        "    self.device = device\n",
        "    self.model = model\n",
        "    self.preprocess = preprocess\n",
        "    self.text_features = None\n",
        "\n",
        "    if dataset != None: self.get_text_features(dataset)\n",
        "\n",
        "  def get_text_features(self, dataset):\n",
        "\n",
        "    text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in dataset.classes]).to(self.device)\n",
        "    with torch.no_grad(): self.text_features = self.model.encode_text(text_inputs)\n",
        "\n",
        "    self.text_features /= self.text_features.norm(dim=-1, p = 2, keepdim=True)\n",
        "\n",
        "\n",
        "  def get_image_features(self, image):\n",
        "\n",
        "    image_prep = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      image_features = self.model.encode_image(image_prep) # image features contains the embeddings of all the elements of the class.\n",
        "      image_features = image_features / image_features.norm(dim = -1, keepdim = True)\n",
        "\n",
        "    return image_features\n",
        "\n",
        "  def get_candidates(self, reference, candidates, num_candidates):\n",
        "\n",
        "    logits = reference @ candidates.T\n",
        "    _, label_indeces = torch.topk(logits, num_candidates)\n",
        "    candidates = candidates[label_indeces].squeeze()\n",
        "\n",
        "    return candidates, label_indeces.squeeze()\n",
        "\n",
        "\n",
        "  def get_closest_features(self, target, candidates, num_candidates):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        tokens = torch.cat([clip.tokenize(c) for c in candidates]).to(device)\n",
        "\n",
        "        features = model.encode_text(tokens)\n",
        "        features /= features.norm(dim = -1, p = 2, keepdim=True)\n",
        "\n",
        "    picked_features, _ = self.get_candidates(target, features, num_candidates = num_candidates)\n",
        "    return picked_features\n",
        "\n",
        "\n",
        "  def get_prob(self, tensor1, tensor2, custom_temp = None, adaptive_temp = False):\n",
        "\n",
        "    logits = tensor1 @ tensor2.T\n",
        "\n",
        "    if adaptive_temp: temp = self.get_temperature(logits)\n",
        "    else:\n",
        "      temp = self.model.logit_scale.exp() if custom_temp == None else custom_temp\n",
        "\n",
        "    logits = temp * logits\n",
        "\n",
        "    return logits.softmax(dim = -1).squeeze()\n",
        "\n",
        "\n",
        "\n",
        "  def __call__(self, image, custom_temp = None):\n",
        "\n",
        "    self.model.eval()\n",
        "\n",
        "    image_features = self.get_image_features(image)\n",
        "    prob = self.get_prob(image_features, self.text_features)\n",
        "\n",
        "\n",
        "    predicted = torch.argmax(prob)\n",
        "    entropy = self.get_entropy(prob).item()\n",
        "\n",
        "\n",
        "    return predicted.item(), prob, entropy"
      ],
      "metadata": {
        "id": "-VevYQpqi5VC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLIP - sanity check"
      ],
      "metadata": {
        "id": "boYeQ1N3oM32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_clip = clip_model(model, preprocess, device, dataset = cifar100)"
      ],
      "metadata": {
        "id": "B8-L1VlOql8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1\n",
        "correct_clip = 0\n",
        "\n",
        "\n",
        "for i in tqdm(range(n)):\n",
        "\n",
        "  image, label = cifar100[i]\n",
        "  predicted_label, _, entropy = base_clip(image)\n",
        "\n",
        "  if predicted_label == label: correct_clip += 1\n",
        "\n",
        "print(f'Correct: {correct_clip}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyyVIEYJoLjM",
        "outputId": "b4d7eda7-dca3-4292-cb52-b8e76ac25a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  2.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. MEMO class"
      ],
      "metadata": {
        "id": "oB5guLVxuxNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MEMO_model(clip_model):\n",
        "\n",
        "  def __init__(self, model, preprocess, device = \"cuda\" if torch.cuda.is_available() else \"cpu\", dataset = None,\n",
        "               optimizer = 'SGD', num_augmentations = 50, lr = 1e-5, momentum = 0.9):\n",
        "\n",
        "    super().__init__(model, preprocess, device, dataset)\n",
        "\n",
        "    self.optimizer = None\n",
        "    self.optimizer_type = optimizer\n",
        "\n",
        "    self.num_augmentations = num_augmentations\n",
        "\n",
        "    self.lr = lr\n",
        "    self.momentum = momentum\n",
        "\n",
        "\n",
        "  def use_ADAM(self): self.optimizer = optim.Adam(self.model.parameters(), lr = self.lr)\n",
        "  def use_SGD(self): self.optimizer = optim.SGD(self.model.parameters(), lr = self.lr, momentum = self.momentum)\n",
        "\n",
        "\n",
        "  def set_optimizer(self, s):\n",
        "\n",
        "    if   s == 'ADAM': self.use_ADAM()\n",
        "    elif s == 'SGD' : self.use_SGD()\n",
        "    else: print('wrong input')\n",
        "\n",
        "\n",
        "  def require_model_gradients(self, state = True):\n",
        "    for param in self.model.parameters(): param.requires_grad = state\n",
        "\n",
        "\n",
        "  def entropy_loss_MEMO(self, logits):\n",
        "\n",
        "      z = logits - logits.logsumexp(dim = -1, keepdim=True)     # compute z_ij\n",
        "      marginal_logp = z.logsumexp(dim=0) - np.log(z.shape[0])   # compute marginal log probabilities\n",
        "\n",
        "      min_real = torch.finfo(marginal_logp.dtype).min          # for numerical stability, The smallest representable number given the dtype of logits.\n",
        "      avg_logits = torch.clamp(marginal_logp, min = min_real)  # put a threshold to avoid underflow\n",
        "\n",
        "      return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
        "\n",
        "\n",
        "  def backprop_sweep(self, image):\n",
        "\n",
        "    self.set_optimizer(self.optimizer_type)\n",
        "\n",
        "    try:\n",
        "\n",
        "      self.model.train(True)\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "      batch = self.get_augmentations(image, num_augmentations = self.num_augmentations, manual_seed = 33)\n",
        "\n",
        "      batch_features = self.model.encode_image(batch)\n",
        "      batch_features = batch_features / batch_features.norm(dim = -1, p = 2, keepdim = True)\n",
        "      logits = 100.0 * batch_features @ self.text_features.T\n",
        "\n",
        "      loss = self.entropy_loss_MEMO(logits)\n",
        "\n",
        "      loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "      print(f\"Exception{type(e).__name__} occurred. Details: {e.args}\")\n",
        "\n",
        "\n",
        "\n",
        "  def __call__(self, image):\n",
        "\n",
        "      original_params = {name: param.clone().detach() for name, param in self.model.named_parameters()}\n",
        "\n",
        "      self.require_model_gradients(state = True) # Require gradients to update the CLIP parameters\n",
        "      self.backprop_sweep(image)\n",
        "\n",
        "      self.require_model_gradients(state = False)\n",
        "\n",
        "      predicted, distribution, entropy = super().__call__(image, custom_temp = 100.0)\n",
        "\n",
        "      # Restore original parameters\n",
        "      with torch.no_grad():\n",
        "\n",
        "        for name, param in self.model.named_parameters():\n",
        "          param.copy_(original_params[name])\n",
        "\n",
        "\n",
        "      return predicted, distribution, entropy"
      ],
      "metadata": {
        "id": "TzsApLyiu1Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MEMO - sanity check"
      ],
      "metadata": {
        "id": "G6kwerrV8j4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "memo = MEMO_model(model, preprocess, device, num_augmentations = 100, dataset = cifar100)"
      ],
      "metadata": {
        "id": "73c801Mg8iex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 0\n",
        "correct = 0\n",
        "\n",
        "for i in tqdm(range(n)):\n",
        "\n",
        "  image, label = cifar100[i]\n",
        "  predicted_label, _, entropy = memo(image)\n",
        "\n",
        "  if predicted_label == label: correct += 1\n",
        "\n",
        "print(f'Correct: {correct}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1pQ3VDS8wQd",
        "outputId": "c3ca3b48-8e57-4455-f34f-a59ef883d483"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. EB class"
      ],
      "metadata": {
        "id": "ic68jtmU0L3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class eb_model(clip_model):\n",
        "\n",
        "  # __init__ same as clip\n",
        "  def __init__(self, model, preprocess, device = \"cuda\" if torch.cuda.is_available() else \"cpu\", dataset = None,\n",
        "               num_augmentations = 50, num_pick_augmenations = 15, num_candidates = 5, max_iter = 15):\n",
        "\n",
        "    super().__init__(model, preprocess, device, dataset)\n",
        "\n",
        "    self.num_augmentations = num_augmentations\n",
        "    self.num_pick_augmenations = num_pick_augmenations\n",
        "    self.num_candidates = num_candidates\n",
        "    self.max_iter = max_iter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def get_and_pick_augmentations(self, image, image_features, text_candidates):\n",
        "\n",
        "    transformations = transforms.Compose([transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333))])\n",
        "    batch = self.get_augmentations(image, num_augmentations = self.num_augmentations, transformations = transformations)\n",
        "\n",
        "    aug_img_features = model.encode_image(batch)\n",
        "    aug_img_features = aug_img_features / aug_img_features.norm(dim = -1, p = 2, keepdim = True)\n",
        "\n",
        "    # pick the ones closest to the original image\n",
        "    similarities_aug = image_features @ aug_img_features.T\n",
        "    _, pick_indeces = torch.topk(similarities_aug, self.num_pick_augmenations)\n",
        "\n",
        "    picked_aug_features = torch.squeeze(aug_img_features[pick_indeces])\n",
        "    aug_probs = self.get_prob(picked_aug_features, text_candidates)\n",
        "\n",
        "    aug_prob = aug_probs.mean(dim = 0).squeeze()\n",
        "\n",
        "    return aug_prob\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def __call__(self, image):\n",
        "\n",
        "    image_features = self.get_image_features(image)\n",
        "    text_candidates, label_indeces = self.get_candidates(image_features, self.text_features, self.num_candidates)\n",
        "\n",
        "    img_prob = self.get_prob(image_features, text_candidates)\n",
        "    aug_prob = None\n",
        "    output_prob = img_prob.clone()\n",
        "\n",
        "    running_predict = torch.argmax(output_prob)\n",
        "    iter = 0\n",
        "    while iter < self.max_iter:\n",
        "\n",
        "      new_aug_prob = self.get_and_pick_augmentations(image, image_features, text_candidates)\n",
        "\n",
        "      if aug_prob == None: aug_prob = new_aug_prob\n",
        "      else:                aug_prob = 0.5 * aug_prob + 0.5 * new_aug_prob\n",
        "\n",
        "      output_prob = 0.6 * img_prob + 0.4 * aug_prob # give a little more weight to the probability of the image.\n",
        "\n",
        "      predict_step = torch.argmax(output_prob)\n",
        "\n",
        "      # if running_predict != predict_step then it changed its mind: the class with highest probability has changed - better do further checks.\n",
        "      if running_predict == predict_step: break\n",
        "      running_predict = predict_step\n",
        "\n",
        "      iter += 1\n",
        "\n",
        "\n",
        "    predicted = label_indeces[predict_step].item()\n",
        "    entropy = self.get_entropy(output_prob).item()\n",
        "\n",
        "    return predicted, output_prob, entropy"
      ],
      "metadata": {
        "id": "7zSrtEWo1cbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EB - sanity check"
      ],
      "metadata": {
        "id": "bnpXgaQmxkjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eb = eb_model(model, preprocess, device, dataset = cifar100)"
      ],
      "metadata": {
        "id": "E8VBfrYl5Yjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 100\n",
        "correct_eb = 0\n",
        "\n",
        "for i in tqdm(range(n)):\n",
        "\n",
        "  image, label = cifar100[i]\n",
        "  predicted_eb, _, entropy_eb = eb(image)\n",
        "\n",
        "  if predicted_eb == label:   correct_eb += 1\n",
        "\n",
        "\n",
        "print(f'Correct: {correct_eb}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt4_EKRS5efr",
        "outputId": "37abfb26-120d-44cb-ccb5-aa7cf9b5e835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:30<00:00,  3.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tmp."
      ],
      "metadata": {
        "id": "w039Hwo4v7ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class features_tmp(clip_model):\n",
        "\n",
        "  def __init__(self, model, preprocess, device = \"cuda\" if torch.cuda.is_available() else \"cpu\", dataset = None,\n",
        "              num_candidates = 10, num_synonyms = 5):\n",
        "\n",
        "    super().__init__(model, preprocess, device, dataset)\n",
        "    self.classes = dataset.classes\n",
        "    self.num_candidates = num_candidates\n",
        "\n",
        "    self.num_synonyms = np.min([num_synonyms, 5])\n",
        "    if num_synonyms > 6: print(\"num_synonims at most 5. Set to 5.\")\n",
        "\n",
        "    self.descript_features = torch.empty(len(self.classes), self.num_synonyms + 1, 512).to(device)\n",
        "    if self.text_features != None: self.get_descript_features()\n",
        "\n",
        "\n",
        "  def get_descript_features(self):\n",
        "\n",
        "    # hard coded data augmentation over the synonyms to have them above a minimum threshold, as some classes don't have synonims.\n",
        "    phrase_patterns = [ 'a photo of: ',\n",
        "      'a picture of a ', 'a picture of: ',\n",
        "      'an image of a ', 'an image of: '\n",
        "    ]\n",
        "\n",
        "    for i in tqdm(range(len(self.classes))):\n",
        "\n",
        "      c = self.classes[i]\n",
        "\n",
        "      words = c.split('_')\n",
        "\n",
        "      synonyms = []\n",
        "      definitions = []\n",
        "\n",
        "      for word in words:\n",
        "        definitions.extend(self.get_definitions(word))\n",
        "        synonyms.extend(self.get_synonyms(word))\n",
        "\n",
        "      strings = [f'{pattern}{s}' for s in synonyms for pattern in phrase_patterns]\n",
        "\n",
        "      synonyms_features = self.get_closest_features(target = self.text_features[i], candidates = strings, num_candidates = self.num_synonyms)\n",
        "      context_feature   = self.get_closest_features(target = self.text_features[i], candidates = definitions, num_candidates = 1)\n",
        "\n",
        "      self.descript_features[i, :, :] = torch.cat([synonyms_features, context_feature.unsqueeze(0)])"
      ],
      "metadata": {
        "id": "dEgXOHU_vbgW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Dictionary, Adaptive temp"
      ],
      "metadata": {
        "id": "aZon52H30tyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DAT_model(clip_model):\n",
        "\n",
        "  def __init__(self, features, model, preprocess, device = \"cuda\" if torch.cuda.is_available() else \"cpu\", dataset = None,\n",
        "               num_candidates = 10, num_synonyms = 5):\n",
        "\n",
        "    super().__init__(model, preprocess, device, dataset)\n",
        "    self.classes = dataset.classes\n",
        "    self.num_candidates = num_candidates\n",
        "\n",
        "    self.num_synonyms = np.min([num_synonyms, 5])\n",
        "    if num_synonyms > 5: print(\"num_synonims at most 5. Set to 5.\")\n",
        "\n",
        "    self.descript_features = torch.empty(len(self.classes), self.num_synonyms + 1, 512).to(device)\n",
        "    self.proj_matrices = torch.empty(len(self.classes), 512, 512).to(device)\n",
        "\n",
        "    if self.text_features != None:\n",
        "      self.get_descript_features()\n",
        "      # self.descript_features = features\n",
        "      self.get_proj_matrix()\n",
        "\n",
        "\n",
        "  def get_descript_features(self):\n",
        "\n",
        "    # hard coded data augmentation over the synonyms to have them above a minimum threshold, as some classes don't have synonims.\n",
        "    phrase_patterns = [ 'a photo of: ',\n",
        "      'a picture of a ', 'a picture of: ',\n",
        "      'an image of a ', 'an image of: '\n",
        "    ]\n",
        "\n",
        "    for i in tqdm(range(len(self.classes))):\n",
        "\n",
        "      c = self.classes[i]\n",
        "\n",
        "      words = c.split('_')\n",
        "\n",
        "      synonyms = []\n",
        "      definitions = []\n",
        "\n",
        "      for word in words:\n",
        "        definitions.extend(self.get_definitions(word))\n",
        "        synonyms.extend(self.get_synonyms(word))\n",
        "\n",
        "      strings = [f'{pattern}{s}' for s in synonyms for pattern in phrase_patterns]\n",
        "\n",
        "      synonyms_features = self.get_closest_features(target = self.text_features[i], candidates = strings, num_candidates = self.num_synonyms)\n",
        "      context_feature   = self.get_closest_features(target = self.text_features[i], candidates = definitions, num_candidates = 1)\n",
        "\n",
        "      self.descript_features[i, :, :] = torch.cat([synonyms_features, context_feature.unsqueeze(0)])\n",
        "\n",
        "\n",
        "  def get_proj_matrix(self):\n",
        "\n",
        "    for i in range(len(self.classes)):\n",
        "\n",
        "      Y = self.descript_features[i, :, :]\n",
        "      X = Y.to(torch.float32).T\n",
        "\n",
        "      # Projection matrix: X (X^T X)^-1 X^T\n",
        "\n",
        "      gram_inv = torch.linalg.inv( torch.matmul(X.T, X))\n",
        "      P = torch.matmul( torch.matmul(X, gram_inv), X.T)\n",
        "\n",
        "      self.proj_matrices[i, :, :] = P\n",
        "\n",
        "\n",
        "  def __call__(self, image):\n",
        "\n",
        "\n",
        "    image_features = self.get_image_features(image)\n",
        "    text_candidates, label_indeces = self.get_candidates(image_features, self.text_features, self.num_candidates)\n",
        "\n",
        "    projections = []\n",
        "\n",
        "    for i in range(self.num_candidates):\n",
        "\n",
        "      P = self.proj_matrices[label_indeces[i], :, :]\n",
        "\n",
        "      project_img = torch.matmul(P, image_features.T.to(torch.float32))\n",
        "      projections.append( project_img.norm(dim = 0, p = 2, keepdim = True))\n",
        "\n",
        "\n",
        "    img_prob = self.get_prob(image_features, self.text_features[label_indeces])\n",
        "\n",
        "    descript_logits = 100.0 * torch.vstack(projections).squeeze().to(device)\n",
        "    descript_prob = torch.nn.functional.softmax(descript_logits, dim = 0)\n",
        "\n",
        "    output_prob = img_prob * 0.6 + descript_prob * 0.4\n",
        "\n",
        "    predicted_candidate = torch.argmax(output_prob)\n",
        "    predicted = label_indeces[predicted_candidate]\n",
        "\n",
        "    return predicted.item()"
      ],
      "metadata": {
        "id": "5b9tE6CuWdZJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = features_tmp(model, preprocess, num_synonyms = 5, dataset = cifar100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gInJL93wN4A",
        "outputId": "8b1c4c49-ed44-4cce-e59d-55eaded147d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [11:33<00:00,  6.94s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dat = DAT_model(features.descript_features, model, preprocess, num_synonyms = 3, dataset = cifar100)"
      ],
      "metadata": {
        "id": "71jCAge1aeNm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "706d8769-bdde-4b9f-98fe-30e8b6483840"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7f407b3b6c90>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDAT_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescript_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_synonyms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcifar100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = 100\n",
        "correct_dat = 0\n",
        "\n",
        "for i in tqdm(range(n)):\n",
        "\n",
        "  image, label = cifar100[i]\n",
        "  if label == dat(image): correct_dat += 1\n",
        "\n",
        "\n",
        "print(f'Correct: {correct_dat}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0OtIYbUbTx2",
        "outputId": "cd7c7822-3ba1-48f0-f792-842232ae5c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 79.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct: 65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "Qiftm7WW8cAP",
        "outputId": "b292eb14-1127-4adf-d294-ab5f8349ee7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-18bf08814031>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_emptyCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.reset_peak_memory_stats()"
      ],
      "metadata": {
        "id": "4ycxUQOUAXxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uaLO9fQoad9y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Original Clip class"
      ],
      "metadata": {
        "id": "ZQYRP5hmnAc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPModel:\n",
        "\n",
        "    def __init__(self, model_name='ViT-B/32', device=None):\n",
        "        self.device = device if device else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model, self.preprocess = clip.load(model_name, self.device)\n",
        "        self.model = self.convert_model_parameters_to_float32(self.model)\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
        "        self.text_features = None\n",
        "        self.requiring_grads = None\n",
        "        self.logit_scale = self.model.logit_scale #temperature parameter learned by CLIP\n",
        "        self.changeseed = 0 #This is to be able to do diverse random transforms but replicable\n",
        "\n",
        "    def use_ADAM(self):\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-5)\n",
        "\n",
        "    def use_SGD(self):\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
        "\n",
        "    def require_CLIP_gradients(self, state = True):\n",
        "        if self.requiring_grads is None or state != self.requiring_grads: #don't change if the state is already OK\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = state\n",
        "            self.requiring_grads = state\n",
        "\n",
        "    def convert_model_parameters_to_float32(self, model):\n",
        "        for param in model.parameters():\n",
        "            param.data = param.data.to(torch.float32)\n",
        "        return model\n",
        "\n",
        "    def load_data(self):\n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', download=True, train=False)\n",
        "        return cifar100\n",
        "\n",
        "    #This are heuristic labels\n",
        "    def tokenize_labels(self, classes):\n",
        "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classes]).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            self.text_features = self.model.encode_text(text_inputs)\n",
        "            self.text_features /= self.text_features.norm(dim=-1, p=2, keepdim=True)\n",
        "\n",
        "    def augment_image(self, image, num_augmentations=100, transformations=None):\n",
        "        if transformations==None:\n",
        "            torch.manual_seed(33)#Set a seed for reproducibility of the random augmentations\n",
        "            augmentations = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomVerticalFlip(p=0.5),\n",
        "                transforms.RandomRotation(degrees=30),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "                transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
        "            ])\n",
        "        augmented_images = [self.preprocess(image).unsqueeze(0).to(self.device)] #Add the original image to the batch of augmentations\n",
        "        for _ in range(num_augmentations):\n",
        "            augmented_images.append(self.preprocess(augmentations(image)).unsqueeze(0).to(self.device))\n",
        "        batch = torch.vstack(augmented_images)\n",
        "        return batch #(num_augumentations + 1, 3, 224, 224)\n",
        "\n",
        "    def cos_sim(self, image_features, text_features):\n",
        "        return  image_features @ text_features.T\n",
        "\n",
        "    def logits(self, image_features, text_features):\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "        return logit_scale * self.cos_sim(image_features, text_features)\n",
        "\n",
        "    def class_probabilities(self, image_features, text_features):\n",
        "        #Compute cosine similarities\n",
        "        return  self.logits(image_features, text_features).softmax(dim=-1)\n",
        "\n",
        "    def marginal_entropy(self, logits):\n",
        "        z = logits - logits.logsumexp(dim = -1, keepdim=True)     # compute z_ij\n",
        "        marginal_logp = z.logsumexp(dim=0) - np.log(z.shape[0])   # compute marginal log probabilities\n",
        "\n",
        "        min_real = torch.finfo(marginal_logp.dtype).min           # for numerical stability, the smallest representable number given the dtype of logits.\n",
        "        avg_logits = torch.clamp(marginal_logp, min = min_real)   # put a threshold to avoid underflow\n",
        "\n",
        "        return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
        "\n",
        "    def compute_entropy(self, x): #Shanon entropy in bits\n",
        "        #This computes the Shanon entropy\n",
        "        log_x = torch.log2(x.clamp_min(1e-20))\n",
        "        entropy = -torch.sum(x * log_x)\n",
        "        return entropy\n",
        "\n",
        "    def confidence_selection(self, probs_matrix, percentile=0.8):\n",
        "        # Compute entropies for each row in the probability matrix\n",
        "        entropies = torch.tensor([self.compute_entropy(row) for row in probs_matrix])\n",
        "\n",
        "        # Find the threshold for the desired percentile\n",
        "        threshold = torch.quantile(entropies, percentile, interpolation = 'linear')\n",
        "\n",
        "        # Create a boolean mask where entropies below the threshold are selected\n",
        "        boolean_mask = entropies < threshold\n",
        "\n",
        "        # Assuming similarities is intended to be probs_matrix, return filtered matrix\n",
        "        return probs_matrix[boolean_mask]\n",
        "\n",
        "    def entropy_loss_MEMO(self, batch_features, text_features = None):\n",
        "        if text_features is None:\n",
        "            text_features = self.text_features\n",
        "        #Logits (unnormalized probabilities)\n",
        "        logits = self.logits(batch_features, text_features)\n",
        "        # Compute the entropy of every text caption accross all augmentations\n",
        "        marginal_entropy = self.marginal_entropy(logits)\n",
        "        return marginal_entropy\n",
        "\n",
        "    def entropy_loss_TPT(self, batch_features, text_features = None):\n",
        "        if text_features is None:\n",
        "            text_features = self.text_features\n",
        "        probs_matrix = self.class_probabilities(batch_features, text_features)\n",
        "        # Confidence selection for the augmented views:\n",
        "        probs_matrix = self.confidence_selection(probs_matrix)\n",
        "        # Average the caption probabilities across all augmentations\n",
        "        avg_probs = torch.tensor([row.mean() for row in probs_matrix.T])\n",
        "        # Compute the entropy of the averaged probability distribution\n",
        "        return self.compute_entropy(avg_probs), avg_probs\n",
        "\n",
        "    def forward(self, image):\n",
        "        image_features = self.model.encode_image(image)\n",
        "        norms = image_features.norm(dim=-1, p=2,  keepdim=True)\n",
        "        if (norms == 0).any():\n",
        "            print(\"Zero norm found in image features\")\n",
        "        image_features = image_features / norms.clamp_min(1e-10)\n",
        "        return self.class_probabilities(image_features, self.text_features)\n",
        "\n",
        "    def predict(self, image):\n",
        "        self.model.eval()\n",
        "        image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            probs = self.forward(image)\n",
        "\n",
        "        prediction = torch.argmax(probs).item()\n",
        "        entropy = float(self.compute_entropy(probs))\n",
        "        return prediction, probs.squeeze(), entropy\n",
        "\n",
        "    def grad_descent_step(self, loss):\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def MEMO(self, image, num_augmentations=100):\n",
        "        # Save original parameters\n",
        "        original_params = {name: param.clone() for name, param in self.model.named_parameters()}\n",
        "\n",
        "        # Require gradients to update the CLIP parameters\n",
        "        self.require_CLIP_gradients(state = True)\n",
        "\n",
        "        try:\n",
        "            self.model.train()\n",
        "            batch = self.augment_image(image, num_augmentations)\n",
        "            batch_features = self.model.encode_image(batch)\n",
        "            norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
        "            if (norms == 0).any():\n",
        "                print(\"Zero norm found in image features\")\n",
        "            batch_features = batch_features / norms.clamp_min(1e-10)\n",
        "            loss = self.entropy_loss_MEMO(batch_features)\n",
        "            self.grad_descent_step(loss)\n",
        "\n",
        "            if any(torch.isnan(param).any() for param in self.model.parameters()):\n",
        "                print(\"nan values detected in model parameters after updating\")\n",
        "            # Predict using the updated model\n",
        "            prediction, probs, entropy = self.predict(image)\n",
        "        finally:\n",
        "            # Restore original parameters\n",
        "            with torch.no_grad():\n",
        "                for name, param in self.model.named_parameters():\n",
        "                    param.copy_(original_params[name])\n",
        "        return prediction, probs.squeeze(), entropy\n",
        "\n",
        "    def TPT(self, image, num_augmentations=100):\n",
        "        self.model.eval()\n",
        "        self.require_CLIP_gradients(False)\n",
        "        batch = self.augment_image(image, num_augmentations)\n",
        "        batch_features = self.model.encode_image(batch)\n",
        "        norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
        "        if (norms == 0).any():\n",
        "            print(\"Zero norm found in image features\")\n",
        "        batch_features = batch_features / norms.clamp_min(1e-10)\n",
        "\n",
        "        entropy, avg_probs = self.entropy_loss_TPT(batch_features)\n",
        "        prediction = torch.argmax(avg_probs).item()\n",
        "        return prediction, avg_probs.squeeze(), float(entropy)\n",
        "\n",
        "    #Implementing the Entropy Boost stuff:\n",
        "    def pick_candidates(self, tensor, classifier, top_num):\n",
        "        # to select a subset of \"candidates\" from a given tensor based on scores provided by a classifier\n",
        "        _, top_indices = torch.topk(classifier, top_num)\n",
        "        candidates = torch.squeeze(tensor[top_indices])\n",
        "\n",
        "        return candidates, top_indices\n",
        "\n",
        "    def expand_tensor(self, tensor, top_indices, n):\n",
        "\n",
        "        exp_tensor = torch.zeros(n).to(self.device)\n",
        "        for i in range(top_indices.shape[0]): exp_tensor[top_indices[i]] = tensor[i]\n",
        "\n",
        "        return exp_tensor\n",
        "\n",
        "    def generate_augmentations_similarities(self, image, image_features, txt_candidates, num_augmentations, top_aug_num):\n",
        "        # augment\n",
        "        torch.manual_seed(33+self.changeseed)\n",
        "        augmentations = transforms.Compose([\n",
        "                                transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
        "                                ])\n",
        "        batch = self.augment_image(image, num_augmentations)\n",
        "        batch_features = self.model.encode_image(batch)\n",
        "        norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
        "        if (norms == 0).any():\n",
        "            print(\"Zero norm found in image features\")\n",
        "        batch_features = batch_features / norms.clamp_min(1e-10)\n",
        "\n",
        "\n",
        "        # pick the ones closest to the original image\n",
        "        probs_matrix = self.class_probabilities(image_features, batch_features).squeeze()\n",
        "        candidates_features, top_indices_aug = self.pick_candidates(batch_features, probs_matrix, top_num = top_aug_num)\n",
        "\n",
        "        candidates_probs_matrix = self.class_probabilities(candidates_features, txt_candidates).squeeze()\n",
        "\n",
        "        del batch # avoid Cuda to run out of memory?\n",
        "        return torch.mean(candidates_probs_matrix, dim=0)\n",
        "\n",
        "    def boost_augmentations(self, image, image_features, prob, num_augmentations, num_candidates, top_aug_num):\n",
        "\n",
        "        _, top_indices = torch.topk(prob, num_candidates)\n",
        "        text_candidates = self.text_features[top_indices]\n",
        "\n",
        "        n = prob.shape[0]\n",
        "\n",
        "        #Candidates probabilities in the original image\n",
        "        candidates_prob_og = self.class_probabilities(image_features, text_candidates).squeeze()\n",
        "        candidates_prob_og = self.expand_tensor(candidates_prob_og, top_indices, n)\n",
        "\n",
        "        #Candidates averaged probability in the augmentations\n",
        "        candidates_avg_prob = self.generate_augmentations_similarities(image, image_features, text_candidates, num_augmentations, top_aug_num)\n",
        "        candidates_avg_prob = self.expand_tensor(candidates_avg_prob, top_indices, n)\n",
        "\n",
        "        return candidates_prob_og, candidates_avg_prob\n",
        "\n",
        "    def entropyboosting(self, image, num_augmentations = 100, num_candidates = 10, top_aug_num = 5):\n",
        "        self.model.eval()\n",
        "        self.require_CLIP_gradients(False)\n",
        "        if num_candidates <= 1:\n",
        "            print(\"If num_candidates=0 this is just CLIP\")\n",
        "            return\n",
        "        image_prepro = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "        image_features = self.model.encode_image(image_prepro)\n",
        "        norms = image_features.norm(dim=-1, p=2,  keepdim=True)\n",
        "        image_features = image_features / norms.clamp_min(1e-10)\n",
        "\n",
        "        #Compute out of the box CLIP probability distribution and prediction\n",
        "        clip_probs = self.class_probabilities(image_features, self.text_features).squeeze()\n",
        "        clip_prediction = torch.argmax(clip_probs).item()\n",
        "\n",
        "        phi = 2 / (1 + np.sqrt(5)) # Aura section - math fetish but it seems to work so...\n",
        "\n",
        "        # Defining loop variables\n",
        "        aug_prob = None #Should this be an input of the method??\n",
        "        output_prob = clip_probs.clone() #Is this necessary??\n",
        "        max_iter = 4\n",
        "        iter = 0\n",
        "        while iter < max_iter:\n",
        "            self.changeseed = self.changeseed + 1 #To make different random transformations at every iter but replicable\n",
        "            candidates_prob_og, candidates_avg_prob = self.boost_augmentations(image, image_features, output_prob, num_augmentations, num_candidates, top_aug_num)\n",
        "\n",
        "            clip_probs = phi * clip_probs + (1 - phi) * candidates_prob_og #Why are we doing this to the clip probs?\n",
        "\n",
        "            if aug_prob == None: aug_prob = candidates_avg_prob\n",
        "            else:                aug_prob = 0.5 * aug_prob + 0.5 * candidates_avg_prob\n",
        "\n",
        "            output_prob = 0.6 * clip_probs + 0.4 * aug_prob # give a little more weight to the probability of the image.\n",
        "            EB_prediction = torch.argmax(output_prob).item()\n",
        "\n",
        "            # if clip_prediction != EB_prediction then it changed its mind: the class with highest probability has changed - better do further checks.\n",
        "            if clip_prediction == EB_prediction: break\n",
        "\n",
        "            iter += 1\n",
        "        self.changeseed = 0\n",
        "        entropy = self.compute_entropy(output_prob)\n",
        "        return EB_prediction, output_prob.squeeze(), float(entropy)\n",
        "\n",
        "\n",
        "# Preparing the class for usage\n",
        "clip_model = CLIPModel()"
      ],
      "metadata": {
        "id": "MDwbZK8-iQx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Good afternoon :)"
      ],
      "metadata": {
        "id": "73Iwny9WTXvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cifar100():\n",
        "    cifar100 = torchvision.datasets.CIFAR100(root='./data', download=True, train=False)\n",
        "    return cifar100\n",
        "\n",
        "def cos_sim(image_features, text_features):\n",
        "    return  image_features @ text_features.T\n",
        "\n",
        "def logits(image_features, text_features, logit_scale):\n",
        "    logit_scale = logit_scale.exp()\n",
        "    return logit_scale * cos_sim(image_features, text_features)\n",
        "\n",
        "def class_probabilities(image_features, text_features, logit_scale):\n",
        "    return  logits(image_features, text_features, logit_scale).softmax(dim=-1)\n",
        "\n",
        "def marginal_entropy(logits):\n",
        "    z = logits - logits.logsumexp(dim = -1, keepdim=True)     # compute z_ij\n",
        "    marginal_logp = z.logsumexp(dim=0) - np.log(z.shape[0])   # compute marginal log probabilities\n",
        "\n",
        "    min_real = torch.finfo(marginal_logp.dtype).min           # for numerical stability, the smallest representable number given the dtype of logits.\n",
        "    avg_logits = torch.clamp(marginal_logp, min = min_real)   # put a threshold to avoid underflow\n",
        "\n",
        "    return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
        "\n",
        "def compute_entropy(x): #Shanon entropy in bits\n",
        "    log_x = torch.log2(x.clamp_min(1e-20))\n",
        "    entropy = -torch.sum(x * log_x)\n",
        "    return entropy\n",
        "\n",
        "class CLIPModel:\n",
        "\n",
        "    def __init__(self, model_name='ViT-B/32', device=None):\n",
        "        self.device = device if device else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model, self.preprocess = clip.load(model_name, self.device)\n",
        "        self.model = self.convert_model_parameters_to_float32(self.model)\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
        "        self.text_features = None\n",
        "        self.requiring_grads = None\n",
        "        self.scale = self.model.logit_scale #temperature parameter learned by CLIP\n",
        "        self.changeseed = 0 #This is to be able to do diverse random transforms but replicable\n",
        "\n",
        "    def use_ADAM(self):\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-5)\n",
        "\n",
        "    def use_SGD(self):\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
        "\n",
        "    def require_CLIP_gradients(self, state = True):\n",
        "        if self.requiring_grads is None or state != self.requiring_grads: #don't change if the state is already OK\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = state\n",
        "            self.requiring_grads = state\n",
        "\n",
        "    def convert_model_parameters_to_float32(self, model):\n",
        "        for param in model.parameters():\n",
        "            param.data = param.data.to(torch.float32)\n",
        "        return model\n",
        "\n",
        "    def tokenize_labels(self, classes): #We use heuristic labels\n",
        "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classes]).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            self.text_features = self.model.encode_text(text_inputs)\n",
        "            self.text_features /= self.text_features.norm(dim=-1, p=2, keepdim=True)\n",
        "\n",
        "    def grad_descent_step(self, loss):\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def forward(self, image):\n",
        "        image_features = self.model.encode_image(image)\n",
        "        norms = image_features.norm(dim=-1, p=2,  keepdim=True)\n",
        "        if (norms == 0).any():\n",
        "            print(\"Zero norm found in image features\")\n",
        "        image_features = image_features / norms.clamp_min(1e-10)\n",
        "        return class_probabilities(image_features, self.text_features, self.scale)\n",
        "\n",
        "    def predict(self, image):\n",
        "        self.model.eval()\n",
        "        image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            probs = self.forward(image)\n",
        "\n",
        "        prediction = torch.argmax(probs).item()\n",
        "        entropy = float(compute_entropy(probs))\n",
        "        return prediction, probs.squeeze(), entropy\n",
        "\n"
      ],
      "metadata": {
        "id": "MwMFbupTTiPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MEMO:\n",
        "\n",
        "    def __init__(self, CLIP):\n",
        "        self.clip = CLIP\n",
        "\n",
        "    def augment_image(self, image, num_augmentations=100, transformations=None):\n",
        "        if transformations==None:\n",
        "            torch.manual_seed(33)#Set a seed for reproducibility of the random augmentations\n",
        "            augmentations = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomVerticalFlip(p=0.5),\n",
        "                transforms.RandomRotation(degrees=30),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "                transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
        "            ])\n",
        "        augmented_images = [self.clip.preprocess(image).unsqueeze(0).to(self.clip.device)] #Add the original image to the batch of augmentations\n",
        "        for _ in range(num_augmentations):\n",
        "            augmented_images.append(self.clip.preprocess(augmentations(image)).unsqueeze(0).to(self.clip.device))\n",
        "        batch = torch.vstack(augmented_images)\n",
        "        return batch #(num_augumentations + 1, 3, 224, 224)\n",
        "\n",
        "    def entropy_loss_MEMO(self, batch_features, text_features = None):\n",
        "        if text_features is None:\n",
        "            text_features = self.clip.text_features\n",
        "        #Logits (unnormalized probabilities)\n",
        "        logits = logits(batch_features, text_features, self.clip.scale)\n",
        "        # Compute the entropy of every text caption accross all augmentations\n",
        "        marginal_entropy = marginal_entropy(logits)\n",
        "        return marginal_entropy\n",
        "\n",
        "    def MEMO(self, image, num_augmentations=100):\n",
        "        # Save original parameters\n",
        "        original_params = {name: param.clone() for name, param in self.clip.model.named_parameters()}\n",
        "\n",
        "        # Require gradients to update the CLIP parameters\n",
        "        self.clip.require_CLIP_gradients(state = True)\n",
        "\n",
        "        try:\n",
        "            self.clip..model.train()\n",
        "            batch = self.augment_image(image, num_augmentations)\n",
        "            batch_features = self.clip.model.encode_image(batch)\n",
        "            norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
        "            if (norms == 0).any():\n",
        "                print(\"Zero norm found in image features\")\n",
        "            batch_features = batch_features / norms.clamp_min(1e-10)\n",
        "            loss = self.entropy_loss_MEMO(batch_features)\n",
        "            self.clip.grad_descent_step(loss)\n",
        "\n",
        "            if any(torch.isnan(param).any() for param in self.clip.model.parameters()):\n",
        "                print(\"nan values detected in model parameters after updating\")\n",
        "            # Predict using the updated model\n",
        "            prediction, probs, entropy = self.clip.predict(image)\n",
        "        finally:\n",
        "            # Restore original parameters\n",
        "            with torch.no_grad():\n",
        "                for name, param in self.clip.model.named_parameters():\n",
        "                    param.copy_(original_params[name])\n",
        "        return prediction, probs.squeeze(), entropy\n"
      ],
      "metadata": {
        "id": "LYIwyq8xWpCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TPT:\n",
        "\n",
        "    def __init__(self, CLIP):\n",
        "        self.clip = CLIP\n",
        "\n",
        "    def augment_image(self, image, num_augmentations=100, transformations=None):\n",
        "        if transformations==None:\n",
        "            torch.manual_seed(33)#Set a seed for reproducibility of the random augmentations\n",
        "            augmentations = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomVerticalFlip(p=0.5),\n",
        "                transforms.RandomRotation(degrees=30),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "                transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
        "            ])\n",
        "        augmented_images = [self.clip.preprocess(image).unsqueeze(0).to(self.clip.device)] #Add the original image to the batch of augmentations\n",
        "        for _ in range(num_augmentations):\n",
        "            augmented_images.append(self.clip.preprocess(augmentations(image)).unsqueeze(0).to(self.clip.device))\n",
        "        batch = torch.vstack(augmented_images)\n",
        "        return batch #(num_augumentations + 1, 3, 224, 224)\n",
        "\n",
        "    def confidence_selection(self, probs_matrix, percentile=0.8):\n",
        "        # Compute entropies for each row in the probability matrix\n",
        "        entropies = torch.tensor([compute_entropy(row) for row in probs_matrix])\n",
        "\n",
        "        # Find the threshold for the desired percentile\n",
        "        threshold = torch.quantile(entropies, percentile, interpolation = 'linear')\n",
        "\n",
        "        # Create a boolean mask where entropies below the threshold are selected\n",
        "        boolean_mask = entropies < threshold\n",
        "\n",
        "        # Assuming similarities is intended to be probs_matrix, return filtered matrix\n",
        "        return probs_matrix[boolean_mask]\n",
        "\n",
        "    def entropy_loss_TPT(self, batch_features, text_features = None):\n",
        "        if text_features is None:\n",
        "            text_features = self.clip.text_features\n",
        "        probs_matrix = class_probabilities(batch_features, text_features, self.clip.scale)\n",
        "        # Confidence selection for the augmented views:\n",
        "        probs_matrix = self.confidence_selection(probs_matrix)\n",
        "        # Average the caption probabilities across all augmentations\n",
        "        avg_probs = torch.tensor([row.mean() for row in probs_matrix.T])\n",
        "        # Compute the entropy of the averaged probability distribution\n",
        "        return compute_entropy(avg_probs), avg_probs\n",
        "\n",
        "    def TPT(self, image, num_augmentations=100):\n",
        "        self.clip.model.eval()\n",
        "        self.clip.require_CLIP_gradients(False)\n",
        "        batch = self.augment_image(image, num_augmentations)\n",
        "        batch_features = self.clip.model.encode_image(batch)\n",
        "        norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
        "        if (norms == 0).any():\n",
        "            print(\"Zero norm found in image features\")\n",
        "        batch_features = batch_features / norms.clamp_min(1e-10)\n",
        "\n",
        "        entropy, avg_probs = self.entropy_loss_TPT(batch_features)\n",
        "        prediction = torch.argmax(avg_probs).item()\n",
        "        return prediction, avg_probs.squeeze(), float(entropy)"
      ],
      "metadata": {
        "id": "Yp-UDHVNZjeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EB:\n",
        "\n",
        "    def __init__(self, CLIP):\n",
        "        self.clip = CLIP\n",
        "\n",
        "    def augment_image(self, image, num_augmentations=100, transformations=None):\n",
        "        if transformations==None:\n",
        "            torch.manual_seed(33)#Set a seed for reproducibility of the random augmentations\n",
        "            augmentations = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(p=0.5),\n",
        "                transforms.RandomVerticalFlip(p=0.5),\n",
        "                transforms.RandomRotation(degrees=30),\n",
        "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "                transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
        "            ])\n",
        "        augmented_images = [self.clip.preprocess(image).unsqueeze(0).to(self.clip.device)] #Add the original image to the batch of augmentations\n",
        "        for _ in range(num_augmentations):\n",
        "            augmented_images.append(self.clip.preprocess(augmentations(image)).unsqueeze(0).to(self.clip.device))\n",
        "        batch = torch.vstack(augmented_images)\n",
        "        return batch #(num_augumentations + 1, 3, 224, 224)\n",
        "\n",
        "    #Implementing the Entropy Boost stuff:\n",
        "    def pick_candidates(self, tensor, classifier, top_num):\n",
        "        # to select a subset of \"candidates\" from a given tensor based on scores provided by a classifier\n",
        "        _, top_indices = torch.topk(classifier, top_num)\n",
        "        candidates = torch.squeeze(tensor[top_indices])\n",
        "\n",
        "        return candidates, top_indices\n",
        "\n",
        "    def expand_tensor(self, tensor, top_indices, n):\n",
        "\n",
        "        exp_tensor = torch.zeros(n).to(self.clip.device)\n",
        "        for i in range(top_indices.shape[0]): exp_tensor[top_indices[i]] = tensor[i]\n",
        "\n",
        "        return exp_tensor\n",
        "\n",
        "    def generate_augmentations_similarities(self, image, image_features, txt_candidates, num_augmentations, top_aug_num):\n",
        "        # augment\n",
        "        torch.manual_seed(33+self.clip.changeseed)\n",
        "        augmentations = transforms.Compose([\n",
        "                                transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
        "                                ])\n",
        "        batch = self.augment_image(image, num_augmentations)\n",
        "        batch_features = self.clip.model.encode_image(batch)\n",
        "        norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
        "        if (norms == 0).any():\n",
        "            print(\"Zero norm found in image features\")\n",
        "        batch_features = batch_features / norms.clamp_min(1e-10)\n",
        "\n",
        "\n",
        "        # pick the ones closest to the original image\n",
        "        probs_matrix = class_probabilities(image_features, batch_features, self.clip.scale).squeeze()\n",
        "        candidates_features, top_indices_aug = self.pick_candidates(batch_features, probs_matrix, top_num = top_aug_num)\n",
        "\n",
        "        candidates_probs_matrix = class_probabilities(candidates_features, txt_candidates, self.clip.scale).squeeze()\n",
        "\n",
        "        del batch # avoid Cuda to run out of memory?\n",
        "        return torch.mean(candidates_probs_matrix, dim=0)\n",
        "\n",
        "    def boost_augmentations(self, image, image_features, prob, num_augmentations, num_candidates, top_aug_num):\n",
        "\n",
        "        _, top_indices = torch.topk(prob, num_candidates)\n",
        "        text_candidates = self.clip.text_features[top_indices]\n",
        "\n",
        "        n = prob.shape[0]\n",
        "\n",
        "        #Candidates probabilities in the original image\n",
        "        candidates_prob_og = class_probabilities(image_features, text_candidates, self.clip.scale).squeeze()\n",
        "        candidates_prob_og = self.expand_tensor(candidates_prob_og, top_indices, n)\n",
        "\n",
        "        #Candidates averaged probability in the augmentations\n",
        "        candidates_avg_prob = self.generate_augmentations_similarities(image, image_features, text_candidates, num_augmentations, top_aug_num)\n",
        "        candidates_avg_prob = self.expand_tensor(candidates_avg_prob, top_indices, n)\n",
        "\n",
        "        return candidates_prob_og, candidates_avg_prob\n",
        "\n",
        "    def entropyboosting(self, image, num_augmentations = 100, num_candidates = 10, top_aug_num = 5):\n",
        "        self.clip.model.eval()\n",
        "        self.clip.require_CLIP_gradients(False)\n",
        "        if num_candidates <= 1:\n",
        "            print(\"If num_candidates=0 this is just CLIP\")\n",
        "            return\n",
        "        image_prepro = self.clip.preprocess(image).unsqueeze(0).to(self.clip.device)\n",
        "        image_features = self.clip.model.encode_image(image_prepro)\n",
        "        norms = image_features.norm(dim=-1, p=2,  keepdim=True)\n",
        "        image_features = image_features / norms.clamp_min(1e-10)\n",
        "\n",
        "        #Compute out of the box CLIP probability distribution and prediction\n",
        "        clip_probs = class_probabilities(image_features, self.clip.text_features, self.clip.scale).squeeze()\n",
        "        clip_prediction = torch.argmax(clip_probs).item()\n",
        "\n",
        "        phi = 2 / (1 + np.sqrt(5)) # Aura section - math fetish but it seems to work so...\n",
        "\n",
        "        # Defining loop variables\n",
        "        aug_prob = None #Should this be an input of the method??\n",
        "        output_prob = clip_probs.clone() #Is this necessary??\n",
        "        max_iter = 4\n",
        "        iter = 0\n",
        "        while iter < max_iter:\n",
        "            self.clip.changeseed = self.clip.changeseed + 1 #To make different random transformations at every iter but replicable\n",
        "            candidates_prob_og, candidates_avg_prob = self.boost_augmentations(image, image_features, output_prob, num_augmentations, num_candidates, top_aug_num)\n",
        "\n",
        "            clip_probs = phi * clip_probs + (1 - phi) * candidates_prob_og #Why are we doing this to the clip probs?\n",
        "\n",
        "            if aug_prob == None: aug_prob = candidates_avg_prob\n",
        "            else:                aug_prob = 0.5 * aug_prob + 0.5 * candidates_avg_prob\n",
        "\n",
        "            output_prob = 0.6 * clip_probs + 0.4 * aug_prob # give a little more weight to the probability of the image.\n",
        "            EB_prediction = torch.argmax(output_prob).item()\n",
        "\n",
        "            # if clip_prediction != EB_prediction then it changed its mind: the class with highest probability has changed - better do further checks.\n",
        "            if clip_prediction == EB_prediction: break\n",
        "\n",
        "            iter += 1\n",
        "        self.clip.changeseed = 0\n",
        "        entropy = compute_entropy(output_prob)\n",
        "        return EB_prediction, output_prob.squeeze(), float(entropy)"
      ],
      "metadata": {
        "id": "SFZid7ykZ5C5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}