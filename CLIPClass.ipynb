{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIg1G6AD6fyQ",
    "outputId": "ec7ad4fe-ae62-4ba5-fac5-d737547a332b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (6.2.0)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.2)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.13)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-z_2c5y17\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-z_2c5y17\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (6.2.0)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.3.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.15.2a0+072ec57)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.12.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0) (12.5.40)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/conda/lib/python3.10/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.10/site-packages (from seaborn) (2.1.4)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/lib/python3.10/site-packages (from seaborn) (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a62fe4b6d144cae93af36ad604b8cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git\n",
    "! pip install datasets transformers\n",
    "! pip install seaborn\n",
    "\n",
    "# Used for CLIP:\n",
    "import clip\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#Used for testing:\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "#Used for visualizing results\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from ipywidgets import widgets, interactive_output, Dropdown, Output, VBox, Button\n",
    "from IPython.display import display  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIPModel Class Method Explanations\n",
    "\n",
    "## Constructor\n",
    "- **`__init__(self, model_name='ViT-B/32', device=None)`**\n",
    "  - Initializes the model with a specified model name and device. Defaults to using CUDA if available. It sets up the optimizer and default parameters for the model. And casts it to float 32 to use ADAM. Also sets a `changeseed` to be able to have different random generations in a loop for Entropy Boosting.\n",
    "\n",
    "## Optimizer Methods\n",
    "- **`use_ADAM(self)`**\n",
    "  - Changes the optimizer of the model to ADAM.\n",
    "- **`use_SGD(self)`**\n",
    "  - Switches the optimizer to SGD.\n",
    "- **`convert_model_parameters_to_float32(self, model)`**\n",
    "    - This is for ADAM to work. Converts all model parameters to float32 data type.\n",
    "- **`grad_descent_step(self, loss)`**\n",
    "    - Performs a single gradient descent step using the computed loss, updating the model parameters accordingly. This is what modifies the clip model stored in the object.\n",
    "\n",
    "## Gradient Handling\n",
    "- **`require_CLIP_gradients(self, state=True)`**\n",
    "  - Enables or disables the calculation of gradients for model parameters based on the specified state.\n",
    "\n",
    "## Data Handling\n",
    "- **`load_data(self)`**\n",
    "  - This is just to have some data easy to access to test the methods on some image. Loads the CIFAR100 dataset which is useful for testing or validating the model performance on image data.\n",
    " \n",
    "## Tokenization of labels\n",
    "- **`tokenize_labels(self, classes)`**\n",
    "    - Converts a list of class labels into text tokens using CLIP's tokenizer and encodes them into text features. This is used for matching text descriptions with images. For now this is just used with a photo of a {class} and then stored as an attribute so that we can reuse it for every method.\n",
    " \n",
    "## Transformations\n",
    "- **`augment_image(self, image, num_augmentations=100, transformations=None)`**\n",
    "  - Applies specified transformations to an image (or a random transformation if none given) to generate multiple augmented versions.\n",
    "\n",
    "## Feature and Probability Computations\n",
    "- **`cos_sim(self, image_features, text_features)`**\n",
    "  - Calculates the cosine similarity between image and text features.\n",
    "- **`logits(self, image_features, text_features)`**\n",
    "  - Computes the logits by applying a scale factor (temperature already learned by CLIP) to the cosine similarities.\n",
    "- **`class_probabilities(self, image_features, text_features)`**\n",
    "  - Converts logits to class probabilities using the softmax function.\n",
    "\n",
    "## Entropy and Loss Calculations\n",
    "- **`compute_entropy(self, x)`**\n",
    "  - Computes the Shannon entropy of a probability distribution, measuring uncertainty.\n",
    "- **`marginal_entropy(self, logits)`**\n",
    "    - Calculates the marginal entropy of the logits, which quantifies the uncertainty or spread of the predicted probabilities across different classes.\n",
    "- **`entropy_loss_MEMO(self, batch_features, text_features=None)`**\n",
    "  - Calculates the entropy loss for the MEMO strategy using marginal entropy.\n",
    "- **`entropy_loss_TPT(self, batch_features, text_features=None)`**\n",
    "    - Calculates and returns the entropy loss for TPT.\n",
    "\n",
    "## Predictions with the CLIP model out of the box\n",
    "- **`forward(self, image)`**\n",
    "    - Processes an image through the model, encoding it into image features, normalizing these features, and then computing the class probabilities.\n",
    "- **`predict(self, image)`**\n",
    "    - Completes the prediction process including preprocessing the image, computing probabilities, and determining the most probable class along with its entropy. This is what you can use to compute CLIP out of the box predictions.\n",
    "\n",
    "## Methods used to get predictions\n",
    "- **`MEMO(self, image, num_augmentations=100)`**\n",
    "  - Implements MEMO strategy by optimizing model parameters to minimize entropy across predictions.\n",
    "- **`TPT(self, image, num_augmentations=100)`**\n",
    "    - Applies the TPT strategy, focusing on refining the model's ability to predict with high certainty by averaging probabilities across \"good\" augmentations.\n",
    "- **`entropyboosting(self, image, num_augmentations=100, num_candidates=10, top_aug_num=5)`**\n",
    "  - Uses \"Entropy Boosting\" to enhance model confidence by iterating over augmentation cycles to stabilize and improve prediction certainty.\n",
    "\n",
    "## Extra for TPT\n",
    "- **`confidence_selection(self, probs_matrix, percentile=0.8)`**\n",
    "    - Selects predictions with entropy below a specified percentile, filtering out less confident (high entropy) predictions to focus on more certain outcomes.\n",
    "\n",
    "# Extra for Entropy Boosting\n",
    "- **`pick_candidates(self, tensor, classifier, top_num)`**\n",
    "    - Selects top candidates based on a classifier score.\n",
    "- **`expand_tensor(self, tensor, top_indices, n)`**\n",
    "    - Expands a smaller tensor into a larger one based on specified indices.\n",
    "- **`generate_augmentations_similarities(self, image, image_features, txt_candidates, num_augmentations, top_aug_num)`**\n",
    "    - Generates and evaluates the similarity of augmented images to text candidates, aiding in the identification of most relevant image features.\n",
    "- **`boost_augmentations(self, image, image_features, prob, num_augmentations, num_candidates, top_aug_num)`**\n",
    "    - Boosts the confidence in model predictions by blending probabilities from original and augmented images, focusing on candidates with high initial probabilities.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to split the CLass:\n",
    "We have a parent class CLIP and 3 children (for now) which are MEMO, TPT and EB (Entropy Boost). Somehow, those children should be able to interact and use the same CLIP object )(access its attributes, methods as if they where the same object) without creating a copy of it. Not sure how to do it, but ideally it would work like one single class but splitting the code accross multiple classes so that we can structure the code better.\n",
    "## Attributes:\n",
    "- device (CLIP)\n",
    "- model (CLIP)\n",
    "- preprocess (CLIP)\n",
    "- optimizer (CLIP)\n",
    "- text_features (CLIP)\n",
    "- requiring_grads (CLIP)\n",
    "- logit_scale (CLIP)\n",
    "- changeseed \n",
    "\n",
    "## Methods:\n",
    "- __init__ (CLIP has exactly the one written)\n",
    "- use_ADAM (CLIP)\n",
    "- use_SGD (CLIP)\n",
    "- require_CLIP_gradients (CLIP)\n",
    "- convert_model_parameters_to_float32 (CLIP)\n",
    "- load_data (this one does not matter, won't be in the final version)\n",
    "- tokenize_labels (CLIP)\n",
    "- augment_image (CLIP)\n",
    "- cos_sim (CLIP)\n",
    "- logits (CLIP)\n",
    "- class_probabilities (CLIP)\n",
    "- marginal_entropy (CLIP)\n",
    "- compute_entropy (CLIP)\n",
    "- confidence_selection (TPT)\n",
    "- entropy_loss_MEMO (MEMO)\n",
    "- entropy_loss_TPT (TPT)\n",
    "- forward (CLIP)\n",
    "- predict (CLIP)\n",
    "- grad_descent_step (CLIP)\n",
    "- MEMO (MEMO)\n",
    "- TPT (TPT)\n",
    "- pick_candidates (EB)\n",
    "- expand_tensor (EB)\n",
    "- generate_augmentations_similarities (EB)\n",
    "- boost_augmentations (EB)\n",
    "- entropyboosting (EB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XGJSO0FnKTp",
    "outputId": "5f97a486-48f4-48ca-e713-83c576ebcd03"
   },
   "outputs": [],
   "source": [
    "class CLIPModel:\n",
    "\n",
    "    def __init__(self, model_name='ViT-B/32', device=None):\n",
    "        self.device = device if device else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model, self.preprocess = clip.load(model_name, self.device)\n",
    "        self.model = self.convert_model_parameters_to_float32(self.model)\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
    "        self.text_features = None\n",
    "        self.requiring_grads = None\n",
    "        self.logit_scale = self.model.logit_scale #temperature parameter learned by CLIP\n",
    "        self.changeseed = 0 #This is to be able to do diverse random transforms but replicable\n",
    "    \n",
    "    def use_ADAM(self):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-5)\n",
    "\n",
    "    def use_SGD(self):\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
    "\n",
    "    def require_CLIP_gradients(self, state = True):\n",
    "        if self.requiring_grads is None or state != self.requiring_grads: #don't change if the state is already OK\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = state\n",
    "            self.requiring_grads = state\n",
    "\n",
    "    def convert_model_parameters_to_float32(self, model):\n",
    "        for param in model.parameters():\n",
    "            param.data = param.data.to(torch.float32)\n",
    "        return model\n",
    "\n",
    "    def load_data(self):\n",
    "        cifar100 = torchvision.datasets.CIFAR100(root='./data', download=True, train=False)\n",
    "        return cifar100\n",
    "\n",
    "    #This are heuristic labels\n",
    "    def tokenize_labels(self, classes):\n",
    "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classes]).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            self.text_features = self.model.encode_text(text_inputs)\n",
    "            self.text_features /= self.text_features.norm(dim=-1, p=2, keepdim=True)\n",
    "\n",
    "    def augment_image(self, image, num_augmentations=100, transformations=None):\n",
    "        if transformations==None:\n",
    "            torch.manual_seed(33)#Set a seed for reproducibility of the random augmentations\n",
    "            augmentations = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=30),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "                transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
    "            ])\n",
    "        augmented_images = [self.preprocess(image).unsqueeze(0).to(self.device)] #Add the original image to the batch of augmentations\n",
    "        for _ in range(num_augmentations):\n",
    "            augmented_images.append(self.preprocess(augmentations(image)).unsqueeze(0).to(self.device))\n",
    "        batch = torch.vstack(augmented_images)\n",
    "        return batch #(num_augumentations + 1, 3, 224, 224)\n",
    "\n",
    "    def cos_sim(self, image_features, text_features):\n",
    "        return  image_features @ text_features.T\n",
    "\n",
    "    def logits(self, image_features, text_features):\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        return logit_scale * self.cos_sim(image_features, text_features)\n",
    "\n",
    "    def class_probabilities(self, image_features, text_features):\n",
    "        #Compute cosine similarities\n",
    "        return  self.logits(image_features, text_features).softmax(dim=-1)\n",
    "\n",
    "    def marginal_entropy(self, logits):\n",
    "        z = logits - logits.logsumexp(dim = -1, keepdim=True) # compute z_ij\n",
    "        marginal_logp = z.logsumexp(dim=0) - np.log(z.shape[0])   # compute marginal log probabilities\n",
    "\n",
    "        min_real = torch.finfo(marginal_logp.dtype).min # for numerical stability,\n",
    "        # the smallest representable number given the dtype of logits.\n",
    "        avg_logits = torch.clamp(marginal_logp, min = min_real)  # put a threshold to avoid underflow\n",
    "\n",
    "        return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
    "\n",
    "    def compute_entropy(self, x): #Shanon entropy in bits\n",
    "        #This computes the Shanon entropy\n",
    "        log_x = torch.log2(x.clamp_min(1e-20))\n",
    "        entropy = -torch.sum(x * log_x)\n",
    "        return entropy\n",
    "\n",
    "    def confidence_selection(self, probs_matrix, percentile=0.8):\n",
    "        # Compute entropies for each row in the probability matrix\n",
    "        entropies = torch.tensor([self.compute_entropy(row) for row in probs_matrix])\n",
    "\n",
    "        # Find the threshold for the desired percentile\n",
    "        threshold = torch.quantile(entropies, percentile, interpolation = 'linear')\n",
    "\n",
    "        # Create a boolean mask where entropies below the threshold are selected\n",
    "        boolean_mask = entropies < threshold\n",
    "\n",
    "        # Assuming similarities is intended to be probs_matrix, return filtered matrix\n",
    "        return probs_matrix[boolean_mask]\n",
    "\n",
    "    def entropy_loss_MEMO(self, batch_features, text_features = None):\n",
    "        if text_features is None:\n",
    "            text_features = self.text_features\n",
    "        #Logits (unnormalized probabilities)\n",
    "        logits = self.logits(batch_features, text_features)\n",
    "        # Compute the entropy of every text caption accross all augmentations\n",
    "        marginal_entropy = self.marginal_entropy(logits)\n",
    "        return marginal_entropy\n",
    "\n",
    "    def entropy_loss_TPT(self, batch_features, text_features = None):\n",
    "        if text_features is None:\n",
    "            text_features = self.text_features\n",
    "        probs_matrix = self.class_probabilities(batch_features, text_features)\n",
    "        # Confidence selection for the augmented views:\n",
    "        probs_matrix = self.confidence_selection(probs_matrix)\n",
    "        # Average the caption probabilities across all augmentations\n",
    "        avg_probs = torch.tensor([row.mean() for row in probs_matrix.T])\n",
    "        # Compute the entropy of the averaged probability distribution\n",
    "        return self.compute_entropy(avg_probs), avg_probs\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.model.encode_image(image)\n",
    "        norms = image_features.norm(dim=-1, p=2,  keepdim=True)\n",
    "        if (norms == 0).any():\n",
    "            print(\"Zero norm found in image features\")\n",
    "        image_features = image_features / norms.clamp_min(1e-10)\n",
    "        return self.class_probabilities(image_features, self.text_features)\n",
    "\n",
    "    def predict(self, image):\n",
    "        self.model.eval()\n",
    "        image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            probs = self.forward(image)\n",
    "\n",
    "        prediction = torch.argmax(probs).item()\n",
    "        entropy = float(self.compute_entropy(probs))\n",
    "        return prediction, probs.squeeze(), entropy\n",
    "\n",
    "    def grad_descent_step(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def MEMO(self, image, num_augmentations=100):\n",
    "        # Save original parameters\n",
    "        original_params = {name: param.clone() for name, param in self.model.named_parameters()}\n",
    "\n",
    "        # Require gradients to update the CLIP parameters\n",
    "        self.require_CLIP_gradients(state = True)\n",
    "\n",
    "        try:\n",
    "            self.model.train()\n",
    "            batch = self.augment_image(image, num_augmentations)\n",
    "            batch_features = self.model.encode_image(batch)\n",
    "            norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
    "            if (norms == 0).any():\n",
    "                print(\"Zero norm found in image features\")\n",
    "            batch_features = batch_features / norms.clamp_min(1e-10)\n",
    "            loss = self.entropy_loss_MEMO(batch_features)\n",
    "            self.grad_descent_step(loss)\n",
    "\n",
    "            if any(torch.isnan(param).any() for param in self.model.parameters()):\n",
    "                print(\"nan values detected in model parameters after updating\")\n",
    "            # Predict using the updated model\n",
    "            prediction, probs, entropy = self.predict(image)\n",
    "        finally:\n",
    "            # Restore original parameters\n",
    "            with torch.no_grad():\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    param.copy_(original_params[name])\n",
    "        return prediction, probs.squeeze(), entropy\n",
    "\n",
    "    def TPT(self, image, num_augmentations=100):\n",
    "        self.model.eval()\n",
    "        self.require_CLIP_gradients(False)\n",
    "        batch = self.augment_image(image, num_augmentations)\n",
    "        batch_features = self.model.encode_image(batch)\n",
    "        norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
    "        if (norms == 0).any():\n",
    "            print(\"Zero norm found in image features\")\n",
    "        batch_features = batch_features / norms.clamp_min(1e-10)\n",
    "\n",
    "        entropy, avg_probs = self.entropy_loss_TPT(batch_features)\n",
    "        prediction = torch.argmax(avg_probs).item()\n",
    "        return prediction, avg_probs.squeeze(), float(entropy)\n",
    "\n",
    "    #Implementing the Entropy Boost stuff:\n",
    "    def pick_candidates(self, tensor, classifier, top_num):\n",
    "        # to select a subset of \"candidates\" from a given tensor based on scores provided by a classifier\n",
    "        _, top_indices = torch.topk(classifier, top_num)\n",
    "        candidates = torch.squeeze(tensor[top_indices])\n",
    "        \n",
    "        return candidates, top_indices\n",
    "\n",
    "    def expand_tensor(self, tensor, top_indices, n):\n",
    "    \n",
    "        exp_tensor = torch.zeros(n).to(self.device)\n",
    "        for i in range(top_indices.shape[0]): exp_tensor[top_indices[i]] = tensor[i]\n",
    "    \n",
    "        return exp_tensor\n",
    "\n",
    "    def generate_augmentations_similarities(self, image, image_features, txt_candidates, num_augmentations, top_aug_num):\n",
    "        # augment\n",
    "        torch.manual_seed(33+self.changeseed)\n",
    "        augmentations = transforms.Compose([\n",
    "                                transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
    "                                ])\n",
    "        batch = self.augment_image(image, num_augmentations)\n",
    "        batch_features = self.model.encode_image(batch)\n",
    "        norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
    "        if (norms == 0).any():\n",
    "            print(\"Zero norm found in image features\")\n",
    "        batch_features = batch_features / norms.clamp_min(1e-10)\n",
    "\n",
    "    \n",
    "        # pick the ones closest to the original image\n",
    "        probs_matrix = self.class_probabilities(image_features, batch_features).squeeze()\n",
    "        candidates_features, top_indices_aug = self.pick_candidates(batch_features, probs_matrix, top_num = top_aug_num)\n",
    "\n",
    "        candidates_probs_matrix = self.class_probabilities(candidates_features, txt_candidates).squeeze()\n",
    "    \n",
    "        del batch # avoid Cuda to run out of memory?\n",
    "        return torch.mean(candidates_probs_matrix, dim=0)\n",
    "    \n",
    "    def boost_augmentations(self, image, image_features, prob, num_augmentations, num_candidates, top_aug_num):\n",
    "    \n",
    "        _, top_indices = torch.topk(prob, num_candidates)\n",
    "        text_candidates = self.text_features[top_indices]\n",
    "    \n",
    "        n = prob.shape[0]\n",
    "\n",
    "        #Candidates probabilities in the original image\n",
    "        candidates_prob_og = self.class_probabilities(image_features, text_candidates).squeeze()\n",
    "        candidates_prob_og = self.expand_tensor(candidates_prob_og, top_indices, n)\n",
    "\n",
    "        #Candidates averaged probability in the augmentations\n",
    "        candidates_avg_prob = self.generate_augmentations_similarities(image, image_features, text_candidates, num_augmentations, top_aug_num)\n",
    "        candidates_avg_prob = self.expand_tensor(candidates_avg_prob, top_indices, n)\n",
    "    \n",
    "        return candidates_prob_og, candidates_avg_prob\n",
    "\n",
    "    def entropyboosting(self, image, num_augmentations = 100, num_candidates = 10, top_aug_num = 5):\n",
    "        self.model.eval()\n",
    "        self.require_CLIP_gradients(False)\n",
    "        if num_candidates <= 1:\n",
    "            print(\"If num_candidates=0 this is just CLIP\")\n",
    "            return\n",
    "        image_prepro = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        image_features = self.model.encode_image(image_prepro)\n",
    "        norms = image_features.norm(dim=-1, p=2,  keepdim=True)\n",
    "        image_features = image_features / norms.clamp_min(1e-10)\n",
    "\n",
    "        #Compute out of the box CLIP probability distribution and prediction\n",
    "        clip_probs = self.class_probabilities(image_features, self.text_features).squeeze()\n",
    "        clip_prediction = torch.argmax(clip_probs).item()\n",
    "\n",
    "        phi = 2 / (1 + np.sqrt(5)) # Aura section - math fetish but it seems to work so...\n",
    "        \n",
    "        # Defining loop variables\n",
    "        aug_prob = None #Should this be an input of the method??\n",
    "        output_prob = clip_probs.clone() #Is this necessary??\n",
    "        max_iter = 4\n",
    "        iter = 0\n",
    "        while iter < max_iter:\n",
    "            self.changeseed = self.changeseed + 1 #To make different random transformations at every iter but replicable\n",
    "            candidates_prob_og, candidates_avg_prob = self.boost_augmentations(image, image_features, output_prob, num_augmentations, num_candidates, top_aug_num)\n",
    "            \n",
    "            clip_probs = phi * clip_probs + (1 - phi) * candidates_prob_og #Why are we doing this to the clip probs?\n",
    "            \n",
    "            if aug_prob == None: aug_prob = candidates_avg_prob\n",
    "            else:                aug_prob = 0.5 * aug_prob + 0.5 * candidates_avg_prob\n",
    "            \n",
    "            output_prob = 0.6 * clip_probs + 0.4 * aug_prob # give a little more weight to the probability of the image.\n",
    "            EB_prediction = torch.argmax(output_prob).item()\n",
    "            \n",
    "            # if clip_prediction != EB_prediction then it changed its mind: the class with highest probability has changed - better do further checks.\n",
    "            if clip_prediction == EB_prediction: break\n",
    "\n",
    "            iter += 1\n",
    "        self.changeseed = 0\n",
    "        entropy = self.compute_entropy(output_prob)\n",
    "        return EB_prediction, output_prob.squeeze(), float(entropy)\n",
    "\n",
    "\n",
    "# Preparing the class for usage\n",
    "clip_model = CLIPModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yYC2YqhknDk"
   },
   "source": [
    "1 IMAGE TRIES ON CLIP, MEMO, TPT and EB :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B52IQTi6OyfW",
    "outputId": "d8e52605-ea10-48f4-e56e-776e5a2b1c21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading CIFAR100 for one image tries_\n",
    "cifar100 = clip_model.load_data()\n",
    "clip_model.tokenize_labels(cifar100.classes)\n",
    "image, class_id = cifar100[3637]\n",
    "len(cifar100.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "2.208749294281006\n",
      "tensor([4.4409e-04, 2.0257e-03, 1.7850e-03, 6.7053e-05, 6.8302e-05, 1.2799e-04,\n",
      "        2.1407e-04, 8.0405e-03, 3.6831e-05, 1.3982e-04, 3.8572e-04, 3.0443e-04,\n",
      "        1.2713e-05, 3.9793e-05, 7.2395e-04, 5.2543e-05, 6.3892e-04, 1.7170e-05,\n",
      "        1.5631e-03, 3.5367e-04, 1.0214e-04, 4.6957e-04, 1.1729e-04, 1.9360e-05,\n",
      "        2.2551e-04, 6.2725e-05, 2.7089e-03, 4.3508e-02, 2.1955e-04, 1.7460e-03,\n",
      "        4.6290e-06, 1.6038e-04, 1.2927e-02, 3.8837e-05, 5.5391e-06, 2.1283e-04,\n",
      "        3.4115e-04, 4.3326e-05, 3.8308e-05, 9.3167e-05, 4.4401e-04, 2.5278e-03,\n",
      "        1.5629e-02, 3.2312e-04, 4.5569e-02, 5.2967e-05, 1.3752e-04, 4.2287e-05,\n",
      "        3.7950e-05, 1.7290e-04, 2.4531e-03, 1.0843e-03, 2.3604e-04, 2.8239e-04,\n",
      "        4.5561e-05, 5.3201e-05, 3.1678e-04, 5.2178e-05, 9.5018e-05, 5.7305e-04,\n",
      "        3.0544e-04, 6.4293e-04, 2.1116e-04, 1.2552e-04, 1.8514e-04, 2.5836e-04,\n",
      "        1.7865e-04, 2.3634e-04, 2.7643e-06, 4.6131e-06, 6.4169e-05, 2.6901e-05,\n",
      "        4.1291e-04, 1.9324e-05, 4.9106e-04, 9.3546e-05, 1.0680e-05, 1.4651e-02,\n",
      "        5.8744e-01, 1.0724e-03, 3.9147e-05, 2.2297e-05, 2.9193e-04, 3.2322e-02,\n",
      "        7.7673e-05, 2.4090e-03, 5.8943e-04, 9.7330e-05, 8.7041e-04, 4.2988e-05,\n",
      "        6.9078e-05, 1.6199e-04, 5.6775e-05, 1.9426e-01, 2.7203e-05, 6.5766e-06,\n",
      "        4.0043e-05, 2.9092e-05, 1.3313e-04, 1.1812e-02], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Try entropyboosting\n",
    "prediction, probs, entropy = clip_model.entropyboosting(image)\n",
    "print(prediction)\n",
    "print(entropy)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "087FNIiqpDUb",
    "outputId": "157fb454-8aae-497a-b5c6-aecf4b3d4569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "2.3266210556030273\n",
      "tensor([[1.1976e-03, 5.4628e-03, 4.8136e-03, 1.8082e-04, 1.8419e-04, 3.4514e-04,\n",
      "         5.7728e-04, 8.2105e-03, 9.9322e-05, 3.7705e-04, 1.0402e-03, 8.2096e-04,\n",
      "         3.4284e-05, 1.0731e-04, 1.9523e-03, 1.4169e-04, 1.7230e-03, 4.6304e-05,\n",
      "         4.2152e-03, 9.5374e-04, 2.7545e-04, 1.2663e-03, 3.1629e-04, 5.2210e-05,\n",
      "         6.0814e-04, 1.6915e-04, 7.3052e-03, 1.7485e-02, 5.9205e-04, 4.7085e-03,\n",
      "         1.2483e-05, 4.3251e-04, 8.4594e-03, 1.0473e-04, 1.4937e-05, 5.7395e-04,\n",
      "         9.1999e-04, 1.1684e-04, 1.0331e-04, 2.5125e-04, 1.1974e-03, 6.8169e-03,\n",
      "         1.7164e-02, 8.7137e-04, 1.8750e-02, 1.4284e-04, 3.7087e-04, 1.1404e-04,\n",
      "         1.0234e-04, 4.6626e-04, 6.6154e-03, 2.9242e-03, 6.3653e-04, 7.6153e-04,\n",
      "         1.2287e-04, 1.4347e-04, 8.5428e-04, 1.4071e-04, 2.5624e-04, 1.5454e-03,\n",
      "         8.2368e-04, 1.7338e-03, 5.6945e-04, 3.3850e-04, 4.9928e-04, 6.9673e-04,\n",
      "         4.8177e-04, 6.3735e-04, 7.4547e-06, 1.2440e-05, 1.7305e-04, 7.2545e-05,\n",
      "         1.1135e-03, 5.2111e-05, 1.3243e-03, 2.5227e-04, 2.8802e-05, 1.5945e-02,\n",
      "         6.5313e-01, 2.8920e-03, 1.0557e-04, 6.0129e-05, 7.8726e-04, 3.8252e-02,\n",
      "         2.0946e-04, 6.4964e-03, 1.5895e-03, 2.6247e-04, 2.3473e-03, 1.1593e-04,\n",
      "         1.8628e-04, 4.3683e-04, 1.5311e-04, 1.2288e-01, 7.3359e-05, 1.7735e-05,\n",
      "         1.0799e-04, 7.8453e-05, 3.5900e-04, 8.4517e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Prediction using CLIP out of the box\n",
    "prediction1, probs1, entropy1 = clip_model.predict(image)\n",
    "print(prediction1)\n",
    "print(entropy1)\n",
    "print(probs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "_RheDQxnoeB7",
    "outputId": "7b03df7f-81bc-4555-c017-377d44e5b3c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "2.077364206314087\n",
      "tensor([[9.8620e-04, 4.7828e-03, 4.6798e-03, 1.5572e-04, 1.5197e-04, 3.0867e-04,\n",
      "         4.7689e-04, 6.2922e-03, 8.2343e-05, 3.0923e-04, 8.7553e-04, 8.1575e-04,\n",
      "         2.9356e-05, 8.9023e-05, 1.6763e-03, 1.1114e-04, 1.4614e-03, 3.6086e-05,\n",
      "         3.8958e-03, 8.0583e-04, 2.2591e-04, 1.1260e-03, 2.5036e-04, 4.4503e-05,\n",
      "         4.7512e-04, 1.3567e-04, 5.4395e-03, 1.5579e-02, 5.2545e-04, 4.0048e-03,\n",
      "         1.0843e-05, 3.6359e-04, 7.3783e-03, 9.3484e-05, 1.3365e-05, 5.7313e-04,\n",
      "         7.7305e-04, 1.0464e-04, 9.2250e-05, 1.8495e-04, 9.6642e-04, 6.5645e-03,\n",
      "         1.4847e-02, 7.2444e-04, 1.7678e-02, 1.1078e-04, 3.5591e-04, 1.0298e-04,\n",
      "         7.9667e-05, 3.7379e-04, 6.0870e-03, 2.3019e-03, 5.8701e-04, 6.6893e-04,\n",
      "         1.1238e-04, 1.3150e-04, 7.1440e-04, 1.1645e-04, 2.0226e-04, 1.4091e-03,\n",
      "         7.9140e-04, 1.5631e-03, 4.4642e-04, 2.7754e-04, 4.8233e-04, 6.4062e-04,\n",
      "         4.2063e-04, 5.6923e-04, 6.2323e-06, 9.5952e-06, 1.4697e-04, 6.2266e-05,\n",
      "         9.2542e-04, 4.5426e-05, 1.2508e-03, 2.6994e-04, 2.5094e-05, 1.3129e-02,\n",
      "         7.0722e-01, 2.4452e-03, 1.0118e-04, 5.2856e-05, 6.6586e-04, 3.6857e-02,\n",
      "         1.8823e-04, 4.7386e-03, 1.3757e-03, 2.2036e-04, 2.0671e-03, 8.7088e-05,\n",
      "         1.4390e-04, 3.7851e-04, 1.2681e-04, 9.3765e-02, 6.8134e-05, 1.4196e-05,\n",
      "         1.0045e-04, 6.6866e-05, 3.4894e-04, 8.3519e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Prediction using MEMO with SGD\n",
    "clip_model.use_SGD()\n",
    "prediction2, probs2, entropy2 = clip_model.MEMO(image, num_augmentations=10)\n",
    "print(prediction2)\n",
    "print(entropy2)\n",
    "print(probs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mlTDDcr_IRYf",
    "outputId": "8a58029d-da53-4ed3-96a2-5a039d91d2ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "0.3890056312084198\n",
      "tensor([[7.0291e-05, 1.2202e-03, 2.0966e-03, 2.8387e-05, 4.4688e-06, 2.8498e-05,\n",
      "         3.2856e-05, 6.2306e-05, 1.0148e-05, 4.9512e-05, 2.7819e-04, 5.3239e-04,\n",
      "         6.4909e-06, 6.8183e-06, 9.8106e-05, 1.4464e-06, 4.3893e-04, 2.5695e-06,\n",
      "         4.9707e-04, 5.9817e-05, 5.5518e-06, 6.2155e-05, 4.5031e-06, 1.0935e-05,\n",
      "         5.5434e-06, 4.1206e-06, 2.6603e-05, 4.4461e-04, 3.8518e-04, 2.5397e-04,\n",
      "         1.2974e-06, 1.2624e-05, 4.2113e-05, 4.5163e-05, 2.7994e-06, 4.2294e-04,\n",
      "         7.2896e-05, 4.4254e-05, 1.0838e-05, 1.8192e-06, 3.2608e-05, 2.0624e-03,\n",
      "         5.1893e-04, 3.0729e-05, 3.5040e-03, 1.8276e-06, 1.2709e-04, 2.3962e-05,\n",
      "         5.4501e-06, 1.4678e-05, 1.1937e-03, 3.1151e-05, 1.7511e-04, 5.6660e-05,\n",
      "         2.8462e-05, 1.3722e-05, 4.6307e-05, 2.8664e-06, 9.7663e-06, 8.9210e-04,\n",
      "         2.8619e-04, 3.3893e-04, 2.8193e-06, 5.9115e-06, 1.3063e-04, 8.1566e-05,\n",
      "         3.1865e-05, 1.4796e-05, 5.0939e-07, 1.6153e-06, 1.7927e-05, 1.1527e-05,\n",
      "         8.4458e-06, 4.9185e-06, 1.6388e-04, 6.4259e-04, 8.2300e-06, 1.8448e-04,\n",
      "         9.5956e-01, 1.1634e-04, 1.8333e-05, 6.7579e-06, 2.8254e-05, 1.6869e-02,\n",
      "         4.2036e-05, 1.1157e-04, 7.6165e-05, 1.6402e-05, 2.6695e-04, 5.7434e-06,\n",
      "         6.3032e-06, 7.7760e-05, 5.5851e-06, 1.5060e-04, 1.2123e-05, 7.9850e-07,\n",
      "         7.1923e-05, 1.1627e-05, 1.2605e-04, 4.3244e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Prediction using MEMO with ADAM\n",
    "clip_model.use_ADAM()\n",
    "clip_model.tokenize_labels(cifar100.classes) #You have to tokenize the labels again for the change in precision\n",
    "prediction3, probs3, entropy3 = clip_model.MEMO(image, num_augmentations=10)\n",
    "print(prediction3)\n",
    "print(entropy3)\n",
    "print(probs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nrxX4QrwsCbv",
    "outputId": "828b2345-5334-418b-b40b-444910986132"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "5.611426830291748\n",
      "tensor([0.0064, 0.0020, 0.0094, 0.0159, 0.0024, 0.0015, 0.0036, 0.0047, 0.0037,\n",
      "        0.0079, 0.0029, 0.0115, 0.0021, 0.0032, 0.0041, 0.0024, 0.0094, 0.0036,\n",
      "        0.0035, 0.0114, 0.0044, 0.0107, 0.0029, 0.0023, 0.0009, 0.0038, 0.0035,\n",
      "        0.0242, 0.0036, 0.0294, 0.0014, 0.0065, 0.0031, 0.0114, 0.0015, 0.0182,\n",
      "        0.0032, 0.0038, 0.0027, 0.0075, 0.0080, 0.0273, 0.0121, 0.0070, 0.0103,\n",
      "        0.0014, 0.0164, 0.0213, 0.0034, 0.0102, 0.0039, 0.0064, 0.0574, 0.0015,\n",
      "        0.0009, 0.0005, 0.0391, 0.0023, 0.0041, 0.1248, 0.0108, 0.0077, 0.0039,\n",
      "        0.0030, 0.0048, 0.0042, 0.0057, 0.0013, 0.0009, 0.0124, 0.0019, 0.0014,\n",
      "        0.0031, 0.0034, 0.0014, 0.0026, 0.0052, 0.0044, 0.1003, 0.0022, 0.0020,\n",
      "        0.0020, 0.0149, 0.0114, 0.0022, 0.0200, 0.0051, 0.0162, 0.0090, 0.0066,\n",
      "        0.0070, 0.0077, 0.0038, 0.0256, 0.0022, 0.0030, 0.0308, 0.0060, 0.0206,\n",
      "        0.0081])\n"
     ]
    }
   ],
   "source": [
    "# Prediction using TPT\n",
    "prediction4,prob_avg, entropy4 = clip_model.TPT(image, num_augmentations=10)\n",
    "print(prediction4)\n",
    "print(entropy4)\n",
    "print(prob_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory (just to put it somewhere, later we can move this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that for now we can start writting the explanations for the methods here and then we can reorganize it for the final version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-Time Prompt Tuning (TPT) for Image Classification with CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-Time Prompt Tuning (TPT) is a method designed to enhance the performance of vision-language models like CLIP by adapting the text prompts dynamically at test time. This approach is particularly useful in zero-shot learning scenarios where the model needs to generalize to new tasks without additional training.\n",
    "\n",
    "### Concept Overview\n",
    "\n",
    "TPT optimizes the text prompt to improve consistency and confidence in the model's predictions across various augmented views of a test image. By minimizing the entropy of the prediction distribution average across low-entropy augmentations, TPT attempts to make the model more reliable and robust, even under slight variations in input data.\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "#### Step 1: Initial Setup\n",
    "- **Model Preparation**: Initialize the pre-trained CLIP model using `CLIPModel(model_name='ViT-B/32')`.\n",
    "- **Prompt Initialization**: Start with a basic prompt template like \"a photo of a {object}\", processed using the `tokenize_labels` method.\n",
    "\n",
    "#### Step 2: Augmentation and Prediction\n",
    "- **Generate Augmented Views**: Create multiple augmented versions of the test image using the `augment_image` method, which might include crops, rotations, and color adjustments, etc.\n",
    "\n",
    "#### Step 3: Define the Optimization Objective\n",
    "The goal is to minimize the entropy of the predicted probability distribution across different classes for the test image, computed using the `entropy_loss_TPT` method.\n",
    "\n",
    "##### Equation for Entropy Minimization:\n",
    "\\\\[ p^* = \\arg\\min_p -\\sum_{i=1}^K \\tilde{p}_p(y_i | X_{\\text{test}}) \\log \\tilde{p}_p(y_i | X_{\\text{test}}) \\\\]\n",
    "Where:\n",
    "- \\\\( p^* \\\\) is the optimized prompt.\n",
    "- \\\\( K \\\\) is the number of classes.\n",
    "- \\\\( \\tilde{p}_p(y_i | X_{\\text{test}}) \\\\) is the averaged predicted probability for class \\( y_i \\) also returned in `entropy_loss_TPT`.\n",
    "\n",
    "With the Averaged Prediction Probability defined as:\n",
    "\\\\[ \\tilde{p}_p(y_i | X_{\\text{test}}) = \\frac{1}{N} \\sum_{n=1}^N p_p(y_i | A_i(X_{\\text{test}})) \\\\]\n",
    "- \\\\( N \\\\) is the number of augmented views.\n",
    "- \\\\( A_i(X_{\\text{test}}) \\\\) denotes the \\( i \\)-th augmented view of the test image.\n",
    "\n",
    "#### Step 4: Optimize the Prompt\n",
    "In the paper, they use gradient-based optimization to adjust the prompt parameters (provided by CoOp or CoCoOp) by minimizing the entropy. In our case, we decided that training CoOp or CoCoOp is not what we want to focus on for the project, therefor, for now what `TPT` method gives us is a evaluation of how good the prompts are that we will use to guide a wider classification method.\n",
    "\n",
    "#### Step 5: Confidence Selection\n",
    "Filter out noisy predictions by applying a confidence selection mechanism (`confidence_selection`) that discards the augmentations with the highest entropy (over a threshold value).\n",
    "\n",
    "##### Confidence-Based Averaging:\n",
    "\\\\[ \\tilde{p}_p(y | X_{\\text{test}}) = \\frac{1}{\\rho N} \\sum_{i=1}^N 1[H(p_i) \\leq \\tau] p_p(y | A_i(X_{\\text{test}})) \\\\]\n",
    "- \\\\(\\rho\\\\) is the cutoff percentile on the N augmented views (set to 0.8 by default in our implementation).\n",
    "- \\\\( 1[H(p_i) \\leq \\tau] \\\\) is an indicator function that filters predictions based on their entropy \\\\( H \\\\), with \\\\( \\tau \\\\) being a threshold set in the `confidence_selection` method as the entropy of the \\\\(\\rho\\\\) percentile. We are removing the augmentations with the highest entropy in the top 20%.\n",
    "\n",
    "\n",
    "#### Step 6: Final Prediction\n",
    "Apply the optimized prompt to make the final prediction on the original test image using the `TPT` method, which performs the entire sequence described above except for the prompt optimization which we will implement in other method without using CoOp or CoCoOp.\n",
    "\n",
    "### Note:\n",
    "Apart from what the paper says, I TPT alone without prompt optimization returns the averaged probability over low-entropy augmentations without doing any training. This is basically saying that all the top 80% lowest entropy augmentations are equally valid and averages the predictions across them, so the original image does not have more weight in the output than the augmentations. This is not the use proposed bythe paper but I am runing it as well to see how it does. I mean, the idea is not that crazy actually, although it is quite sensitive to the kind of augmentations we do (if we have bad augmentations in the low entropy 80%, we will be saying that those are equally valid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS in AWS Bucket DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to inspect the file directories we have access to with the datasets.\n",
    "\n",
    "With the following function we can print the folders and the first 3 subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to list the s3_bucket folders and the first 3 subfolders\n",
    "def list_datasets_with_subfolders(s3_bucket, s3_region):\n",
    "    s3_client = boto3.client(\"s3\", region_name=s3_region, verify=True)\n",
    "    response = s3_client.list_objects_v2(Bucket=s3_bucket, Delimiter='/')\n",
    "    \n",
    "    # Use a set to keep track of unique folder names\n",
    "    folders = set()\n",
    "    \n",
    "    # Append the common prefixes which are the folder names\n",
    "    for prefix in response.get('CommonPrefixes', []):\n",
    "        folders.add(prefix.get('Prefix'))\n",
    "    \n",
    "    # Print out the available folders and their first 3 subfolders in the bucket\n",
    "    for folder in sorted(folders):\n",
    "        print(f\"Folder: {folder}\")\n",
    "        \n",
    "        # List objects within the folder to find subfolders\n",
    "        sub_response = s3_client.list_objects_v2(Bucket=s3_bucket, Prefix=folder, Delimiter='/')\n",
    "        subfolders = []\n",
    "        \n",
    "        for sub_prefix in sub_response.get('CommonPrefixes', []):\n",
    "            subfolders.append(sub_prefix.get('Prefix'))\n",
    "        \n",
    "        # Print the first 3 subfolders\n",
    "        for subfolder in sorted(subfolders)[:3]:\n",
    "            print(f\"  Subfolder: {subfolder}\")\n",
    "        if len(subfolders) > 3:\n",
    "            print(\"  ... and more subfolders\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: OfficeHomeDataset_10072016/\n",
      "  Subfolder: OfficeHomeDataset_10072016/Art/\n",
      "  Subfolder: OfficeHomeDataset_10072016/Clipart/\n",
      "  Subfolder: OfficeHomeDataset_10072016/Product/\n",
      "  ... and more subfolders\n",
      "\n",
      "Folder: imagenet-a/\n",
      "  Subfolder: imagenet-a/n01498041/\n",
      "  Subfolder: imagenet-a/n01531178/\n",
      "  Subfolder: imagenet-a/n01534433/\n",
      "  ... and more subfolders\n",
      "\n",
      "Folder: imagenetv2-matched-frequency-format-val/\n",
      "  Subfolder: imagenetv2-matched-frequency-format-val/0/\n",
      "  Subfolder: imagenetv2-matched-frequency-format-val/1/\n",
      "  Subfolder: imagenetv2-matched-frequency-format-val/10/\n",
      "  ... and more subfolders\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the folders and first 3 subfolders of the AWS bucket\n",
    "s3_bucket = \"deeplearning2024-datasets\"\n",
    "s3_region = \"eu-west-1\"\n",
    "list_datasets_with_subfolders(s3_bucket, s3_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SERIALIZING OBJECTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store all the relevant information of each prediction, we are going to use the PredictionResult class. This dataclass stores for each image (from which we store the file name as well) we save the indeces and probabilities of the top 5 predicted classes, the true label index and probability and the Shanon entropy of the whole probability vector. So for each image we get a dictionary with all those values.\n",
    "\n",
    "For saving and loading this results we will use .json files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an object to store the result of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PredictionResult:\n",
    "    image_id: str  # Name of the image file\n",
    "    top_indices: List[int]\n",
    "    top_probabilities: List[float]\n",
    "    true_label: int\n",
    "    true_label_rank: int\n",
    "    true_label_probability: float\n",
    "    prediction_entropy: float\n",
    "\n",
    "    def __init__(self, name_img: str, true_label: int, prediction: int, probs: torch.Tensor, entropy: float, num_top: int = 5, decimal_places: int = 6):\n",
    "        self.image_id = name_img\n",
    "        self.true_label = true_label\n",
    "        # Get the top 'num_top' indices and probabilities\n",
    "        top_probs, top_indices = torch.topk(probs, num_top)\n",
    "        self.top_indices = top_indices.tolist()\n",
    "        self.top_probabilities = [round(prob.item(), decimal_places) for prob in top_probs]\n",
    "        # True label information\n",
    "        self.true_label_rank = (probs.argsort(descending=True) == true_label).nonzero(as_tuple=True)[0].item() + 1\n",
    "        self.true_label_probability = round(probs[prediction].item(), decimal_places)\n",
    "        # Prediction entropy\n",
    "        self.prediction_entropy = entropy\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data: dict):\n",
    "        obj = cls.__new__(cls)  # Create a new instance without calling __init__\n",
    "        # Directly assign the values\n",
    "        obj.image_id = data['image_id']\n",
    "        obj.top_indices  = data['top_indices']\n",
    "        obj.top_probabilities  = data['top_probabilities']\n",
    "        obj.true_label = data['true_label']\n",
    "        obj.true_label_rank = data['true_label_rank']\n",
    "        obj.true_label_probability = data['true_label_probability']\n",
    "        obj.prediction_entropy = data['prediction_entropy']\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 'example_image.jpg', 'top_indices': [2, 1, 3], 'top_probabilities': [0.4, 0.3, 0.2], 'true_label_index': 2, 'true_label_probability': 0.4, 'prediction_entropy': 0.97}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "name_img = 'example_image.jpg'\n",
    "prediction = 2\n",
    "probs = torch.tensor([0.1, 0.3, 0.4, 0.2])  # example tensor\n",
    "entropy = 0.97\n",
    "num_top = 3\n",
    "\n",
    "pred_result = PredictionResult(name_img, prediction, probs, entropy, num_top)\n",
    "print(pred_result.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a function to serialize a list of prediction objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serializing a list of Prediction objects\n",
    "\n",
    "# Function to serialize a list of PredictionResult objects\n",
    "def serialize_results(prediction_results: List[PredictionResult], file_path: str):\n",
    "    # Convert each PredictionResult object to a dictionary\n",
    "    results_as_dicts = [result.to_dict() for result in prediction_results]\n",
    "    \n",
    "    # Serialize the list of dictionaries to a JSON string\n",
    "    json_string = json.dumps(results_as_dicts, indent=4)\n",
    "    \n",
    "    # Write the JSON string to a file\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "prediction_results = [\n",
    "    PredictionResult('example_image1.jpg', 2, torch.tensor([0.1, 0.3, 0.4, 0.2]), 0.97, 3),\n",
    "    PredictionResult('example_image2.jpg', 0, torch.tensor([0.6, 0.2, 0.1, 0.1]), 0.85, 3)\n",
    "]\n",
    "\n",
    "serialize_results(prediction_results, 'prediction_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a function to load the serialized list of predictions from the .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to deserialize a list of PredictionResult objects from a file\n",
    "# Function to deserialize a list of PredictionResult objects from a file\n",
    "def deserialize_results(file_path: str) -> List[PredictionResult]:\n",
    "    # Read the JSON string from the file\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_string = f.read()\n",
    "    \n",
    "    # Deserialize the JSON string to a list of dictionaries\n",
    "    results_as_dicts = json.loads(json_string)\n",
    "    \n",
    "    # Convert each dictionary back to a PredictionResult object using from_dict\n",
    "    prediction_results = [PredictionResult.from_dict(d) for d in results_as_dicts]\n",
    "    \n",
    "    return prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 'example_image1.jpg', 'top_indices': [2, 1, 3], 'top_probabilities': [0.4, 0.3, 0.2], 'true_label_index': 2, 'true_label_probability': 0.4, 'prediction_entropy': 0.97}\n",
      "{'image_id': 'example_image2.jpg', 'top_indices': [0, 1, 3], 'top_probabilities': [0.6, 0.2, 0.1], 'true_label_index': 0, 'true_label_probability': 0.6, 'prediction_entropy': 0.85}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "deserialized_results = deserialize_results('prediction_results.json')\n",
    "for result in deserialized_results:\n",
    "    print(result.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing on testing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to inherit the dataset class and define our own version to access the AWS bucket. \n",
    "\n",
    "Note: for the imageneta we have some class identifiers which are of the form n23423523... So since for creating the class labels we have to use the class names, we get them from a map defined using the .txt file that I found in HuggingFace. Also, the part were it sorts those class identifiers... I just copied it from the notebook from moodle but for this dataset, doesn't make much sense. I still have to implement the classnames for imagenetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefine for getting the imagefile name as well\n",
    "class S3ImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None, imageneta=True):\n",
    "        self.s3_bucket = \"deeplearning2024-datasets\"\n",
    "        self.s3_region = \"eu-west-1\"\n",
    "        self.s3_client = boto3.client(\"s3\", region_name=self.s3_region, verify=True)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get list of objects in the bucket\n",
    "        response = self.s3_client.list_objects_v2(Bucket=self.s3_bucket, Prefix=root)\n",
    "        objects = response.get(\"Contents\", [])\n",
    "        while response.get(\"NextContinuationToken\"):\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Prefix=root,\n",
    "                ContinuationToken=response[\"NextContinuationToken\"]\n",
    "            )\n",
    "            objects.extend(response.get(\"Contents\", []))\n",
    "\n",
    "        # Iterate and keep valid files only\n",
    "        self.instances = []\n",
    "        self.samples = []  # Add samples attribute\n",
    "        for item in objects:\n",
    "            key = item[\"Key\"]\n",
    "            path = Path(key)\n",
    "            \n",
    "            # Check if file is valid\n",
    "            if path.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\"):\n",
    "                continue\n",
    "\n",
    "            # Get label\n",
    "            label = path.parent.name\n",
    "\n",
    "            # Keep track of valid instances\n",
    "            self.instances.append((label, key))\n",
    "            self.samples.append((key, label))  # Add to samples\n",
    "\n",
    "        # Sort classes in alphabetical order (as in ImageFolder)\n",
    "        self.classes = sorted(set(label for label, _ in self.instances)) #This contains keys of the form n0349583...\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        # Get the labels that will be used for the CLIP text_features\n",
    "        if imageneta:\n",
    "            self.class_to_name = self.map_classnames_imagenetA()\n",
    "            self.class_names = [self.class_to_name[cls] for cls in self.classes if cls in self.class_to_name]\n",
    "            self.idx_to_class_names = {value: self.class_to_name[key] for key, value in self.class_to_idx.items()}\n",
    "\n",
    "\n",
    "    \n",
    "    def map_classnames_imagenetA(self):\n",
    "        # Define the path to the words file\n",
    "        file_path = 'List_ClassNames_imageneta.txt'\n",
    "    \n",
    "        # Initialize an empty dictionary to store the class names with their corresponding IDs\n",
    "        class_dict = {}\n",
    "    \n",
    "        # Open and read the file line by line\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                # Split each line into WordNet ID and class name, and strip to remove any leading/trailing whitespace\n",
    "                parts = line.strip().split(' ', 1)\n",
    "                if len(parts) > 1:\n",
    "                    # Use the WordNet ID as the key and the class name as the value in the dictionary\n",
    "                    class_dict[parts[0]] = parts[1]\n",
    "    \n",
    "        return class_dict\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, key = self.instances[idx]\n",
    "        response = self.s3_client.get_object(Bucket=self.s3_bucket, Key=key)\n",
    "        img = Image.open(response['Body'])\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        filename = os.path.basename(key)  # Extract the filename from the key\n",
    "\n",
    "        return img, self.class_to_idx[label], filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is to create a subset with the same amount of images per each class. The class inherited is because I neede to do the initialization of the datasetclass to avoid some errors. Don't worry much about it. \n",
    "\n",
    "The subset that the function returns is just the same object as the full dataset but with a fixed amount of images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a subclass of Subset to include additional attributes\n",
    "class SubsetWithAttributes(Subset):\n",
    "    def __init__(self, dataset, indices):\n",
    "        super().__init__(dataset, indices)\n",
    "        self.classes = dataset.classes\n",
    "        self.class_to_idx = dataset.class_to_idx\n",
    "\n",
    "#Function for getting the subset\n",
    "def create_stratified_subset(dataset, num_samples_per_class=5):\n",
    "    # Fix the random seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Determine class indices\n",
    "    targets = np.array([dataset.class_to_idx[s[1]] for s in dataset.samples])\n",
    "    classes, class_indices = np.unique(targets, return_inverse=True)\n",
    "\n",
    "    # Select samples from each class\n",
    "    indices = []\n",
    "    for c in classes:\n",
    "        class_idx = np.where(class_indices == c)[0]\n",
    "        if len(class_idx) >= num_samples_per_class:\n",
    "            selected_indices = np.random.choice(class_idx, num_samples_per_class, replace=False)\n",
    "            indices.extend(selected_indices)\n",
    "        else:\n",
    "            # If a class has fewer than the desired number, take all\n",
    "            indices.extend(class_idx)\n",
    "\n",
    "    # Create subset\n",
    "    subset = SubsetWithAttributes(dataset, indices)\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of a subset of Imagenet-A with 5 samples per class (Remember to define imagenetA before creating a subset)\n",
    "subset = create_stratified_subset(imageneta, num_samples_per_class=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the testing function. It tokenizes the class labels of the dataset using the tokenize_labels. So if we want to use different labels, we should probably create another method of CLIP and put it here or modify the existing tokenize_labels to do something else than \"A photo of a {class}\".\n",
    "\n",
    "Note: In the implementation, inside the loop, I declaring the resultfilename in every single iteration. Which does not make sense from a performance point of view. But I did not want to put it outside the loop and put another 4 conditionals because I thought it would be less readable and the performance, would not be that much worse.\n",
    "\n",
    "Note 2: Also the average calculations, could be done with the data from the .json files, so there is really no point on doing it here but I left it since I had it from a previous implementation of the tests. But I think, we should leave the testing just to create the .json files and then use them to do all the computations afterwards, without having CLIP computing stuff around. Just used the already collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the testing function\n",
    "def testing(dataset, model, method='CLIP', batch_size=32, num_aug=20):\n",
    "    model.tokenize_labels(dataset.class_names)\n",
    "\n",
    "    def custom_collate_fn(batch):\n",
    "        images = [item[0] for item in batch]  # PIL images\n",
    "        labels = [item[1] for item in batch]  # Corresponding labels\n",
    "        filenames = [item[2] for item in batch]    # Corresponding keys/IDs\n",
    "        return images, labels, filenames\n",
    "    \n",
    "    #We use num_workers=4 but this only works well when using the gpu, not the cpu.\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn, num_workers=4)\n",
    "\n",
    "    # Initialize lists to store results\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    entropies = []\n",
    "    confidences = []\n",
    "    results = []\n",
    "\n",
    "    # Evaluation loop\n",
    "    for images, labels, filenames in tqdm(dataloader):  \n",
    "        for image, label, filename in zip(images, labels, filenames): \n",
    "            try:\n",
    "                # We choose how the test time predictions are made\n",
    "                if method == 'CLIP':\n",
    "                    prediction, probs, entropy = model.predict(image)\n",
    "                    resultsfilename = 'CLIP_Base_results.json'\n",
    "                elif method == 'MEMO':\n",
    "                    prediction, probs, entropy = model.MEMO(image, num_augmentations=num_aug)\n",
    "                    resultsfilename = 'CLIP_MEMO_results.json'\n",
    "                elif method == 'TPT':\n",
    "                    prediction, probs, entropy = model.TPT(image, num_augmentations=num_aug)\n",
    "                    resultsfilename = 'CLIP_TPT_results.json'\n",
    "                elif method == 'EB':\n",
    "                    prediction, probs, entropy = model.entropyboosting(image, num_augmentations=num_aug)\n",
    "                    resultsfilename = 'CLIP_EB_results.json'\n",
    "                else:\n",
    "                    print('Enter a valid method for testing.')\n",
    "\n",
    "                #Store the results of each image:\n",
    "                results.append(PredictionResult(filename, label, prediction, probs, entropy))\n",
    "\n",
    "                #Some computations for getting average results at the end\n",
    "                if int(prediction) == int(label):\n",
    "                    correct_predictions += 1\n",
    "                total_predictions += 1\n",
    "                entropies.append(entropy)\n",
    "                confidences.append(torch.max(probs).item())\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "    #Serializing results for each image:\n",
    "    serialize_results(results, resultsfilename)\n",
    "    \n",
    "    # Post evaluation general statistics\n",
    "    accuracy = (correct_predictions / total_predictions) * 100\n",
    "    average_entropy = sum(entropies) / len(entropies)\n",
    "    average_confidence = sum(confidences) / len(confidences)\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Average entropy across all predictions: {average_entropy:.2f}')\n",
    "    print(f'Average confidence across all predictions: {average_confidence:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After all the definitions here we create the dataset object and use the testing function with the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the imagenet-a\n",
    "s3_rootA = 'imagenet-a'\n",
    "imageneta = S3ImageFolder(root=s3_rootA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the imagenet-v2\n",
    "s3_rootV2 = 'imagenetv2-matched-frequency-format-val'\n",
    "imagenetv2 = S3ImageFolder(root=s3_rootV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stingray', 'goldfinch', 'junco', 'American robin', 'jay', 'bald eagle', 'vulture', 'newt', 'American bullfrog', 'box turtle', 'green iguana', 'agama', 'chameleon', 'American alligator', 'garter snake', 'harvestman', 'scorpion', 'tarantula', 'centipede', 'sulphur-crested cockatoo', 'lorikeet', 'hummingbird', 'toucan', 'duck', 'goose', 'koala', 'jellyfish', 'sea anemone', 'flatworm', 'snail', 'crayfish', 'hermit crab', 'flamingo', 'great egret', 'oystercatcher', 'pelican', 'sea lion', 'Chihuahua', 'Golden Retriever', 'Rottweiler', 'German Shepherd Dog', 'pug', 'red fox', 'Persian cat', 'lynx', 'lion', 'American black bear', 'mongoose', 'ladybug', 'rhinoceros beetle', 'weevil', 'fly', 'bee', 'ant', 'grasshopper', 'stick insect', 'cockroach', 'mantis', 'leafhopper', 'dragonfly', 'monarch butterfly', 'small white', 'gossamer-winged butterfly', 'starfish', 'cottontail rabbit', 'porcupine', 'fox squirrel', 'marmot', 'bison', 'skunk', 'armadillo', 'baboon', 'white-headed capuchin', 'African bush elephant', 'pufferfish', 'academic gown', 'accordion', 'acoustic guitar', 'airliner', 'ambulance', 'apron', 'balance beam', 'balloon', 'banjo', 'barn', 'wheelbarrow', 'basketball', 'lighthouse', 'beaker', 'bikini', 'bow', 'bow tie', 'breastplate', 'broom', 'candle', 'canoe', 'castle', 'cello', 'chain', 'chest', 'Christmas stocking', 'cowboy boot', 'cradle', 'rotary dial telephone', 'digital clock', 'doormat', 'drumstick', 'dumbbell', 'envelope', 'feather boa', 'flagpole', 'forklift', 'fountain', 'garbage truck', 'goblet', 'go-kart', 'golf cart', 'grand piano', 'hair dryer', 'clothes iron', \"jack-o'-lantern\", 'jeep', 'kimono', 'lighter', 'limousine', 'manhole cover', 'maraca', 'marimba', 'mask', 'mitten', 'mosque', 'nail', 'obelisk', 'ocarina', 'organ', 'parachute', 'parking meter', 'piggy bank', 'billiard table', 'hockey puck', 'quill', 'racket', 'reel', 'revolver', 'rocking chair', 'rugby ball', 'salt shaker', 'sandal', 'saxophone', 'school bus', 'schooner', 'sewing machine', 'shovel', 'sleeping bag', 'snowmobile', 'snowplow', 'soap dispenser', 'spatula', 'spider web', 'steam locomotive', 'stethoscope', 'couch', 'submarine', 'sundial', 'suspension bridge', 'syringe', 'tank', 'teddy bear', 'toaster', 'torch', 'tricycle', 'umbrella', 'unicycle', 'viaduct', 'volleyball', 'washing machine', 'water tower', 'wine bottle', 'shipwreck', 'guacamole', 'pretzel', 'cheeseburger', 'hot dog', 'broccoli', 'cucumber', 'bell pepper', 'mushroom', 'lemon', 'banana', 'custard apple', 'pomegranate', 'carbonara', 'bubble', 'cliff', 'volcano', 'baseball player', 'rapeseed', \"yellow lady's slipper\", 'corn', 'acorn']\n"
     ]
    }
   ],
   "source": [
    "# Class names from ImagenetA\n",
    "print(imageneta.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [01:42<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 29.85%\n",
      "Average entropy across all predictions: 3.16\n",
      "Average confidence across all predictions: 0.46\n"
     ]
    }
   ],
   "source": [
    "#subset = create_stratified_subset(imageneta, num_samples_per_class=1)\n",
    "testing(imageneta, clip_model, method='CLIP', batch_size=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [1:23:58<00:00, 23.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 30.81%\n",
      "Average entropy across all predictions: 2.69\n",
      "Average confidence across all predictions: 0.45\n"
     ]
    }
   ],
   "source": [
    "testing(imageneta, clip_model, method='EB', batch_size=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 213/215 [1:12:31<00:40, 20.08s/it]"
     ]
    }
   ],
   "source": [
    "testing(imageneta, clip_model, method='MEMO', batch_size=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [1:02:46<00:00, 17.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 27.60%\n",
      "Average entropy across all predictions: 4.70\n",
      "Average confidence across all predictions: 0.25\n"
     ]
    }
   ],
   "source": [
    "testing(imageneta, clip_model, method='TPT', batch_size=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is to compute some meaningful values from the data logged into the .json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(results_list: List[PredictionResult]) -> Dict[str, List[float]]:\n",
    "    # Dictionary to hold statistics for each class\n",
    "    class_stats = {}\n",
    "    # Global count of cases where the true label was among the top 5 predictions\n",
    "    global_top5 = 0\n",
    "    accumulated_true_label_rank = 0\n",
    "    accumulated_true_label_prob = 0\n",
    "    # Lists to hold entropies for all correctly and incorrectly predicted cases globally\n",
    "    correct_global_entropy = []\n",
    "    incorrect_global_entropy = []\n",
    "\n",
    "    # Process each prediction result in the list\n",
    "    for result in results_list:\n",
    "        class_index = result.true_label  # Extract the class index from the result\n",
    "\n",
    "        # Initialize statistics for the class if it hasn't been done yet\n",
    "        if class_index not in class_stats:\n",
    "            class_stats[class_index] = {\n",
    "                'total': 0,  # Total predictions for this class\n",
    "                'correct': 0,  # Count of correct predictions\n",
    "                'true_in_top5': 0,  # Count of times true label was in the top 5 predictions\n",
    "                'entropies': [],  # List of entropies for all predictions\n",
    "                'correct_entropies': [],  # List of entropies for correct predictions\n",
    "                'incorrect_entropies': [],  # List of entropies for incorrect predictions\n",
    "                'true_label_rank': [], # List of ranks of the true labels\n",
    "                'true_label_prob': [], # List of the probabilities of the true labels\n",
    "                'worst_image': \"\",\n",
    "                'rank_5_image': \"\",\n",
    "                'rank_4_image': \"\",\n",
    "                'rank_3_image': \"\",\n",
    "                'rank_2_image': \"\",\n",
    "                'rank_1_image': \"\",\n",
    "            }\n",
    "\n",
    "        # Update the total number of predictions for this class\n",
    "        class_stats[class_index]['total'] += 1\n",
    "\n",
    "        # Check if the true label is in the top 5 predictions\n",
    "        if class_index in result.top_indices:\n",
    "            class_stats[class_index]['true_in_top5'] += 1\n",
    "            global_top5 += 1  # Increment global top-5 counter\n",
    "\n",
    "        # Check if the prediction was correct (true label is the top prediction)\n",
    "        if class_index == result.top_indices[0]:\n",
    "            class_stats[class_index]['correct'] += 1\n",
    "            class_stats[class_index]['correct_entropies'].append(result.prediction_entropy)\n",
    "            correct_global_entropy.append(result.prediction_entropy)  # Add entropy to global correct list\n",
    "        else:\n",
    "            class_stats[class_index]['incorrect_entropies'].append(result.prediction_entropy)\n",
    "            incorrect_global_entropy.append(result.prediction_entropy)  # Add entropy to global incorrect list\n",
    "\n",
    "        # Record the entropy for this prediction to calculate the average later\n",
    "        class_stats[class_index]['entropies'].append(result.prediction_entropy)\n",
    "\n",
    "        #Add the filename of the worst performing image in the class\n",
    "        if class_stats[class_index]['worst_image'] == \"\":\n",
    "            class_stats[class_index]['worst_image'] = result.image_id\n",
    "        elif result.true_label_rank >= max(class_stats[class_index]['true_label_rank']):\n",
    "            class_stats[class_index]['worst_image'] = result.image_id\n",
    "        \n",
    "        # Add the rank and probability of the true label as well\n",
    "        class_stats[class_index]['true_label_rank'].append(result.true_label_rank)\n",
    "        class_stats[class_index]['true_label_prob'].append(result.true_label_probability)\n",
    "        \n",
    "        # Update global accumulated rank and probability of the true label\n",
    "        accumulated_true_label_rank = accumulated_true_label_rank + result.true_label_rank\n",
    "        accumulated_true_label_prob = accumulated_true_label_prob + result.true_label_probability\n",
    "        \n",
    "        #Storing one rank 1,2,3,4,5 image per class\n",
    "        if result.true_label_rank == 5:\n",
    "            class_stats[class_index]['rank_5_image'] = result.image_id\n",
    "        elif result.true_label_rank == 4:\n",
    "            class_stats[class_index]['rank_4_image'] = result.image_id\n",
    "        elif result.true_label_rank == 3:\n",
    "            class_stats[class_index]['rank_3_image'] = result.image_id\n",
    "        elif result.true_label_rank == 2:\n",
    "            class_stats[class_index]['rank_2_image'] = result.image_id\n",
    "        elif result.true_label_rank == 1:\n",
    "            class_stats[class_index]['rank_1_image'] = result.image_id\n",
    "\n",
    "\n",
    "    # Create a dictionary to store computed insights\n",
    "    # Here we store the global insights\n",
    "    result_insights = {\n",
    "    'accuracy_per_class': {}, # Accuracy for each class\n",
    "    'top5_per_class': {}, # Top-5 accuracy for each class\n",
    "    'global_top5': global_top5 / len(results_list),  # Global top-5 accuracy\n",
    "    'avg_entropy_per_class': {}, # Average entropy for each class\n",
    "    'avg_correct_entropy_per_class': {}, # Average entropy for correct predictions per class\n",
    "    'global_avg_correct_entropy': sum(correct_global_entropy) / len(correct_global_entropy) if correct_global_entropy else 0,\n",
    "    'avg_incorrect_entropy_per_class': {}, # Average entropy for incorrect predictions per class\n",
    "    'global_avg_incorrect_entropy': sum(incorrect_global_entropy) / len(incorrect_global_entropy) if incorrect_global_entropy else 0,\n",
    "    'avg_rank_per_class': {},  # Average rank of the true label per class\n",
    "    'global_avg_rank': accumulated_true_label_rank / len(results_list),# Global average rank of the true label\n",
    "    'avg_true_label_prob_per_class': {}, # Average probability of the true label per class\n",
    "    'global_avg_true_label_prob': accumulated_true_label_prob / len(results_list), # Global average probability of the true label\n",
    "    'worst_image_per_class':{}, #The image with the lowest rank in the class\n",
    "    'rank_5_image_per_class':{}, #Images where the true lable is exactly rank 5\n",
    "    'rank_4_image_per_class':{}, #Images where the true lable is exactly rank 4\n",
    "    'rank_3_image_per_class':{}, #Images where the true lable is exactly rank 3\n",
    "    'rank_2_image_per_class':{}, #Images where the true lable is exactly rank 2\n",
    "    'rank_1_image_per_class':{}, #Images where the true lable is exactly rank 1\n",
    "}\n",
    "\n",
    "    # Here we compute and store also the insights per class into the result_insights\n",
    "    for idx, stats in class_stats.items():\n",
    "        total = stats['total']\n",
    "        correct = stats['correct']\n",
    "        top5 = stats['true_in_top5']\n",
    "        entropies = stats['entropies']\n",
    "        correct_entropies = stats['correct_entropies']\n",
    "        incorrect_entropies = stats['incorrect_entropies']\n",
    "        true_label_rank = stats['true_label_rank']\n",
    "        true_label_prob = stats['true_label_prob']\n",
    "        worst_image = stats['worst_image']\n",
    "        rank_5_image = stats['rank_5_image']\n",
    "        rank_4_image = stats['rank_4_image']\n",
    "        rank_3_image = stats['rank_3_image']\n",
    "        rank_2_image = stats['rank_2_image']\n",
    "        rank_1_image = stats['rank_1_image']\n",
    "\n",
    "        # Compute and store class-specific statistics\n",
    "        result_insights['accuracy_per_class'][idx] = correct / total if total > 0 else 0\n",
    "        result_insights['top5_per_class'][idx] = top5 / total if total > 0 else 0\n",
    "        result_insights['avg_entropy_per_class'][idx] = sum(entropies) / total if total > 0 else 0\n",
    "        if correct_entropies:\n",
    "            result_insights['avg_correct_entropy_per_class'][idx] = sum(correct_entropies) / len(correct_entropies)\n",
    "        else:\n",
    "            result_insights['avg_correct_entropy_per_class'][idx] = 0  # No correct predictions in this class\n",
    "        if incorrect_entropies:\n",
    "            result_insights['avg_incorrect_entropy_per_class'][idx] = sum(incorrect_entropies) / len(incorrect_entropies)\n",
    "        else:\n",
    "            result_insights['avg_incorrect_entropy_per_class'][idx] = 0  # No incorrect predictions in this class\n",
    "        result_insights['avg_rank_per_class'][idx] = sum(true_label_rank) / total\n",
    "        result_insights['avg_true_label_prob_per_class'][idx] = sum(true_label_prob) / total\n",
    "        result_insights['worst_image_per_class'][idx] = worst_image\n",
    "        result_insights['rank_5_image_per_class'][idx] = rank_5_image\n",
    "        result_insights['rank_4_image_per_class'][idx] = rank_4_image\n",
    "        result_insights['rank_3_image_per_class'][idx] = rank_3_image\n",
    "        result_insights['rank_2_image_per_class'][idx] = rank_2_image\n",
    "        result_insights['rank_1_image_per_class'][idx] = rank_1_image\n",
    "    \n",
    "    return result_insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to visualize this results we will define some other functions: One for visualizing heatmaps for each class data and another for printing the some global results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_insights(result_insights, class_to_name):\n",
    "    # Create a DataFrame with the insights\n",
    "    df = pd.DataFrame({\n",
    "        'Accuracy': result_insights['accuracy_per_class'],\n",
    "        'Top-5 Accuracy': result_insights['top5_per_class'],\n",
    "        'Average Entropy': result_insights['avg_entropy_per_class'],\n",
    "        'Average Correct Entropy': result_insights['avg_correct_entropy_per_class'],\n",
    "        'Average Incorrect Entropy': result_insights['avg_incorrect_entropy_per_class'],\n",
    "        'Average True Label Rank': result_insights['avg_rank_per_class'],\n",
    "        'Average True Label Probability': result_insights['avg_true_label_prob_per_class'],\n",
    "    })\n",
    "\n",
    "    # Map class indices to names using the provided dictionary\n",
    "    df.index = [class_to_name.get(i, f\"Class {i}\") for i in df.index]\n",
    "\n",
    "    # Create a dropdown menu for selecting the metric\n",
    "    dropdown_metric = widgets.Dropdown(\n",
    "        options=list(df.columns),\n",
    "        value='Accuracy',\n",
    "        description='Metric:',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    # Create another dropdown for selecting the number of extremes\n",
    "    dropdown_extremes = widgets.Dropdown(\n",
    "        options=[('All Classes', None), ('Top & Bottom 5', 5), ('Top & Bottom 10', 10), ('Top & Bottom 15', 15)],\n",
    "        value=None,\n",
    "        description='Extremes:',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    def update_plot(metric, extremes):\n",
    "        plt.figure(figsize=(20, 10))  # Adjust figure size to better fit the heatmap\n",
    "        data = df[[metric]].sort_values(by=metric, ascending=False)\n",
    "        \n",
    "        # Decide whether to show annotations based on whether extremes are selected\n",
    "        annotate = False if extremes is None else True\n",
    "        \n",
    "        # If extremes is set, filter data to show only the specified top & bottom N values\n",
    "        if extremes:\n",
    "            top_data = data.nlargest(extremes, metric)\n",
    "            bottom_data = data.nsmallest(extremes, metric)\n",
    "            data = pd.concat([top_data, bottom_data]).sort_values(by=metric, ascending=False)\n",
    "\n",
    "        sns.heatmap(data, annot=annotate, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
    "        plt.title(f'{metric} per Class with {extremes if extremes else \"All\"} Extremes')\n",
    "        plt.ylabel('Class Name')\n",
    "        plt.xlabel('Metric Value')\n",
    "        plt.yticks(rotation=0)  # Ensure class names are readable\n",
    "        plt.xticks(rotation=0)  # Ensure metric names are readable\n",
    "        plt.show()\n",
    "\n",
    "    # Display the dropdowns and connect the interactive output\n",
    "    display(dropdown_metric, dropdown_extremes)\n",
    "    out = widgets.interactive_output(update_plot, {'metric': dropdown_metric, 'extremes': dropdown_extremes})\n",
    "    display(out)\n",
    "\n",
    "# Example usage:\n",
    "# class_to_name = {0: 'Cat', 1: 'Dog', ...}\n",
    "# plot_class_insights(result_insights, class_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_global_insights(result_insights):\n",
    "    # Define title and header\n",
    "    title = \"Global Insights\"\n",
    "    print(f\"{title:^60}\")  # Center the title in a 60-char wide line\n",
    "    print(\"-\" * 60)  # Print a divider line\n",
    "    \n",
    "    # Display global results with formatted strings for better alignment\n",
    "    print(f\"{'Global Top-5 Accuracy:':<40} {result_insights['global_top5']:.2%}\")\n",
    "    print(f\"{'Global Average Correct Entropy:':<40} {result_insights['global_avg_correct_entropy']:.4f}\")\n",
    "    print(f\"{'Global Average Incorrect Entropy:':<40} {result_insights['global_avg_incorrect_entropy']:.4f}\")\n",
    "    print(f\"{'Global Average Rank of True Label:':<40} {result_insights['global_avg_rank']:.2f}\")\n",
    "    print(f\"{'Global Average Probability of True Label:':<40} {result_insights['global_avg_true_label_prob']:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 60)  # Print a closing divider line\n",
    "# Example usage:\n",
    "# display_global_insights(result_insights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to display some images according to their ranks. For that we define the following functions. \n",
    "\n",
    "For getting the image from the dataset given a list of image filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_by_filenames(dataset, target_filenames, batch_size=20):\n",
    "    def custom_collate_fn(batch):\n",
    "        images = [item[0] for item in batch]  # PIL images\n",
    "        labels = [item[1] for item in batch]  # Corresponding labels\n",
    "        filenames = [item[2] for item in batch]  # Corresponding filenames\n",
    "        return images, labels, filenames\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn, num_workers=4)\n",
    "\n",
    "    found_items = []  # List to store found images, labels, and filenames\n",
    "\n",
    "    # Iterate over the entire dataset to find the target filenames\n",
    "    for images, labels, filenames in dataloader:\n",
    "        for image, label, filename in zip(images, labels, filenames):\n",
    "            if filename in target_filenames:\n",
    "                found_items.append([image, label, filename])\n",
    "                # Remove found filename from target_filenames to avoid searching for it again\n",
    "                target_filenames.remove(filename)\n",
    "\n",
    "    # Check if any items were found\n",
    "    if not found_items:\n",
    "        print('No images found')\n",
    "        return []\n",
    "\n",
    "    return found_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For displaying in every class, which image had the lowest rank for the true class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_worst_image_per_class(prediction_results, result_insights, idx_to_class_names, dataset):\n",
    "    # Convert list to dictionary for easier access\n",
    "    results_dict = {result.image_id: result for result in prediction_results}\n",
    "\n",
    "    # Preload all worst images once\n",
    "    worst_filenames = list(result_insights['worst_image_per_class'].values())\n",
    "    images_data = get_images_by_filenames(dataset, worst_filenames)\n",
    "    image_dict = {item[2]: (item[0], item[1]) for item in images_data}\n",
    "    \n",
    "    # Create dropdown options from class names\n",
    "    dropdown_options = {idx_to_class_names[idx]: idx for idx in result_insights['worst_image_per_class'].keys()}\n",
    "    dropdown = Dropdown(options=dropdown_options)\n",
    "\n",
    "    # Output widget for displaying images and stats\n",
    "    output = Output()\n",
    "    save_button = Button(description=\"Save Image\")\n",
    "\n",
    "    current_image = None\n",
    "    current_filename = \"\"\n",
    "\n",
    "    def update_output(change):\n",
    "        nonlocal current_image, current_filename\n",
    "        class_idx = dropdown.value\n",
    "        filename = result_insights['worst_image_per_class'][class_idx]\n",
    "        rank = results_dict[filename].true_label_rank\n",
    "\n",
    "        try:\n",
    "            # Retrieve preloaded image and label\n",
    "            image, label = image_dict[filename]\n",
    "            current_image = image\n",
    "            current_filename = filename\n",
    "            # Display image and stats\n",
    "            output.clear_output()\n",
    "            with output:\n",
    "                plt.imshow(image)\n",
    "                plt.axis('off')\n",
    "                plt.title(f\"Class: {idx_to_class_names[class_idx]}, Rank: {rank}\")\n",
    "                plt.show()\n",
    "        except KeyError:\n",
    "            print(f\"KeyError: Filename '{filename}' not found in image_dict.\")\n",
    "            output.clear_output()\n",
    "            with output:\n",
    "                print(f\"Filename '{filename}' not found. Check dataset consistency.\")\n",
    "\n",
    "    def on_save(b):\n",
    "        nonlocal current_image, current_filename\n",
    "        if current_image and current_filename:\n",
    "            save_path = os.path.join(\"Saved Images\", f\"{current_filename}.png\")\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            plt.imsave(save_path, current_image)  # Ensure image is saved correctly\n",
    "            print(f\"Image saved as {save_path}\")\n",
    "\n",
    "    save_button.on_click(on_save)\n",
    "\n",
    "    # Listen to changes in the dropdown value\n",
    "    dropdown.observe(update_output, names='value')\n",
    "    \n",
    "    # Initial call to update the output for the default selection\n",
    "    update_output({'new': dropdown.value})\n",
    "\n",
    "    # Display widgets\n",
    "    display(VBox([dropdown, output, save_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for displaying only in the top 5 and bottom 5 accuracy classes, the rank 1-5 and lowest rank true predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function displays some images where the true label ranked in a particular way\n",
    "# You can only chooose between top 5 and bottom 5 accuracy classes\n",
    "def display_image_by_rank(prediction_results, result_insights, idx_to_class_names, dataset):\n",
    "    results_dict = {result.image_id: result for result in prediction_results}\n",
    "    sorted_classes_by_accuracy = sorted(result_insights['accuracy_per_class'].items(), key=lambda item: item[1])\n",
    "    top_bottom_classes = sorted_classes_by_accuracy[:5] + sorted_classes_by_accuracy[-5:]\n",
    "    class_indices = [cls[0] for cls in top_bottom_classes]\n",
    "    filenames_to_load = [fn for idx in class_indices for fn in [\n",
    "        result_insights['rank_1_image_per_class'].get(idx),\n",
    "        result_insights['rank_2_image_per_class'].get(idx),\n",
    "        result_insights['rank_3_image_per_class'].get(idx),\n",
    "        result_insights['rank_4_image_per_class'].get(idx),\n",
    "        result_insights['rank_5_image_per_class'].get(idx),\n",
    "        result_insights['worst_image_per_class'].get(idx),\n",
    "    ] if fn]\n",
    "    images_data = get_images_by_filenames(dataset, filenames_to_load)\n",
    "    image_dict = {item[2]: item[0] for item in images_data}\n",
    "    \n",
    "    current_image = None\n",
    "    current_filename = \"\"\n",
    "\n",
    "    class_dropdown = Dropdown(options={idx_to_class_names[idx]: idx for idx in class_indices})\n",
    "    rank_dropdown = Dropdown(options=['Rank 1', 'Rank 2', 'Rank 3', 'Rank 4', 'Rank 5', 'Worst Image'])\n",
    "    output = Output()\n",
    "    save_button = Button(description=\"Save Image\")\n",
    "\n",
    "    def update_output(change):\n",
    "        nonlocal current_image, current_filename\n",
    "        class_idx = class_dropdown.value\n",
    "        rank_choice = rank_dropdown.value\n",
    "        rank_key_map = {\n",
    "            'Rank 1': 'rank_1_image_per_class',\n",
    "            'Rank 2': 'rank_2_image_per_class',\n",
    "            'Rank 3': 'rank_3_image_per_class',\n",
    "            'Rank 4': 'rank_4_image_per_class',\n",
    "            'Rank 5': 'rank_5_image_per_class',\n",
    "            'Worst Image': 'worst_image_per_class'\n",
    "        }\n",
    "        filename = result_insights[rank_key_map[rank_choice]].get(class_idx, '')\n",
    "        image = image_dict.get(filename, None)\n",
    "        current_image = image\n",
    "        current_filename = filename\n",
    "        if image:\n",
    "            rank = results_dict[filename].true_label_rank if rank_choice == 'Worst Image' else rank_choice.replace(\"Rank \", \"\")\n",
    "            output.clear_output()\n",
    "            with output:\n",
    "                plt.imshow(image)\n",
    "                plt.axis('off')\n",
    "                plt.title(f\"Class: {idx_to_class_names[class_idx]}, Image Rank: {rank}\")\n",
    "                plt.show()\n",
    "        else:\n",
    "            output.clear_output()\n",
    "            with output:\n",
    "                print(f\"No image found for '{rank_choice}' in class {idx_to_class_names[class_idx]}.\")\n",
    "\n",
    "    def on_save(b):\n",
    "        nonlocal current_image, current_filename\n",
    "        if current_image and current_filename:\n",
    "            save_path = os.path.join(\"Saved Images\", f\"{current_filename}.png\")\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            current_image.save(save_path)\n",
    "            print(f\"Image saved as {save_path}\")\n",
    "\n",
    "    save_button.on_click(on_save)\n",
    "\n",
    "    class_dropdown.observe(update_output, names='value')\n",
    "    rank_dropdown.observe(update_output, names='value')\n",
    "    display(VBox([class_dropdown, rank_dropdown, output, save_button]))\n",
    "    update_output({'new': class_dropdown.value})  # Trigger initial display update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the imagenet-a dataset if you haven't run it before\n",
    "s3_rootA = 'imagenet-a'\n",
    "imageneta = S3ImageFolder(root=s3_rootA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .json file and compute the insights\n",
    "results_list= deserialize_results('CLIP_Base_results.json')\n",
    "result_insights = analyze_results(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba9cb6967f241dbba8f1b4778bc2edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Metric:', options=('Accuracy', 'Top-5 Accuracy', 'Average Entropy', 'Average Correct Ent…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5afb65d2c28941e199a68fd792ea6729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Extremes:', options=(('All Classes', None), ('Top & Bottom 5', 5), ('Top & Bottom 10', 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50507b8be2124886bbfb91c48d6bcd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display per class insights\n",
    "plot_class_insights(result_insights, imageneta.idx_to_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa55a30fcbe4494db8fa42f734cf2a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(options={'German Shepherd Dog': 40, 'weevil': 50, 'baboon': 71, 'drumstick': 106, 'nai…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image_by_rank(results_list, result_insights, imageneta.idx_to_class_names, imageneta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/process.py\", line 317, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/multiprocessing/queues.py\", line 199, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 1096, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/opt/conda/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "display_worst_image_per_class(results_list, result_insights, imageneta.idx_to_class_names, imageneta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Global Insights                       \n",
      "------------------------------------------------------------\n",
      "Global Top-5 Accuracy:                   61.53%\n",
      "Global Average Correct Entropy:          2.3933\n",
      "Global Average Incorrect Entropy:        3.4909\n",
      "Global Average Rank of True Label:       12.83\n",
      "Global Average Probability of True Label: 0.4563\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display global insights\n",
    "display_global_insights(result_insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, if CLIP was able to distinguish among the top 5 classes which is the correct one, it would double its accuracy accross the entire dataset.\n",
    "\n",
    "Now, to see whether \"good\" crops might help the CLIP classify better or not, we are going to get some of the worst images predicted in the class with the least accuracy and crop them by hand.\n",
    "\n",
    "First, we define a function to retrieve a specific image given its filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CLIP with MEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .json file and compute the insights\n",
    "results_listMEMO= deserialize_results('CLIP_MEMO_results.json')\n",
    "result_insightsMEMO = analyze_results(results_listMEMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display per class insights\n",
    "plot_class_insights(result_insightsMEMO, imageneta.idx_to_class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display global insights\n",
    "display_global_insights(result_insightsMEMO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Depth-Anything' already exists and is not an empty directory.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libdrm-amdgpu1 libdrm-common libdrm-intel1 libdrm-nouveau2 libdrm-radeon1\n",
      "  libdrm2 libelf1 libgl1 libgl1-amber-dri libgl1-mesa-dri libglapi-mesa\n",
      "  libglvnd0 libglx-mesa0 libglx0 libllvm15 libpciaccess0 libsensors-config\n",
      "  libsensors5 libx11-6 libx11-data libx11-xcb1 libxau6 libxcb-dri2-0\n",
      "  libxcb-dri3-0 libxcb-glx0 libxcb-present0 libxcb-randr0 libxcb-shm0\n",
      "  libxcb-sync1 libxcb-xfixes0 libxcb1 libxdmcp6 libxext6 libxfixes3\n",
      "  libxshmfence1 libxxf86vm1\n",
      "Suggested packages:\n",
      "  pciutils lm-sensors\n",
      "The following NEW packages will be installed:\n",
      "  libdrm-amdgpu1 libdrm-common libdrm-intel1 libdrm-nouveau2 libdrm-radeon1\n",
      "  libdrm2 libelf1 libgl1 libgl1-amber-dri libgl1-mesa-dri libgl1-mesa-glx\n",
      "  libglapi-mesa libglvnd0 libglx-mesa0 libglx0 libllvm15 libpciaccess0\n",
      "  libsensors-config libsensors5 libx11-6 libx11-data libx11-xcb1 libxau6\n",
      "  libxcb-dri2-0 libxcb-dri3-0 libxcb-glx0 libxcb-present0 libxcb-randr0\n",
      "  libxcb-shm0 libxcb-sync1 libxcb-xfixes0 libxcb1 libxdmcp6 libxext6\n",
      "  libxfixes3 libxshmfence1 libxxf86vm1\n",
      "0 upgraded, 37 newly installed, 0 to remove and 13 not upgraded.\n",
      "Need to get 40.2 MB of archives.\n",
      "After this operation, 173 MB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libelf1 amd64 0.186-1build1 [51.0 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-common all 2.4.113-2~ubuntu0.22.04.1 [5450 B]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm2 amd64 2.4.113-2~ubuntu0.22.04.1 [38.1 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxau6 amd64 1:1.0.9-1build5 [7634 B]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxdmcp6 amd64 1:1.1.3-0ubuntu5 [10.9 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb1 amd64 1.14-3ubuntu3 [49.0 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libx11-data all 2:1.7.5-1ubuntu0.3 [120 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libx11-6 amd64 2:1.7.5-1ubuntu0.3 [667 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxext6 amd64 2:1.3.4-1build1 [31.8 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-amdgpu1 amd64 2.4.113-2~ubuntu0.22.04.1 [19.9 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpciaccess0 amd64 0.16-3 [19.1 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-intel1 amd64 2.4.113-2~ubuntu0.22.04.1 [66.7 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-nouveau2 amd64 2.4.113-2~ubuntu0.22.04.1 [17.5 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-radeon1 amd64 2.4.113-2~ubuntu0.22.04.1 [21.6 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libglapi-mesa amd64 23.2.1-1ubuntu3.1~22.04.2 [37.1 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-amber-dri amd64 21.3.9-0ubuntu1~22.04.1 [4218 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libllvm15 amd64 1:15.0.7-0ubuntu0.22.04.3 [25.4 MB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsensors-config all 1:3.6.0-7ubuntu1 [5274 B]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsensors5 amd64 1:3.6.0-7ubuntu1 [26.3 kB]\n",
      "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-dri3-0 amd64 1.14-3ubuntu3 [6968 B]\n",
      "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dri amd64 23.2.1-1ubuntu3.1~22.04.2 [8860 kB]\n",
      "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd0 amd64 1.4.0-1 [73.6 kB]\n",
      "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libx11-xcb1 amd64 2:1.7.5-1ubuntu0.3 [7802 B]\n",
      "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-dri2-0 amd64 1.14-3ubuntu3 [7206 B]\n",
      "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-glx0 amd64 1.14-3ubuntu3 [25.9 kB]\n",
      "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-present0 amd64 1.14-3ubuntu3 [5734 B]\n",
      "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-randr0 amd64 1.14-3ubuntu3 [18.3 kB]\n",
      "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-shm0 amd64 1.14-3ubuntu3 [5780 B]\n",
      "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-sync1 amd64 1.14-3ubuntu3 [9416 B]\n",
      "Get:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xfixes0 amd64 1.14-3ubuntu3 [9996 B]\n",
      "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfixes3 amd64 1:6.0.0-1 [11.7 kB]\n",
      "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxshmfence1 amd64 1.3-1build4 [5394 B]\n",
      "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86vm1 amd64 1:1.1.4-1build3 [10.4 kB]\n",
      "Get:34 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libglx-mesa0 amd64 23.2.1-1ubuntu3.1~22.04.2 [158 kB]\n",
      "Get:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx0 amd64 1.4.0-1 [41.0 kB]\n",
      "Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl1 amd64 1.4.0-1 [110 kB]\n",
      "Get:37 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libgl1-mesa-glx amd64 23.0.4-0ubuntu1~22.04.1 [5584 B]\n",
      "Fetched 40.2 MB in 2s (18.3 MB/s)          \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package libelf1:amd64.\n",
      "(Reading database ... 13791 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libelf1_0.186-1build1_amd64.deb ...\n",
      "Unpacking libelf1:amd64 (0.186-1build1) ...\n",
      "Selecting previously unselected package libdrm-common.\n",
      "Preparing to unpack .../01-libdrm-common_2.4.113-2~ubuntu0.22.04.1_all.deb ...\n",
      "Unpacking libdrm-common (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libdrm2:amd64.\n",
      "Preparing to unpack .../02-libdrm2_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libdrm2:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libxau6:amd64.\n",
      "Preparing to unpack .../03-libxau6_1%3a1.0.9-1build5_amd64.deb ...\n",
      "Unpacking libxau6:amd64 (1:1.0.9-1build5) ...\n",
      "Selecting previously unselected package libxdmcp6:amd64.\n",
      "Preparing to unpack .../04-libxdmcp6_1%3a1.1.3-0ubuntu5_amd64.deb ...\n",
      "Unpacking libxdmcp6:amd64 (1:1.1.3-0ubuntu5) ...\n",
      "Selecting previously unselected package libxcb1:amd64.\n",
      "Preparing to unpack .../05-libxcb1_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb1:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libx11-data.\n",
      "Preparing to unpack .../06-libx11-data_2%3a1.7.5-1ubuntu0.3_all.deb ...\n",
      "Unpacking libx11-data (2:1.7.5-1ubuntu0.3) ...\n",
      "Selecting previously unselected package libx11-6:amd64.\n",
      "Preparing to unpack .../07-libx11-6_2%3a1.7.5-1ubuntu0.3_amd64.deb ...\n",
      "Unpacking libx11-6:amd64 (2:1.7.5-1ubuntu0.3) ...\n",
      "Selecting previously unselected package libxext6:amd64.\n",
      "Preparing to unpack .../08-libxext6_2%3a1.3.4-1build1_amd64.deb ...\n",
      "Unpacking libxext6:amd64 (2:1.3.4-1build1) ...\n",
      "Selecting previously unselected package libdrm-amdgpu1:amd64.\n",
      "Preparing to unpack .../09-libdrm-amdgpu1_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libdrm-amdgpu1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libpciaccess0:amd64.\n",
      "Preparing to unpack .../10-libpciaccess0_0.16-3_amd64.deb ...\n",
      "Unpacking libpciaccess0:amd64 (0.16-3) ...\n",
      "Selecting previously unselected package libdrm-intel1:amd64.\n",
      "Preparing to unpack .../11-libdrm-intel1_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libdrm-intel1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libdrm-nouveau2:amd64.\n",
      "Preparing to unpack .../12-libdrm-nouveau2_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libdrm-nouveau2:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libdrm-radeon1:amd64.\n",
      "Preparing to unpack .../13-libdrm-radeon1_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
      "Unpacking libdrm-radeon1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Selecting previously unselected package libglapi-mesa:amd64.\n",
      "Preparing to unpack .../14-libglapi-mesa_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n",
      "Unpacking libglapi-mesa:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
      "Selecting previously unselected package libgl1-amber-dri:amd64.\n",
      "Preparing to unpack .../15-libgl1-amber-dri_21.3.9-0ubuntu1~22.04.1_amd64.deb ...\n",
      "Unpacking libgl1-amber-dri:amd64 (21.3.9-0ubuntu1~22.04.1) ...\n",
      "Selecting previously unselected package libllvm15:amd64.\n",
      "Preparing to unpack .../16-libllvm15_1%3a15.0.7-0ubuntu0.22.04.3_amd64.deb ...\n",
      "Unpacking libllvm15:amd64 (1:15.0.7-0ubuntu0.22.04.3) ...\n",
      "Selecting previously unselected package libsensors-config.\n",
      "Preparing to unpack .../17-libsensors-config_1%3a3.6.0-7ubuntu1_all.deb ...\n",
      "Unpacking libsensors-config (1:3.6.0-7ubuntu1) ...\n",
      "Selecting previously unselected package libsensors5:amd64.\n",
      "Preparing to unpack .../18-libsensors5_1%3a3.6.0-7ubuntu1_amd64.deb ...\n",
      "Unpacking libsensors5:amd64 (1:3.6.0-7ubuntu1) ...\n",
      "Selecting previously unselected package libxcb-dri3-0:amd64.\n",
      "Preparing to unpack .../19-libxcb-dri3-0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-dri3-0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libgl1-mesa-dri:amd64.\n",
      "Preparing to unpack .../20-libgl1-mesa-dri_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n",
      "Unpacking libgl1-mesa-dri:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
      "Selecting previously unselected package libglvnd0:amd64.\n",
      "Preparing to unpack .../21-libglvnd0_1.4.0-1_amd64.deb ...\n",
      "Unpacking libglvnd0:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libx11-xcb1:amd64.\n",
      "Preparing to unpack .../22-libx11-xcb1_2%3a1.7.5-1ubuntu0.3_amd64.deb ...\n",
      "Unpacking libx11-xcb1:amd64 (2:1.7.5-1ubuntu0.3) ...\n",
      "Selecting previously unselected package libxcb-dri2-0:amd64.\n",
      "Preparing to unpack .../23-libxcb-dri2-0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-dri2-0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-glx0:amd64.\n",
      "Preparing to unpack .../24-libxcb-glx0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-glx0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-present0:amd64.\n",
      "Preparing to unpack .../25-libxcb-present0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-present0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-randr0:amd64.\n",
      "Preparing to unpack .../26-libxcb-randr0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-randr0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-shm0:amd64.\n",
      "Preparing to unpack .../27-libxcb-shm0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-shm0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-sync1:amd64.\n",
      "Preparing to unpack .../28-libxcb-sync1_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-sync1:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxcb-xfixes0:amd64.\n",
      "Preparing to unpack .../29-libxcb-xfixes0_1.14-3ubuntu3_amd64.deb ...\n",
      "Unpacking libxcb-xfixes0:amd64 (1.14-3ubuntu3) ...\n",
      "Selecting previously unselected package libxfixes3:amd64.\n",
      "Preparing to unpack .../30-libxfixes3_1%3a6.0.0-1_amd64.deb ...\n",
      "Unpacking libxfixes3:amd64 (1:6.0.0-1) ...\n",
      "Selecting previously unselected package libxshmfence1:amd64.\n",
      "Preparing to unpack .../31-libxshmfence1_1.3-1build4_amd64.deb ...\n",
      "Unpacking libxshmfence1:amd64 (1.3-1build4) ...\n",
      "Selecting previously unselected package libxxf86vm1:amd64.\n",
      "Preparing to unpack .../32-libxxf86vm1_1%3a1.1.4-1build3_amd64.deb ...\n",
      "Unpacking libxxf86vm1:amd64 (1:1.1.4-1build3) ...\n",
      "Selecting previously unselected package libglx-mesa0:amd64.\n",
      "Preparing to unpack .../33-libglx-mesa0_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n",
      "Unpacking libglx-mesa0:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
      "Selecting previously unselected package libglx0:amd64.\n",
      "Preparing to unpack .../34-libglx0_1.4.0-1_amd64.deb ...\n",
      "Unpacking libglx0:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libgl1:amd64.\n",
      "Preparing to unpack .../35-libgl1_1.4.0-1_amd64.deb ...\n",
      "Unpacking libgl1:amd64 (1.4.0-1) ...\n",
      "Selecting previously unselected package libgl1-mesa-glx:amd64.\n",
      "Preparing to unpack .../36-libgl1-mesa-glx_23.0.4-0ubuntu1~22.04.1_amd64.deb ...\n",
      "Unpacking libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
      "Setting up libpciaccess0:amd64 (0.16-3) ...\n",
      "Setting up libxau6:amd64 (1:1.0.9-1build5) ...\n",
      "Setting up libxdmcp6:amd64 (1:1.1.3-0ubuntu5) ...\n",
      "Setting up libxcb1:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libxcb-xfixes0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libglvnd0:amd64 (1.4.0-1) ...\n",
      "Setting up libxcb-glx0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libsensors-config (1:3.6.0-7ubuntu1) ...\n",
      "Setting up libxcb-shm0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libxcb-present0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libx11-data (2:1.7.5-1ubuntu0.3) ...\n",
      "Setting up libxcb-sync1:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libsensors5:amd64 (1:3.6.0-7ubuntu1) ...\n",
      "Setting up libglapi-mesa:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
      "Setting up libxcb-dri2-0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libxshmfence1:amd64 (1.3-1build4) ...\n",
      "Setting up libxcb-randr0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libllvm15:amd64 (1:15.0.7-0ubuntu0.22.04.3) ...\n",
      "Setting up libx11-6:amd64 (2:1.7.5-1ubuntu0.3) ...\n",
      "Setting up libdrm-common (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Setting up libelf1:amd64 (0.186-1build1) ...\n",
      "Setting up libxcb-dri3-0:amd64 (1.14-3ubuntu3) ...\n",
      "Setting up libx11-xcb1:amd64 (2:1.7.5-1ubuntu0.3) ...\n",
      "Setting up libxext6:amd64 (2:1.3.4-1build1) ...\n",
      "Setting up libxxf86vm1:amd64 (1:1.1.4-1build3) ...\n",
      "Setting up libxfixes3:amd64 (1:6.0.0-1) ...\n",
      "Setting up libdrm2:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Setting up libdrm-amdgpu1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Setting up libdrm-nouveau2:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Setting up libdrm-radeon1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Setting up libdrm-intel1:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
      "Setting up libgl1-mesa-dri:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
      "Setting up libgl1-amber-dri:amd64 (21.3.9-0ubuntu1~22.04.1) ...\n",
      "Setting up libglx-mesa0:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
      "Setting up libglx0:amd64 (1.4.0-1) ...\n",
      "Setting up libgl1:amd64 (1.4.0-1) ...\n",
      "Setting up libgl1-mesa-glx:amd64 (23.0.4-0ubuntu1~22.04.1) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.6) ...\n",
      "Requirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python-headless) (1.26.4)\n",
      "/home/sagemaker-user/ImageClassificationAssignment/Depth-Anything\n",
      "Requirement already satisfied: gradio_imageslider in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (0.0.20)\n",
      "Requirement already satisfied: gradio==4.14.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (4.14.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.3.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.15.2a0+072ec57)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (4.9.0.80)\n",
      "Requirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.22.2)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (5.3.0)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (0.103.2)\n",
      "Requirement already satisfied: ffmpy in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: gradio-client==0.8.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (0.8.0)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (0.27.0)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (6.4.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (3.1.3)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (2.1.5)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (3.8.4)\n",
      "Requirement already satisfied: numpy~=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (3.9.15)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (2.1.4)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (9.5.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (2.7.1)\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (0.0.9)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.14.0->-r requirements.txt (line 2)) (0.9.4)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (4.12.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio==4.14.0->-r requirements.txt (line 2)) (0.29.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.8.0->gradio==4.14.0->-r requirements.txt (line 2)) (2023.6.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.8.0->gradio==4.14.0->-r requirements.txt (line 2)) (11.0.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.13.4)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->-r requirements.txt (line 3)) (12.5.40)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->-r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->-r requirements.txt (line 6)) (4.66.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==4.14.0->-r requirements.txt (line 2)) (4.17.3)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio==4.14.0->-r requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.14.0->-r requirements.txt (line 2)) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.14.0->-r requirements.txt (line 2)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.14.0->-r requirements.txt (line 2)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.14.0->-r requirements.txt (line 2)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.14.0->-r requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio==4.14.0->-r requirements.txt (line 2)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==4.14.0->-r requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio==4.14.0->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio==4.14.0->-r requirements.txt (line 2)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio==4.14.0->-r requirements.txt (line 2)) (2.18.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio==4.14.0->-r requirements.txt (line 2)) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.14.0->-r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.14.0->-r requirements.txt (line 2)) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio==4.14.0->-r requirements.txt (line 2)) (13.7.1)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio==4.14.0->-r requirements.txt (line 2)) (0.14.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio==4.14.0->-r requirements.txt (line 2)) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio==4.14.0->-r requirements.txt (line 2)) (0.27.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx->gradio==4.14.0->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx->gradio==4.14.0->-r requirements.txt (line 2)) (1.0.5)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx->gradio==4.14.0->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx->gradio==4.14.0->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 4)) (1.26.18)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio==4.14.0->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.14.0->-r requirements.txt (line 2)) (23.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.14.0->-r requirements.txt (line 2)) (0.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio==4.14.0->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.14.0->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.14.0->-r requirements.txt (line 2)) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio==4.14.0->-r requirements.txt (line 2)) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/LiheYoung/Depth-Anything\n",
    "#!sudo apt-get update\n",
    "!sudo apt-get install -y libgl1-mesa-glx\n",
    "!pip install opencv-python-headless\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "%cd Depth-Anything\n",
    "!pip install -r requirements.txt\n",
    "# Import from depth_anything package\n",
    "from depth_anything.dpt import DepthAnything\n",
    "from depth_anything.util.transform import Resize, NormalizeImage, PrepareForNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/ImageClassificationAssignment\n"
     ]
    }
   ],
   "source": [
    "model = DepthAnything.from_pretrained(\"LiheYoung/depth_anything_vits14\")\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DepthAnything(\n",
      "  (pretrained): DinoVisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (blocks): ModuleList(\n",
      "      (0-11): 12 x NestedTensorBlock(\n",
      "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): MemEffAttention(\n",
      "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): LayerScale()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "          (drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): LayerScale()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (depth_head): DPTHead(\n",
      "    (projects): ModuleList(\n",
      "      (0): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (3): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (resize_layers): ModuleList(\n",
      "      (0): ConvTranspose2d(48, 48, kernel_size=(4, 4), stride=(4, 4))\n",
      "      (1): ConvTranspose2d(96, 96, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (2): Identity()\n",
      "      (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "    (scratch): Module(\n",
      "      (layer1_rn): Conv2d(48, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer2_rn): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer3_rn): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (layer4_rn): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (refinenet1): FeatureFusionBlock(\n",
      "        (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (refinenet2): FeatureFusionBlock(\n",
      "        (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (refinenet3): FeatureFusionBlock(\n",
      "        (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (refinenet4): FeatureFusionBlock(\n",
      "        (out_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (resConfUnit1): ResidualConvUnit(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (resConfUnit2): ResidualConvUnit(\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (activation): ReLU()\n",
      "          (skip_add): FloatFunctional(\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "        (skip_add): FloatFunctional(\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "      (output_conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (output_conv2): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    Resize(\n",
    "        width=518,\n",
    "        height=518,\n",
    "        resize_target=False,\n",
    "        keep_aspect_ratio=True,\n",
    "        ensure_multiple_of=14,\n",
    "        resize_method='lower_bound',\n",
    "        image_interpolation_method=cv2.INTER_CUBIC,\n",
    "    ),\n",
    "    NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    PrepareForNet(),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('Saved Images/dog.png')\n",
    "image = np.array(image) / 255.0\n",
    "image = transform({'image': image})['image']\n",
    "image = torch.from_numpy(image).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "No operator found for `memory_efficient_attention_forward` with inputs:\n     query       : shape=(1, 2073, 6, 64) (torch.float32)\n     key         : shape=(1, 2073, 6, 64) (torch.float32)\n     value       : shape=(1, 2073, 6, 64) (torch.float32)\n     attn_bias   : <class 'NoneType'>\n     p           : 0.0\n`flshattF@v2.5.6` is not supported because:\n    device=cpu (supported: {'cuda'})\n    dtype=torch.float32 (supported: {torch.float16, torch.bfloat16})\n`cutlassF` is not supported because:\n    device=cpu (supported: {'cuda'})\n`smallkF` is not supported because:\n    max(query.shape[-1] != value.shape[-1]) > 32\n    device=cpu (supported: {'cuda'})\n    unsupported embed per head: 64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Capture the start time\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m depth \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m# Execute the model prediction\u001b[39;00m\n\u001b[1;32m      5\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()    \u001b[38;5;66;03m# Capture the end time\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ImageClassificationAssignment/Depth-Anything/depth_anything/dpt.py:158\u001b[0m, in \u001b[0;36mDPT_DINOv2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    156\u001b[0m     h, w \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m--> 158\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_intermediate_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_class_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     patch_h, patch_w \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m14\u001b[39m, w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m14\u001b[39m\n\u001b[1;32m    162\u001b[0m     depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdepth_head(features, patch_h, patch_w)\n",
      "File \u001b[0;32m~/ImageClassificationAssignment/Depth-Anything/torchhub/facebookresearch_dinov2_main/vision_transformer.py:308\u001b[0m, in \u001b[0;36mDinoVisionTransformer.get_intermediate_layers\u001b[0;34m(self, x, n, reshape, return_class_token, norm)\u001b[0m\n\u001b[1;32m    306\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_intermediate_layers_chunked(x, n)\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_intermediate_layers_not_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    310\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(out) \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/ImageClassificationAssignment/Depth-Anything/torchhub/facebookresearch_dinov2_main/vision_transformer.py:277\u001b[0m, in \u001b[0;36mDinoVisionTransformer._get_intermediate_layers_not_chunked\u001b[0;34m(self, x, n)\u001b[0m\n\u001b[1;32m    275\u001b[0m blocks_to_take \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(total_block_len \u001b[38;5;241m-\u001b[39m n, total_block_len) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m n\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m--> 277\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m blocks_to_take:\n\u001b[1;32m    279\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ImageClassificationAssignment/Depth-Anything/torchhub/facebookresearch_dinov2_main/dinov2/layers/block.py:247\u001b[0m, in \u001b[0;36mNestedTensorBlock.forward\u001b[0;34m(self, x_or_x_list)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_or_x_list):\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x_or_x_list, Tensor):\n\u001b[0;32m--> 247\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_or_x_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x_or_x_list, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m XFORMERS_AVAILABLE, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install xFormers for nested tensors usage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/ImageClassificationAssignment/Depth-Anything/torchhub/facebookresearch_dinov2_main/dinov2/layers/block.py:105\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    103\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(ffn_residual_func(x))  \u001b[38;5;66;03m# FIXME: drop_path2\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[43mattn_residual_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m ffn_residual_func(x)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/ImageClassificationAssignment/Depth-Anything/torchhub/facebookresearch_dinov2_main/dinov2/layers/block.py:84\u001b[0m, in \u001b[0;36mBlock.forward.<locals>.attn_residual_func\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattn_residual_func\u001b[39m(x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/ImageClassificationAssignment/Depth-Anything/torchhub/facebookresearch_dinov2_main/dinov2/layers/attention.py:76\u001b[0m, in \u001b[0;36mMemEffAttention.forward\u001b[0;34m(self, x, attn_bias)\u001b[0m\n\u001b[1;32m     72\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv(x)\u001b[38;5;241m.\u001b[39mreshape(B, N, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, C \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads)\n\u001b[1;32m     74\u001b[0m q, k, v \u001b[38;5;241m=\u001b[39m unbind(qkv, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mmemory_efficient_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape([B, N, C])\n\u001b[1;32m     79\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py:268\u001b[0m, in \u001b[0;36mmemory_efficient_attention\u001b[0;34m(query, key, value, attn_bias, p, scale, op, output_dtype)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmemory_efficient_attention\u001b[39m(\n\u001b[1;32m    157\u001b[0m     query: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    158\u001b[0m     key: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     output_dtype: Optional[torch\u001b[38;5;241m.\u001b[39mdtype] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    166\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implements the memory-efficient attention mechanism following\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    `\"Self-Attention Does Not Need O(n^2) Memory\" <http://arxiv.org/abs/2112.05682>`_.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m    :return: multi-head attention Tensor with shape ``[B, Mq, H, Kv]``\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_memory_efficient_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mInputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py:392\u001b[0m, in \u001b[0;36m_memory_efficient_attention\u001b[0;34m(inp, op)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _memory_efficient_attention_forward(\n\u001b[1;32m    388\u001b[0m         inp, op\u001b[38;5;241m=\u001b[39mop[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    389\u001b[0m     )\n\u001b[1;32m    391\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m inp\u001b[38;5;241m.\u001b[39mnormalize_bmhk()\n\u001b[0;32m--> 392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fMHA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(output_shape)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py:67\u001b[0m, in \u001b[0;36m_fMHA.forward\u001b[0;34m(ctx, op, *args)\u001b[0m\n\u001b[1;32m     64\u001b[0m op_fw \u001b[38;5;241m=\u001b[39m op[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     65\u001b[0m op_bw \u001b[38;5;241m=\u001b[39m op[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m out, op_ctx \u001b[38;5;241m=\u001b[39m \u001b[43m_memory_efficient_attention_forward_requires_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43minp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_fw\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Saving attn_bias is a bit complicated, as the\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# torch part should go in `save_for_backward`\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inp\u001b[38;5;241m.\u001b[39mattn_bias, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/__init__.py:417\u001b[0m, in \u001b[0;36m_memory_efficient_attention_forward_requires_grad\u001b[0;34m(inp, op)\u001b[0m\n\u001b[1;32m    415\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m inp\u001b[38;5;241m.\u001b[39mnormalize_bmhk()\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 417\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_dispatch_fw\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m     _ensure_op_supports_or_raise(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_efficient_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m, op, inp)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py:125\u001b[0m, in \u001b[0;36m_dispatch_fw\u001b[0;34m(inp, needs_gradient)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dispatch_fw\u001b[39m(inp: Inputs, needs_gradient: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Type[AttentionFwOpBase]:\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes the best operator for forward\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    Raises:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m        AttentionOp: The best operator for the configuration\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_priority_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_efficient_attention_forward\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_dispatch_fw_priority_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_gradient\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/dispatch.py:65\u001b[0m, in \u001b[0;36m_run_priority_list\u001b[0;34m(name, priority_list, inp)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m op, not_supported \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(priority_list, not_supported_reasons):\n\u001b[1;32m     64\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m _format_not_supported_reasons(op, not_supported)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: No operator found for `memory_efficient_attention_forward` with inputs:\n     query       : shape=(1, 2073, 6, 64) (torch.float32)\n     key         : shape=(1, 2073, 6, 64) (torch.float32)\n     value       : shape=(1, 2073, 6, 64) (torch.float32)\n     attn_bias   : <class 'NoneType'>\n     p           : 0.0\n`flshattF@v2.5.6` is not supported because:\n    device=cpu (supported: {'cuda'})\n    dtype=torch.float32 (supported: {torch.float16, torch.bfloat16})\n`cutlassF` is not supported because:\n    device=cpu (supported: {'cuda'})\n`smallkF` is not supported because:\n    max(query.shape[-1] != value.shape[-1]) > 32\n    device=cpu (supported: {'cuda'})\n    unsupported embed per head: 64"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()  # Capture the start time\n",
    "depth = model(image)      # Execute the model prediction\n",
    "end_time = time.time()    # Capture the end time\n",
    "\n",
    "print(f\"Execution time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(depth.squeeze().cpu().detach().numpy(), cmap='inferno')\n",
    "plt.colorbar()\n",
    "plt.savefig('Saved Images/depth.png', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(depth)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
