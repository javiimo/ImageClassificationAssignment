{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javiimo/ImageClassificationAssignment/blob/main/Memo_book.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imaplementing MEMO\n",
        "\n",
        "In this Colab we implement MEMO combined with CLIP, to then see if there is any significant performance gain in testing time.\n",
        "The dataset of our choice is cifar100."
      ],
      "metadata": {
        "id": "sFGF5P3XBQy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Import libraries, load CLIP and dataset"
      ],
      "metadata": {
        "id": "777ptigf1KRx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "retRFyx8m3NG",
        "outputId": "50858db1-c6d2-4c08-9b69-25549bb311f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m41.0/54.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.2.0\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-870w9_ye\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-870w9_ye\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.17.1+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369499 sha256=203d431e524a1b4f8982e0155630b7c0be4bc69ea705c543135feb4aa9165d3b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-m2bwcr6w/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n",
            "Successfully installed clip-1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import clip\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # who knows maybe we'll need it later on\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "import torch.amp\n",
        "from torch.cuda.amp import GradScaler\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP model uses quantization to reduce the memory footprint and computational complexity. This is achieved alternating ` dtype = torch.float16` layers with ` dtype = torch.float32` .\n",
        "\n",
        "This, though, makes the backward propagation step more difficult to make and hence, to avoid issues, once the model is initialized every dtype is converted directly to ` dtype = torch.float32`."
      ],
      "metadata": {
        "id": "D5llTyz7-Vad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_model_parameters_to_float32(model):\n",
        "    for param in model.parameters():\n",
        "        param.data = param.data.to(torch.float32)\n",
        "    return model"
      ],
      "metadata": {
        "id": "gl_wMQxknSO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load():\n",
        "\n",
        "    # Load the model\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "    model = convert_model_parameters_to_float32(model).to(device)\n",
        "\n",
        "    # We just need the testing dataset. \"preprocess\" function of clip deals with the transformation of an image into a tensor, so\n",
        "    # we don't have to worry about it applying some transformation to the dataset.\n",
        "\n",
        "    cifar100 = torchvision.datasets.CIFAR100(root= './data', download = True, train = False)\n",
        "\n",
        "    return cifar100, model, device, preprocess\n"
      ],
      "metadata": {
        "id": "41WsyNamm5Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar100, model, device, preprocess = load()\n",
        "\n",
        "#Tokenize the text labels\n",
        "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n",
        "\n",
        "#Encoded text\n",
        "with torch.no_grad():\n",
        "  text_features = model.encode_text(text_inputs)\n",
        "\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n"
      ],
      "metadata": {
        "id": "pYOXLr-ym6xD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2117ab6b-96d1-4182-9092-33bb426429ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:01<00:00, 227MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:03<00:00, 55609382.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Bluid MEMO\n",
        "\n",
        "details of course in the paper: https://arxiv.org/pdf/2110.09506.pdf\n",
        "\n",
        "A first implementation, following what is written in section 3 of the paper , is the following. Let $f_\\theta$ be the model, and $x_0$ the test input:\n",
        "\n",
        "1. Let $x_1, \\ldots x_n$ be the $n$-uple, given by $n$ augmentations of the original sample $x_0$. We shall collect all those elements in a single tensor, which will act as batch for future gradient descent step.\n",
        "\n",
        "2. Compute the probabilities distributions for every element of the batch, say $p_\\theta(\\cdot | x_i)$ .  The model’s average, or $marginal$, output distribution with respect to the augmented points is given by $$ \\bar{p}_\\theta(y | x) = \\frac{1}{n}\\sum_i^n \\bar{p}_\\theta(y | x_i) $$\n",
        "and compute its $marginal$ entropy $H(\\bar{p}_\\theta(\\cdot | x))$.\n",
        "\n",
        "4. Using the entropy as a loss function, perform a single update step (so backprop) to give us the parameters $\\theta'$.\n",
        "\n",
        "5. With those modified parameters we now have the updated $p_{\\theta'}(\\cdot | x_0)$, that we use to predict $y_0 = argmax_y p(y | x_0) $.\n",
        "\n",
        "\n",
        "With this in mind, let's start building what we need, starting with augmentations."
      ],
      "metadata": {
        "id": "2CKKZHst_3HM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# returns a tensor of size (num_augmentations, image size )\n",
        "def augment_image(image, preprocess, num_augmentations = 100):\n",
        "\n",
        "  # apply some random transformations\n",
        "    augmentations = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=30),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "        transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
        "    ])\n",
        "\n",
        "    augmented_images = []\n",
        "\n",
        "    #Add n augmentations to the augmented_images\n",
        "    for _ in range(num_augmentations):\n",
        "        augmented_images.append( preprocess(augmentations(image)).unsqueeze(0).to(device) )\n",
        "\n",
        "    #Save it as a tensor\n",
        "    batch = torch.vstack(augmented_images)\n",
        "\n",
        "    return batch"
      ],
      "metadata": {
        "id": "T-55fC3tnBmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The entropy of a random variable $X$ with discrete distribution $p$ is defined as:\n",
        "$$H(X) = \\sum_i^n p_i \\log (p_i) \\quad \\text{where } p_i = \\mathbb{P}(X = x_i) \\quad \\text{and we set } 0 \\log(0) := 0$$\n",
        "To compute it, we need to be careful to underflow/overflows that may occour while computing it.\n",
        "\n",
        "To this extent, we perform some mathematical operations exploiting logarithms properties to make computations more stable."
      ],
      "metadata": {
        "id": "13Abi9mXB_eO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let $k$ be the number of classes. Let $n$ be the number of samples.\n",
        "Consider $$X_{i = 1, \\ldots n,j = 1, \\ldots k}$$\n",
        "the matrix of logits, where $x_{ij}$ is the $j-th$ logit of the $i-th$ sample, and $p_{ij}$ be the probability output given by the softmax.\n",
        "\n",
        "Then the log of the marginal probability of belonging to class\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\log ( \\bar{p}_j )\n",
        "&= \\log \\biggl( \\frac{1}{n}\\sum_i^n p_{ij} \\biggr) \\\\\n",
        "&= \\log \\biggl( \\frac{1}{n}\\sum_i^n p_{ij} \\biggr) \\\\\n",
        "&= \\log \\biggl( \\frac{1}{n}\\sum_i^n \\frac{e^{x_{ij}}}{ \\sum_h e^{x_{ih}}} \\biggr) \\\\\n",
        "&= \\log \\biggl( \\sum_i^n e^{z_{ij}} \\biggr) - \\log(n) \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "where $z_{ij} = x_{ij} - \\log \\sum_h^k e^{x_{ik}}$.\n",
        "\n",
        "Computing $z_{ij}$ first and then using the upper formula to get $\\log{\\bar{p}_j}$ is more stable, as $\\log$ oprations are done over a sum of elements rather than just a single one."
      ],
      "metadata": {
        "id": "65e1hYGoGDmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def marginal_entropy(logits):\n",
        "\n",
        "    z = logits - logits.logsumexp(dim = -1, keepdim=True)                            # compute z_ij\n",
        "    marginal_logp = z.logsumexp(dim=0) - np.log(z.shape[0])   # compute marginal log probabilities\n",
        "\n",
        "    min_real = torch.finfo(marginal_logp.dtype).min          # for numerical stability, The smallest representable number given the dtype of logits.\n",
        "    avg_logits = torch.clamp(marginal_logp, min = min_real)  # put a threshold to avoid underflow\n",
        "\n",
        "    return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)"
      ],
      "metadata": {
        "id": "OAxcmG-cCCEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CLIP's output of a given image is the embedding on a $512$-dimensional space.\n",
        "CLIP's output of a given text is the embedding on the same $512$-dimensional space.\n",
        "\n",
        "From there it is possible to get the logits of the probabilities distributions by computing the cosine similarities of the embedding of an image with the embeddings of all the labels.\n",
        "\n",
        "We have all the ingredients now to perform the gradient descent step:\n"
      ],
      "metadata": {
        "id": "_PLxVkaID8ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backprop_sweep(model, batch, text_features, optimizer):\n",
        "\n",
        "    model.train(True)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward pass\n",
        "    image_features = model.encode_image(batch) # image features contains the embeddings of all the elements of the class.\n",
        "    image_features = image_features / image_features.norm(dim = -1, keepdim = True)\n",
        "    logits = 100.0 * image_features @ text_features.T # get logits\n",
        "\n",
        "    # compute loss\n",
        "    loss = marginal_entropy(logits)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "GRrWYkuFG4EB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's build a function that now predicts the output of the original sample on the updated model:"
      ],
      "metadata": {
        "id": "6gUQctF6I2_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, image, text_features, label):\n",
        "\n",
        "    model.eval()\n",
        "    image_prep = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        image_features = model.encode_image(image_prep) # image features contains the embeddings of all the elements of the class.\n",
        "        image_features = image_features / image_features.norm(dim = -1, keepdim = True)\n",
        "        outputs = 100.0 * image_features @ text_features.T\n",
        "\n",
        "        _, predicted = outputs.max(1)\n",
        "        confidence = nn.functional.softmax(outputs,dim=1).squeeze()[predicted].item()\n",
        "\n",
        "    correctness = 1 if predicted.item() == label else 0\n",
        "\n",
        "    return correctness, confidence"
      ],
      "metadata": {
        "id": "KgFwJ-frJCEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally it is possible to write MEMO function:"
      ],
      "metadata": {
        "id": "DV_wMnxFOTjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MEMO_test(model, preprocess, text_features, image, class_id, device, num_augmentations = 50, lr = 1e-5):\n",
        "\n",
        "  batch = augment_image(image, preprocess, num_augmentations = num_augmentations)\n",
        "  model_copy = copy.deepcopy(model).to(device)\n",
        "\n",
        "  optimizer = optim.SGD(model_copy.parameters(), lr = lr)\n",
        "\n",
        "  backprop_sweep(model_copy, batch, text_features, optimizer)\n",
        "\n",
        "  return predict(model_copy, image, text_features, class_id)"
      ],
      "metadata": {
        "id": "aDQEVxe5OdJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Sanity check\n",
        "\n",
        "Let's see if it everything works."
      ],
      "metadata": {
        "id": "v8nW0U8JIBpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, class_id = cifar100[0]\n",
        "\n",
        "print('(correctness, confidence)')\n",
        "print(f'MEMO: {MEMO_test(model, preprocess, text_features, image, class_id, device, lr = 1e-5)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6p9mMvDjIDkb",
        "outputId": "eee06977-fdb0-414b-93b6-1b8aa63389dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(correctness, confidence)\n",
            "MEMO: (1, 0.19025388360023499)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Full test"
      ],
      "metadata": {
        "id": "6wpXeUcnnH88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "print('Running...')\n",
        "\n",
        "correct_memo = []\n",
        "correct_clip = []\n",
        "\n",
        "confidence_memo = []\n",
        "confidence_clip = []\n",
        "\n",
        "for i in tqdm(range(5000)):\n",
        "\n",
        "\n",
        "    image, class_id = cifar100[i]\n",
        "    memo_eval, memo_belief = MEMO_test(model, preprocess, text_features, image, class_id, device)\n",
        "\n",
        "    correct_memo.append(memo_eval)\n",
        "    confidence_memo.append(memo_belief)\n",
        "\n",
        "    clip_eval, clip_belief = predict(model, image, text_features, class_id)\n",
        "\n",
        "    correct_clip.append(clip_eval)\n",
        "    confidence_clip.append(clip_belief)\n",
        "\n",
        "print('')\n",
        "print(f'MEMO adapt test accuracy {(np.mean(correct_memo))*100:.2f}. Average confidence: {(np.mean(confidence_memo))*100:.2f}')\n",
        "print(f'CLIP adapt test accuracy {(np.mean(correct_clip))*100:.2f}. Average confidence: {(np.mean(confidence_clip))*100:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OUbaL1rI4gp",
        "outputId": "a6d6f61f-113d-405f-a418-165699a21e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [04:46<00:00,  1.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MEMO adapt test accuracy 64.60. Average confidence: 51.37\n",
            "CLIP adapt test accuracy 64.40. Average confidence: 50.60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. A few remarks\n",
        "\n",
        "*   It's a non trivial task to set the learning rate of the optimizer properly. In fact since loss function, *the entropy*, is minimized for high confidence predictions regardless of the correctness of the prediction itself: setting it too high will destroy model's parameters and therefore giving lots of erroneus predictions. On the other hand though if the learning rate is way too little then the backpropagation step won't be effectful enough. Setting ```lr = 1e-5``` seems an ok in-between.\n",
        "\n",
        "\n",
        "*   In information theory literature to compute the entropy of a random variable is it usually employed the base 2 logarithm, $\\log_2$. In this implementation we use the classical base $e$ log but this is not only formally correct but also does not influence the backpropagation step as it just differs from $log_2$ by a multiplicative constant ($\\frac{1}{\\log_e(2)}$).\n",
        "\n",
        "* The number of augmentations does not have a huge impact on the outcome. However, the more augmentations the more accuracy at the price of a reduced confidence - averaging to a higher number of probability distributions will land a less peaked distribution, hence more entropy, but gains more robustness to domain shifts.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jsi1KKGvrhzD"
      }
    }
  ]
}