{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javiimo/ImageClassificationAssignment/blob/main/CLIPClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIg1G6AD6fyQ",
        "outputId": "f38a8e2a-d859-4d82-f385-ba6f22ba4f05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.2.0\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-s8sxls1i\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-s8sxls1i\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.17.1+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369499 sha256=4f290ba433f1f1963003612b5c77f3a3ceedb8c9de447246a783e3f10e1d0cf0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-v320aqou/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n",
            "Successfully installed clip-1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import clip\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import copy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To be able to access the folder of datasets (the path will be different for each account I think)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xH-bJXD2jCNk",
        "outputId": "aa33d8c5-a395-419d-c67c-c541d2c5b29e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our data sets\n"
      ],
      "metadata": {
        "id": "IgdgKOGfj-S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CLIPModel:\n",
        "    def __init__(self, model_name='ViT-B/32', device=None):\n",
        "        self.device = device if device else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model, self.preprocess = clip.load(model_name, self.device)\n",
        "        #self.model = self.convert_model_parameters_to_float32(self.model)\n",
        "        #self.optimizer = optim.Adam(self.model.parameters(), lr=0.000000001, weight_decay=2e-4) #Numerical inestability\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr=1000, momentum=0.9)\n",
        "        self.text_features = None\n",
        "\n",
        "    def require_CLIP_gradients(self, state = True):\n",
        "        if state != self.requiring_grads #don't change if the state is already OK\n",
        "            for param in self.model.parameters():\n",
        "                param.requires_grad = state\n",
        "            self.requiring_grads = state\n",
        "\n",
        "    def convert_model_parameters_to_float32(self, model):\n",
        "        for param in model.parameters():\n",
        "            param.data = param.data.to(torch.float32)\n",
        "        return model\n",
        "\n",
        "    def load_data(self):\n",
        "        cifar100 = torchvision.datasets.CIFAR100(root='./data', download=True, train=False)\n",
        "        return cifar100\n",
        "\n",
        "    #This are heuristic labels\n",
        "    def tokenize_labels(self, classes):\n",
        "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classes]).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            self.text_features = self.model.encode_text(text_inputs)\n",
        "            self.text_features /= self.text_features.norm(dim=-1, keepdim=True)\n",
        "        return self.text_features\n",
        "\n",
        "    def augment_image(self, image, num_augmentations=100):\n",
        "        augmentations = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomVerticalFlip(p=0.5),\n",
        "            transforms.RandomRotation(degrees=30),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "            transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
        "        ])\n",
        "        augmented_images = [self.preprocess(image).unsqueeze(0).to(self.device)]\n",
        "        for _ in range(num_augmentations):\n",
        "            augmented_images.append(self.preprocess(augmentations(image)).unsqueeze(0).to(self.device))\n",
        "        batch = torch.vstack(augmented_images)\n",
        "        return batch\n",
        "\n",
        "    def compute_entropy(self, x):\n",
        "        log_x = torch.log2(x.clamp_min(1e-20))\n",
        "        entropy = -torch.sum(x * log_x)\n",
        "        return entropy\n",
        "\n",
        "    def class_probabilities(self, text_features, image_features):\n",
        "        #Compute cosine similarities\n",
        "        return (image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "    def confidence_selection(self, similarities, percentile = 0.8):\n",
        "        entropies = torch.tensor([self.compute_entropy(row) for row in similarities])\n",
        "        sorted_entropies, _ = torch.sort(entropies, descending=True)\n",
        "        threshold = sorted_entropies[int(len(sorted_entropies) * percentile)]\n",
        "        boolean_mask = entropies < threshold\n",
        "        return similarities[boolean_mask]\n",
        "\n",
        "    def entropy_loss_MEMO(self, image_features, text_features = None, conf_sel=False):\n",
        "        if text_features is None:\n",
        "            text_features = self.text_features\n",
        "        similarities = self.class_probabilities(text_features, image_features)\n",
        "        # Apply confidence selection to rule out high entropy augmentations\n",
        "        if conf_sel:\n",
        "            similarities = self.confidence_selection(similarities)\n",
        "        # Compute the entropy of every text caption accross all augmentations\n",
        "        entropies = [self.compute_entropy(row) for row in similarities]\n",
        "        return torch.stack(entropies).mean()\n",
        "\n",
        "    def entropy_loss_TPT(self, image_features, text_features = None):\n",
        "        if text_features is None:\n",
        "            text_features = self.text_features\n",
        "        similarities = self.class_probabilities(text_features, image_features)\n",
        "        # Confidence selection for the augmented views:\n",
        "        similarities = self.confidence_selection(similarities)\n",
        "        # Average the caption probabilities across all augmentations\n",
        "        avg_probs = torch.tensor([row.mean() for row in similarities.T])\n",
        "        # Compute the entropy of the averaged probability distribution\n",
        "        return self.compute_entropy(avg_probs), avg_probs\n",
        "\n",
        "    def grad_descent_step(self, loss):\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def predict(self, image):\n",
        "        image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            image_features = self.model.encode_image(image)\n",
        "            norms = image_features.norm(dim=-1, keepdim=True)\n",
        "            if (norms == 0).any():\n",
        "                print(\"Zero norm found in image features\")\n",
        "            image_features = image_features / norms.clamp_min(1e-10)\n",
        "\n",
        "        similarity = self.class_probabilities(self.text_features, image_features)\n",
        "        prediction = torch.argmax(similarity).item()\n",
        "        entropy = float(self.compute_entropy(similarity))\n",
        "        return prediction, similarity, entropy\n",
        "\n",
        "    def MEMO(self, image, num_augmentations=100, conf_sel = False):\n",
        "        # Save original parameters\n",
        "        original_params = {name: param.clone() for name, param in self.model.named_parameters()}\n",
        "\n",
        "        # Require gradients to update the CLIP parameters\n",
        "        require_CLIP_gradients(state = True)\n",
        "        try:\n",
        "            batch = self.augment_image(image, num_augmentations)\n",
        "            image_features = self.model.encode_image(batch)\n",
        "            norms = image_features.norm(dim=-1, keepdim=True)\n",
        "            if (norms == 0).any():\n",
        "                print(\"Zero norm found in image features\")\n",
        "            image_features = image_features / norms.clamp_min(1e-10)\n",
        "\n",
        "            loss = self.entropy_loss_MEMO(image_features, conf_sel= conf_sel)\n",
        "            print(loss)\n",
        "            self.grad_descent_step(loss)\n",
        "\n",
        "            if any(torch.isnan(param).any() for param in self.model.parameters()):\n",
        "                print(\"nan values detected in model parameters after updating\")\n",
        "            # Predict using the updated model\n",
        "            prediction, similarity, entropy = self.predict(image)\n",
        "        finally:\n",
        "            # Restore original parameters\n",
        "            with torch.no_grad():\n",
        "                for name, param in self.model.named_parameters():\n",
        "                    param.copy_(original_params[name])\n",
        "        return prediction, similarity, entropy\n",
        "\n",
        "    def TPT(self, image, num_augmentations=100):\n",
        "        batch = self.augment_image(image, num_augmentations)\n",
        "        image_features = self.model.encode_image(batch)\n",
        "        norms = image_features.norm(dim=-1, keepdim=True)\n",
        "        if (norms == 0).any():\n",
        "            print(\"Zero norm found in image features\")\n",
        "        image_features = image_features / norms.clamp_min(1e-10)\n",
        "\n",
        "        entropy, avg_probs = self.entropy_loss_TPT(image_features)\n",
        "        prediction = torch.argmax(avg_probs).item()\n",
        "        return prediction, float(entropy)\n",
        "\n",
        "    def train_CoOp(self):\n",
        "        #Prevent CLIP parameters from changing\n",
        "        self.require_CLIP_gradients(state=False)\n",
        "\n",
        "\n",
        "# Preparing the class for usage\n",
        "clip_model = CLIPModel()\n",
        "cifar100 = clip_model.load_data()\n",
        "clip_model.tokenize_labels(cifar100.classes)\n",
        "image, class_id = cifar100[3637]"
      ],
      "metadata": {
        "id": "6XGJSO0FnKTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ceb7371-1d74-491b-8cee-c8a1a20464cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction using CLIP out of the box\n",
        "prediction1, similarity1, entropy1 = clip_model.predict(image)\n",
        "print(similarity1, prediction1, entropy1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "087FNIiqpDUb",
        "outputId": "fb60af5c-3c4a-4508-a6d2-babb3af0b230"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0101, 0.0102, 0.0102, 0.0099, 0.0099, 0.0100, 0.0100, 0.0103, 0.0098,\n",
            "         0.0100, 0.0101, 0.0100, 0.0097, 0.0098, 0.0101, 0.0099, 0.0101, 0.0098,\n",
            "         0.0102, 0.0101, 0.0099, 0.0101, 0.0099, 0.0098, 0.0100, 0.0099, 0.0103,\n",
            "         0.0104, 0.0100, 0.0102, 0.0096, 0.0100, 0.0103, 0.0098, 0.0096, 0.0100,\n",
            "         0.0101, 0.0098, 0.0098, 0.0099, 0.0101, 0.0103, 0.0104, 0.0100, 0.0104,\n",
            "         0.0099, 0.0100, 0.0098, 0.0098, 0.0100, 0.0103, 0.0102, 0.0100, 0.0100,\n",
            "         0.0098, 0.0099, 0.0100, 0.0099, 0.0099, 0.0101, 0.0100, 0.0101, 0.0100,\n",
            "         0.0099, 0.0100, 0.0100, 0.0100, 0.0100, 0.0096, 0.0096, 0.0099, 0.0098,\n",
            "         0.0101, 0.0098, 0.0101, 0.0099, 0.0097, 0.0103, 0.0107, 0.0102, 0.0098,\n",
            "         0.0098, 0.0100, 0.0104, 0.0099, 0.0102, 0.0101, 0.0099, 0.0101, 0.0098,\n",
            "         0.0099, 0.0100, 0.0099, 0.0106, 0.0098, 0.0097, 0.0098, 0.0098, 0.0100,\n",
            "         0.0103]], device='cuda:0', dtype=torch.float16) 78 6.64453125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction using MEMO at test time\n",
        "prediction2, similarity2, entropy2 = clip_model.MEMO(image, num_augmentations=100)\n",
        "print(similarity2, prediction2, entropy2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RheDQxnoeB7",
        "outputId": "b3916db1-297d-4bf9-d7e4-fc1da55054c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.6445, device='cuda:0', dtype=torch.float16, grad_fn=<MeanBackward0>)\n",
            "tensor([[0.0108, 0.0098, 0.0103, 0.0098, 0.0090, 0.0096, 0.0095, 0.0099, 0.0098,\n",
            "         0.0100, 0.0102, 0.0103, 0.0091, 0.0095, 0.0101, 0.0094, 0.0104, 0.0096,\n",
            "         0.0103, 0.0102, 0.0094, 0.0100, 0.0099, 0.0097, 0.0093, 0.0097, 0.0094,\n",
            "         0.0107, 0.0102, 0.0109, 0.0091, 0.0098, 0.0094, 0.0107, 0.0087, 0.0101,\n",
            "         0.0094, 0.0097, 0.0093, 0.0101, 0.0101, 0.0109, 0.0100, 0.0101, 0.0105,\n",
            "         0.0095, 0.0103, 0.0120, 0.0097, 0.0102, 0.0095, 0.0103, 0.0120, 0.0102,\n",
            "         0.0097, 0.0087, 0.0122, 0.0102, 0.0104, 0.0127, 0.0107, 0.0105, 0.0105,\n",
            "         0.0098, 0.0096, 0.0094, 0.0091, 0.0093, 0.0090, 0.0097, 0.0101, 0.0093,\n",
            "         0.0092, 0.0094, 0.0089, 0.0097, 0.0092, 0.0101, 0.0108, 0.0098, 0.0094,\n",
            "         0.0092, 0.0107, 0.0111, 0.0099, 0.0106, 0.0104, 0.0103, 0.0101, 0.0105,\n",
            "         0.0098, 0.0098, 0.0102, 0.0104, 0.0094, 0.0094, 0.0116, 0.0098, 0.0101,\n",
            "         0.0105]], device='cuda:0', dtype=torch.float16) 59 6.640625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction using TPT\n",
        "prediction3, entropy3 = clip_model.TPT(image)\n",
        "print(entropy3)\n",
        "print(prediction3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrxX4QrwsCbv",
        "outputId": "5597a9d2-cf69-424f-8b34-0675fe279342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nan\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction using MEMO with confidence selection at test time\n",
        "prediction4, similarity4, entropy4 = clip_model.MEMO(image, num_augmentations=100, conf_sel=True)\n",
        "print(similarity4, prediction4, entropy4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "O6EOTB7t1kZG",
        "outputId": "61db78ec-9867-4cd0-e9db-8c55f7a98db8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "stack expects a non-empty TensorList",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-43ecd46a4f98>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Prediction using MEMO with confidence selection at test time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprediction4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarity4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMEMO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_augmentations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_sel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilarity4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-4f7d2d663bcc>\u001b[0m in \u001b[0;36mMEMO\u001b[0;34m(self, image, num_augmentations, conf_sel)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mimage_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_features\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_min\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentropy_loss_MEMO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_sel\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mconf_sel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_descent_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-4f7d2d663bcc>\u001b[0m in \u001b[0;36mentropy_loss_MEMO\u001b[0;34m(self, image_features, text_features, conf_sel)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Compute the entropy of every text caption accross all augmentations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mentropies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentropies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mentropy_loss_TPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the similarities after the two MEMO implementations (with confidence selection and without it) are the same or not\n",
        "s1 = similarity1.to(torch.float16)\n",
        "s2 = similarity2.to(torch.float16)\n",
        "s4 = similarity4.to(torch.float16)\n",
        "print('Clip vs Memo without conf sel')\n",
        "print(s1==s2)\n",
        "print('Clip vs Memo with conf sel')\n",
        "print(s1==s4)\n",
        "print('Memo with conf sel vs Memo without conf sel')\n",
        "print(s2==s4)"
      ],
      "metadata": {
        "id": "WF8GfLAGiUr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w0agt5OPS95-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}