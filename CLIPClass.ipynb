{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIg1G6AD6fyQ",
    "outputId": "ec7ad4fe-ae62-4ba5-fac5-d737547a332b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy\n",
      "  Using cached ftfy-6.2.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.2)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy) (0.2.13)\n",
      "Using cached ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
      "Installing collected packages: ftfy\n",
      "Successfully installed ftfy-6.2.0\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-rdfrhfkw\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-rdfrhfkw\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (6.2.0)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.2)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.0.0.post101)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.15.2a0+072ec57)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369499 sha256=2a6832359266fd342bf7d6cd06bb51eda8504d0ad1c51cd97d48e743f326860d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-l67u9fky/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git\n",
    "! pip install datasets transformers\n",
    "\n",
    "# Used for CLIP:\n",
    "import clip\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "#Used for testing:\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIPModel Class Method Explanations\n",
    "\n",
    "## Constructor\n",
    "- **`__init__(self, model_name='ViT-B/32', device=None)`**\n",
    "  - Initializes the model with a specified model name and device. Defaults to using CUDA if available. It sets up the optimizer and default parameters for the model. And casts it to float 32 to use ADAM. Also sets a `changeseed` to be able to have different random generations in a loop for Entropy Boosting.\n",
    "\n",
    "## Optimizer Methods\n",
    "- **`use_ADAM(self)`**\n",
    "  - Changes the optimizer of the model to ADAM.\n",
    "- **`use_SGD(self)`**\n",
    "  - Switches the optimizer to SGD.\n",
    "- **`convert_model_parameters_to_float32(self, model)`**\n",
    "    - This is for ADAM to work. Converts all model parameters to float32 data type.\n",
    "- **`grad_descent_step(self, loss)`**\n",
    "    - Performs a single gradient descent step using the computed loss, updating the model parameters accordingly. This is what modifies the clip model stored in the object.\n",
    "\n",
    "## Gradient Handling\n",
    "- **`require_CLIP_gradients(self, state=True)`**\n",
    "  - Enables or disables the calculation of gradients for model parameters based on the specified state.\n",
    "\n",
    "## Data Handling\n",
    "- **`load_data(self)`**\n",
    "  - This is just to have some data easy to access to test the methods on some image. Loads the CIFAR100 dataset which is useful for testing or validating the model performance on image data.\n",
    " \n",
    "## Tokenization of labels\n",
    "- **`tokenize_labels(self, classes)`**\n",
    "    - Converts a list of class labels into text tokens using CLIP's tokenizer and encodes them into text features. This is used for matching text descriptions with images. For now this is just used with a photo of a {class} and then stored as an attribute so that we can reuse it for every method.\n",
    " \n",
    "## Transformations\n",
    "- **`augment_image(self, image, num_augmentations=100, transformations=None)`**\n",
    "  - Applies specified transformations to an image (or a random transformation if none given) to generate multiple augmented versions.\n",
    "\n",
    "## Feature and Probability Computations\n",
    "- **`cos_sim(self, image_features, text_features)`**\n",
    "  - Calculates the cosine similarity between image and text features.\n",
    "- **`logits(self, image_features, text_features)`**\n",
    "  - Computes the logits by applying a scale factor (temperature already learned by CLIP) to the cosine similarities.\n",
    "- **`class_probabilities(self, image_features, text_features)`**\n",
    "  - Converts logits to class probabilities using the softmax function.\n",
    "\n",
    "## Entropy and Loss Calculations\n",
    "- **`compute_entropy(self, x)`**\n",
    "  - Computes the Shannon entropy of a probability distribution, measuring uncertainty.\n",
    "- **`marginal_entropy(self, logits)`**\n",
    "    - Calculates the marginal entropy of the logits, which quantifies the uncertainty or spread of the predicted probabilities across different classes.\n",
    "- **`entropy_loss_MEMO(self, batch_features, text_features=None)`**\n",
    "  - Calculates the entropy loss for the MEMO strategy using marginal entropy.\n",
    "- **`entropy_loss_TPT(self, batch_features, text_features=None)`**\n",
    "    - Calculates and returns the entropy loss for TPT.\n",
    "\n",
    "## Predictions with the CLIP model out of the box\n",
    "- **`forward(self, image)`**\n",
    "    - Processes an image through the model, encoding it into image features, normalizing these features, and then computing the class probabilities.\n",
    "- **`predict(self, image)`**\n",
    "    - Completes the prediction process including preprocessing the image, computing probabilities, and determining the most probable class along with its entropy. This is what you can use to compute CLIP out of the box predictions.\n",
    "\n",
    "## Methods used to get predictions\n",
    "- **`MEMO(self, image, num_augmentations=100)`**\n",
    "  - Implements MEMO strategy by optimizing model parameters to minimize entropy across predictions.\n",
    "- **`TPT(self, image, num_augmentations=100)`**\n",
    "    - Applies the TPT strategy, focusing on refining the model's ability to predict with high certainty by averaging probabilities across \"good\" augmentations.\n",
    "- **`entropyboosting(self, image, num_augmentations=100, num_candidates=10, top_aug_num=5)`**\n",
    "  - Uses \"Entropy Boosting\" to enhance model confidence by iterating over augmentation cycles to stabilize and improve prediction certainty.\n",
    "\n",
    "## Extra for TPT\n",
    "- **`confidence_selection(self, probs_matrix, percentile=0.8)`**\n",
    "    - Selects predictions with entropy below a specified percentile, filtering out less confident (high entropy) predictions to focus on more certain outcomes.\n",
    "\n",
    "# Extra for Entropy Boosting\n",
    "- **`pick_candidates(self, tensor, classifier, top_num)`**\n",
    "    - Selects top candidates based on a classifier score.\n",
    "- **`expand_tensor(self, tensor, top_indices, n)`**\n",
    "    - Expands a smaller tensor into a larger one based on specified indices.\n",
    "- **`generate_augmentations_similarities(self, image, image_features, txt_candidates, num_augmentations, top_aug_num)`**\n",
    "    - Generates and evaluates the similarity of augmented images to text candidates, aiding in the identification of most relevant image features.\n",
    "- **`boost_augmentations(self, image, image_features, prob, num_augmentations, num_candidates, top_aug_num)`**\n",
    "    - Boosts the confidence in model predictions by blending probabilities from original and augmented images, focusing on candidates with high initial probabilities.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to split the CLass:\n",
    "We have a parent class CLIP and 3 children (for now) which are MEMO, TPT and EB (Entropy Boost). Somehow, those children should be able to interact and use the same CLIP object )(access its attributes, methods as if they where the same object) without creating a copy of it. Not sure how to do it, but ideally it would work like one single class but splitting the code accross multiple classes so that we can structure the code better.\n",
    "## Attributes:\n",
    "- device (CLIP)\n",
    "- model (CLIP)\n",
    "- preprocess (CLIP)\n",
    "- optimizer (CLIP)\n",
    "- text_features (CLIP)\n",
    "- requiring_grads (CLIP)\n",
    "- logit_scale (CLIP)\n",
    "- changeseed \n",
    "\n",
    "## Methods:\n",
    "- __init__ (CLIP has exactly the one written)\n",
    "- use_ADAM (CLIP)\n",
    "- use_SGD (CLIP)\n",
    "- require_CLIP_gradients (CLIP)\n",
    "- convert_model_parameters_to_float32 (CLIP)\n",
    "- load_data (this one does not matter, won't be in the final version)\n",
    "- tokenize_labels (CLIP)\n",
    "- augment_image (CLIP)\n",
    "- cos_sim (CLIP)\n",
    "- logits (CLIP)\n",
    "- class_probabilities (CLIP)\n",
    "- marginal_entropy (CLIP)\n",
    "- compute_entropy (CLIP)\n",
    "- confidence_selection (TPT)\n",
    "- entropy_loss_MEMO (MEMO)\n",
    "- entropy_loss_TPT (TPT)\n",
    "- forward (CLIP)\n",
    "- predict (CLIP)\n",
    "- grad_descent_step (CLIP)\n",
    "- MEMO (MEMO)\n",
    "- TPT (TPT)\n",
    "- pick_candidates (EB)\n",
    "- expand_tensor (EB)\n",
    "- generate_augmentations_similarities (EB)\n",
    "- boost_augmentations (EB)\n",
    "- entropyboosting (EB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6XGJSO0FnKTp",
    "outputId": "5f97a486-48f4-48ca-e713-83c576ebcd03"
   },
   "outputs": [],
   "source": [
    "class CLIPModel:\n",
    "\n",
    "    def __init__(self, model_name='ViT-B/32', device=None):\n",
    "        self.device = device if device else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model, self.preprocess = clip.load(model_name, self.device)\n",
    "        self.model = self.convert_model_parameters_to_float32(self.model)\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
    "        self.text_features = None\n",
    "        self.requiring_grads = None\n",
    "        self.logit_scale = self.model.logit_scale #temperature parameter learned by CLIP\n",
    "        self.changeseed = 0 #This is to be able to do diverse random transforms but replicable\n",
    "    \n",
    "    def use_ADAM(self):\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-5)\n",
    "\n",
    "    def use_SGD(self):\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-5, momentum=0.9)\n",
    "\n",
    "    def require_CLIP_gradients(self, state = True):\n",
    "        if self.requiring_grads is None or state != self.requiring_grads: #don't change if the state is already OK\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = state\n",
    "            self.requiring_grads = state\n",
    "\n",
    "    def convert_model_parameters_to_float32(self, model):\n",
    "        for param in model.parameters():\n",
    "            param.data = param.data.to(torch.float32)\n",
    "        return model\n",
    "\n",
    "    def load_data(self):\n",
    "        cifar100 = torchvision.datasets.CIFAR100(root='./data', download=True, train=False)\n",
    "        return cifar100\n",
    "\n",
    "    #This are heuristic labels\n",
    "    def tokenize_labels(self, classes):\n",
    "        text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in classes]).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            self.text_features = self.model.encode_text(text_inputs)\n",
    "            self.text_features /= self.text_features.norm(dim=-1, p=2, keepdim=True)\n",
    "\n",
    "    def augment_image(self, image, num_augmentations=100, transformations=None):\n",
    "        if transformations==None:\n",
    "            torch.manual_seed(33)#Set a seed for reproducibility of the random augmentations\n",
    "            augmentations = transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=30),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "                transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
    "            ])\n",
    "        augmented_images = [self.preprocess(image).unsqueeze(0).to(self.device)] #Add the original image to the batch of augmentations\n",
    "        for _ in range(num_augmentations):\n",
    "            augmented_images.append(self.preprocess(augmentations(image)).unsqueeze(0).to(self.device))\n",
    "        batch = torch.vstack(augmented_images)\n",
    "        return batch #(num_augumentations + 1, 3, 224, 224)\n",
    "\n",
    "    def cos_sim(self, image_features, text_features):\n",
    "        return  image_features @ text_features.T\n",
    "\n",
    "    def logits(self, image_features, text_features):\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        return logit_scale * self.cos_sim(image_features, text_features)\n",
    "\n",
    "    def class_probabilities(self, image_features, text_features):\n",
    "        #Compute cosine similarities\n",
    "        return  self.logits(image_features, text_features).softmax(dim=-1)\n",
    "\n",
    "    def marginal_entropy(self, logits):\n",
    "        z = logits - logits.logsumexp(dim = -1, keepdim=True) # compute z_ij\n",
    "        marginal_logp = z.logsumexp(dim=0) - np.log(z.shape[0])   # compute marginal log probabilities\n",
    "\n",
    "        min_real = torch.finfo(marginal_logp.dtype).min # for numerical stability,\n",
    "        # the smallest representable number given the dtype of logits.\n",
    "        avg_logits = torch.clamp(marginal_logp, min = min_real)  # put a threshold to avoid underflow\n",
    "\n",
    "        return -(avg_logits * torch.exp(avg_logits)).sum(dim=-1)\n",
    "\n",
    "    def compute_entropy(self, x): #Shanon entropy in bits\n",
    "        #This computes the Shanon entropy\n",
    "        log_x = torch.log2(x.clamp_min(1e-20))\n",
    "        entropy = -torch.sum(x * log_x)\n",
    "        return entropy\n",
    "\n",
    "    def confidence_selection(self, probs_matrix, percentile=0.8):\n",
    "        # Compute entropies for each row in the probability matrix\n",
    "        entropies = torch.tensor([self.compute_entropy(row) for row in probs_matrix])\n",
    "\n",
    "        # Find the threshold for the desired percentile\n",
    "        threshold = torch.quantile(entropies, percentile, interpolation = 'linear')\n",
    "\n",
    "        # Create a boolean mask where entropies below the threshold are selected\n",
    "        boolean_mask = entropies < threshold\n",
    "\n",
    "        # Assuming similarities is intended to be probs_matrix, return filtered matrix\n",
    "        return probs_matrix[boolean_mask]\n",
    "\n",
    "    def entropy_loss_MEMO(self, batch_features, text_features = None):\n",
    "        if text_features is None:\n",
    "            text_features = self.text_features\n",
    "        #Logits (unnormalized probabilities)\n",
    "        logits = self.logits(batch_features, text_features)\n",
    "        # Compute the entropy of every text caption accross all augmentations\n",
    "        marginal_entropy = self.marginal_entropy(logits)\n",
    "        return marginal_entropy\n",
    "\n",
    "    def entropy_loss_TPT(self, batch_features, text_features = None):\n",
    "        if text_features is None:\n",
    "            text_features = self.text_features\n",
    "        probs_matrix = self.class_probabilities(batch_features, text_features)\n",
    "        # Confidence selection for the augmented views:\n",
    "        probs_matrix = self.confidence_selection(probs_matrix)\n",
    "        # Average the caption probabilities across all augmentations\n",
    "        avg_probs = torch.tensor([row.mean() for row in probs_matrix.T])\n",
    "        # Compute the entropy of the averaged probability distribution\n",
    "        return self.compute_entropy(avg_probs), avg_probs\n",
    "\n",
    "    def forward(self, image):\n",
    "        image_features = self.model.encode_image(image)\n",
    "        norms = image_features.norm(dim=-1, p=2,  keepdim=True)\n",
    "        if (norms == 0).any():\n",
    "            print(\"Zero norm found in image features\")\n",
    "        image_features = image_features / norms.clamp_min(1e-10)\n",
    "        return self.class_probabilities(image_features, self.text_features)\n",
    "\n",
    "    def predict(self, image):\n",
    "        self.model.eval()\n",
    "        image = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            probs = self.forward(image)\n",
    "\n",
    "        prediction = torch.argmax(probs).item()\n",
    "        entropy = float(self.compute_entropy(probs))\n",
    "        return prediction, probs.squeeze(), entropy\n",
    "\n",
    "    def grad_descent_step(self, loss):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def MEMO(self, image, num_augmentations=100):\n",
    "        # Save original parameters\n",
    "        original_params = {name: param.clone() for name, param in self.model.named_parameters()}\n",
    "\n",
    "        # Require gradients to update the CLIP parameters\n",
    "        self.require_CLIP_gradients(state = True)\n",
    "\n",
    "        try:\n",
    "            self.model.train()\n",
    "            batch = self.augment_image(image, num_augmentations)\n",
    "            batch_features = self.model.encode_image(batch)\n",
    "            norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
    "            if (norms == 0).any():\n",
    "                print(\"Zero norm found in image features\")\n",
    "            batch_features = batch_features / norms.clamp_min(1e-10)\n",
    "            loss = self.entropy_loss_MEMO(batch_features)\n",
    "            self.grad_descent_step(loss)\n",
    "\n",
    "            if any(torch.isnan(param).any() for param in self.model.parameters()):\n",
    "                print(\"nan values detected in model parameters after updating\")\n",
    "            # Predict using the updated model\n",
    "            prediction, probs, entropy = self.predict(image)\n",
    "        finally:\n",
    "            # Restore original parameters\n",
    "            with torch.no_grad():\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    param.copy_(original_params[name])\n",
    "        return prediction, probs.squeeze(), entropy\n",
    "\n",
    "    def TPT(self, image, num_augmentations=100):\n",
    "        self.model.eval()\n",
    "        self.require_CLIP_gradients(False)\n",
    "        batch = self.augment_image(image, num_augmentations)\n",
    "        batch_features = self.model.encode_image(batch)\n",
    "        norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
    "        if (norms == 0).any():\n",
    "            print(\"Zero norm found in image features\")\n",
    "        batch_features = batch_features / norms.clamp_min(1e-10)\n",
    "\n",
    "        entropy, avg_probs = self.entropy_loss_TPT(batch_features)\n",
    "        prediction = torch.argmax(avg_probs).item()\n",
    "        return prediction, avg_probs.squeeze(), float(entropy)\n",
    "\n",
    "    #Implementing the Entropy Boost stuff:\n",
    "    def pick_candidates(self, tensor, classifier, top_num):\n",
    "        # to select a subset of \"candidates\" from a given tensor based on scores provided by a classifier\n",
    "        _, top_indices = torch.topk(classifier, top_num)\n",
    "        candidates = torch.squeeze(tensor[top_indices])\n",
    "        \n",
    "        return candidates, top_indices\n",
    "\n",
    "    def expand_tensor(self, tensor, top_indices, n):\n",
    "    \n",
    "        exp_tensor = torch.zeros(n).to(self.device)\n",
    "        for i in range(top_indices.shape[0]): exp_tensor[top_indices[i]] = tensor[i]\n",
    "    \n",
    "        return exp_tensor\n",
    "\n",
    "    def generate_augmentations_similarities(self, image, image_features, txt_candidates, num_augmentations, top_aug_num):\n",
    "        # augment\n",
    "        torch.manual_seed(33+self.changeseed)\n",
    "        augmentations = transforms.Compose([\n",
    "                                transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.333)),\n",
    "                                ])\n",
    "        batch = self.augment_image(image, num_augmentations)\n",
    "        batch_features = self.model.encode_image(batch)\n",
    "        norms = batch_features.norm(dim=-1, p=2, keepdim=True)\n",
    "        if (norms == 0).any():\n",
    "            print(\"Zero norm found in image features\")\n",
    "        batch_features = batch_features / norms.clamp_min(1e-10)\n",
    "\n",
    "    \n",
    "        # pick the ones closest to the original image\n",
    "        probs_matrix = self.class_probabilities(image_features, batch_features).squeeze()\n",
    "        candidates_features, top_indices_aug = self.pick_candidates(batch_features, probs_matrix, top_num = top_aug_num)\n",
    "\n",
    "        candidates_probs_matrix = self.class_probabilities(candidates_features, txt_candidates).squeeze()\n",
    "    \n",
    "        del batch # avoid Cuda to run out of memory?\n",
    "        return torch.mean(candidates_probs_matrix, dim=0)\n",
    "    \n",
    "    def boost_augmentations(self, image, image_features, prob, num_augmentations, num_candidates, top_aug_num):\n",
    "    \n",
    "        _, top_indices = torch.topk(prob, num_candidates)\n",
    "        text_candidates = self.text_features[top_indices]\n",
    "    \n",
    "        n = prob.shape[0]\n",
    "\n",
    "        #Candidates probabilities in the original image\n",
    "        candidates_prob_og = self.class_probabilities(image_features, text_candidates).squeeze()\n",
    "        candidates_prob_og = self.expand_tensor(candidates_prob_og, top_indices, n)\n",
    "\n",
    "        #Candidates averaged probability in the augmentations\n",
    "        candidates_avg_prob = self.generate_augmentations_similarities(image, image_features, text_candidates, num_augmentations, top_aug_num)\n",
    "        candidates_avg_prob = self.expand_tensor(candidates_avg_prob, top_indices, n)\n",
    "    \n",
    "        return candidates_prob_og, candidates_avg_prob\n",
    "\n",
    "    def entropyboosting(self, image, num_augmentations = 100, num_candidates = 10, top_aug_num = 5):\n",
    "        self.model.eval()\n",
    "        self.require_CLIP_gradients(False)\n",
    "        if num_candidates <= 1:\n",
    "            print(\"If num_candidates=0 this is just CLIP\")\n",
    "            return\n",
    "        image_prepro = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        image_features = self.model.encode_image(image_prepro)\n",
    "        norms = image_features.norm(dim=-1, p=2,  keepdim=True)\n",
    "        image_features = image_features / norms.clamp_min(1e-10)\n",
    "\n",
    "        #Compute out of the box CLIP probability distribution and prediction\n",
    "        clip_probs = self.class_probabilities(image_features, self.text_features).squeeze()\n",
    "        clip_prediction = torch.argmax(clip_probs).item()\n",
    "\n",
    "        phi = 2 / (1 + np.sqrt(5)) # Aura section - math fetish but it seems to work so...\n",
    "        \n",
    "        # Defining loop variables\n",
    "        aug_prob = None #Should this be an input of the method??\n",
    "        output_prob = clip_probs.clone() #Is this necessary??\n",
    "        max_iter = 4\n",
    "        iter = 0\n",
    "        while iter < max_iter:\n",
    "            self.changeseed = self.changeseed + 1 #To make different random transformations at every iter but replicable\n",
    "            candidates_prob_og, candidates_avg_prob = self.boost_augmentations(image, image_features, output_prob, num_augmentations, num_candidates, top_aug_num)\n",
    "            \n",
    "            clip_probs = phi * clip_probs + (1 - phi) * candidates_prob_og #Why are we doing this to the clip probs?\n",
    "            \n",
    "            if aug_prob == None: aug_prob = candidates_avg_prob\n",
    "            else:                aug_prob = 0.5 * aug_prob + 0.5 * candidates_avg_prob\n",
    "            \n",
    "            output_prob = 0.6 * clip_probs + 0.4 * aug_prob # give a little more weight to the probability of the image.\n",
    "            EB_prediction = torch.argmax(output_prob).item()\n",
    "            \n",
    "            # if clip_prediction != EB_prediction then it changed its mind: the class with highest probability has changed - better do further checks.\n",
    "            if clip_prediction == EB_prediction: break\n",
    "\n",
    "            iter += 1\n",
    "        self.changeseed = 0\n",
    "        entropy = self.compute_entropy(output_prob)\n",
    "        return EB_prediction, output_prob.squeeze(), float(entropy)\n",
    "\n",
    "\n",
    "# Preparing the class for usage\n",
    "clip_model = CLIPModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yYC2YqhknDk"
   },
   "source": [
    "1 IMAGE TRIES ON CLIP, MEMO, TPT and EB :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B52IQTi6OyfW",
    "outputId": "d8e52605-ea10-48f4-e56e-776e5a2b1c21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading CIFAR100 for one image tries_\n",
    "cifar100 = clip_model.load_data()\n",
    "clip_model.tokenize_labels(cifar100.classes)\n",
    "image, class_id = cifar100[3637]\n",
    "len(cifar100.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "2.208749294281006\n",
      "tensor([4.4409e-04, 2.0257e-03, 1.7850e-03, 6.7053e-05, 6.8302e-05, 1.2799e-04,\n",
      "        2.1407e-04, 8.0405e-03, 3.6831e-05, 1.3982e-04, 3.8572e-04, 3.0443e-04,\n",
      "        1.2713e-05, 3.9793e-05, 7.2395e-04, 5.2543e-05, 6.3892e-04, 1.7170e-05,\n",
      "        1.5631e-03, 3.5367e-04, 1.0214e-04, 4.6957e-04, 1.1729e-04, 1.9360e-05,\n",
      "        2.2551e-04, 6.2725e-05, 2.7089e-03, 4.3508e-02, 2.1955e-04, 1.7460e-03,\n",
      "        4.6290e-06, 1.6038e-04, 1.2927e-02, 3.8837e-05, 5.5391e-06, 2.1283e-04,\n",
      "        3.4115e-04, 4.3326e-05, 3.8308e-05, 9.3167e-05, 4.4401e-04, 2.5278e-03,\n",
      "        1.5629e-02, 3.2312e-04, 4.5569e-02, 5.2967e-05, 1.3752e-04, 4.2287e-05,\n",
      "        3.7950e-05, 1.7290e-04, 2.4531e-03, 1.0843e-03, 2.3604e-04, 2.8239e-04,\n",
      "        4.5561e-05, 5.3201e-05, 3.1678e-04, 5.2178e-05, 9.5018e-05, 5.7305e-04,\n",
      "        3.0544e-04, 6.4293e-04, 2.1116e-04, 1.2552e-04, 1.8514e-04, 2.5836e-04,\n",
      "        1.7865e-04, 2.3634e-04, 2.7643e-06, 4.6131e-06, 6.4169e-05, 2.6901e-05,\n",
      "        4.1291e-04, 1.9324e-05, 4.9106e-04, 9.3546e-05, 1.0680e-05, 1.4651e-02,\n",
      "        5.8744e-01, 1.0724e-03, 3.9147e-05, 2.2297e-05, 2.9193e-04, 3.2322e-02,\n",
      "        7.7673e-05, 2.4090e-03, 5.8943e-04, 9.7330e-05, 8.7041e-04, 4.2988e-05,\n",
      "        6.9078e-05, 1.6199e-04, 5.6775e-05, 1.9426e-01, 2.7203e-05, 6.5766e-06,\n",
      "        4.0043e-05, 2.9092e-05, 1.3313e-04, 1.1812e-02], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Try entropyboosting\n",
    "prediction, probs, entropy = clip_model.entropyboosting(image)\n",
    "print(prediction)\n",
    "print(entropy)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "087FNIiqpDUb",
    "outputId": "157fb454-8aae-497a-b5c6-aecf4b3d4569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "2.3266210556030273\n",
      "tensor([[1.1976e-03, 5.4628e-03, 4.8136e-03, 1.8082e-04, 1.8419e-04, 3.4514e-04,\n",
      "         5.7728e-04, 8.2105e-03, 9.9322e-05, 3.7705e-04, 1.0402e-03, 8.2096e-04,\n",
      "         3.4284e-05, 1.0731e-04, 1.9523e-03, 1.4169e-04, 1.7230e-03, 4.6304e-05,\n",
      "         4.2152e-03, 9.5374e-04, 2.7545e-04, 1.2663e-03, 3.1629e-04, 5.2210e-05,\n",
      "         6.0814e-04, 1.6915e-04, 7.3052e-03, 1.7485e-02, 5.9205e-04, 4.7085e-03,\n",
      "         1.2483e-05, 4.3251e-04, 8.4594e-03, 1.0473e-04, 1.4937e-05, 5.7395e-04,\n",
      "         9.1999e-04, 1.1684e-04, 1.0331e-04, 2.5125e-04, 1.1974e-03, 6.8169e-03,\n",
      "         1.7164e-02, 8.7137e-04, 1.8750e-02, 1.4284e-04, 3.7087e-04, 1.1404e-04,\n",
      "         1.0234e-04, 4.6626e-04, 6.6154e-03, 2.9242e-03, 6.3653e-04, 7.6153e-04,\n",
      "         1.2287e-04, 1.4347e-04, 8.5428e-04, 1.4071e-04, 2.5624e-04, 1.5454e-03,\n",
      "         8.2368e-04, 1.7338e-03, 5.6945e-04, 3.3850e-04, 4.9928e-04, 6.9673e-04,\n",
      "         4.8177e-04, 6.3735e-04, 7.4547e-06, 1.2440e-05, 1.7305e-04, 7.2545e-05,\n",
      "         1.1135e-03, 5.2111e-05, 1.3243e-03, 2.5227e-04, 2.8802e-05, 1.5945e-02,\n",
      "         6.5313e-01, 2.8920e-03, 1.0557e-04, 6.0129e-05, 7.8726e-04, 3.8252e-02,\n",
      "         2.0946e-04, 6.4964e-03, 1.5895e-03, 2.6247e-04, 2.3473e-03, 1.1593e-04,\n",
      "         1.8628e-04, 4.3683e-04, 1.5311e-04, 1.2288e-01, 7.3359e-05, 1.7735e-05,\n",
      "         1.0799e-04, 7.8453e-05, 3.5900e-04, 8.4517e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Prediction using CLIP out of the box\n",
    "prediction1, probs1, entropy1 = clip_model.predict(image)\n",
    "print(prediction1)\n",
    "print(entropy1)\n",
    "print(probs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "_RheDQxnoeB7",
    "outputId": "7b03df7f-81bc-4555-c017-377d44e5b3c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "2.077364206314087\n",
      "tensor([[9.8620e-04, 4.7828e-03, 4.6798e-03, 1.5572e-04, 1.5197e-04, 3.0867e-04,\n",
      "         4.7689e-04, 6.2922e-03, 8.2343e-05, 3.0923e-04, 8.7553e-04, 8.1575e-04,\n",
      "         2.9356e-05, 8.9023e-05, 1.6763e-03, 1.1114e-04, 1.4614e-03, 3.6086e-05,\n",
      "         3.8958e-03, 8.0583e-04, 2.2591e-04, 1.1260e-03, 2.5036e-04, 4.4503e-05,\n",
      "         4.7512e-04, 1.3567e-04, 5.4395e-03, 1.5579e-02, 5.2545e-04, 4.0048e-03,\n",
      "         1.0843e-05, 3.6359e-04, 7.3783e-03, 9.3484e-05, 1.3365e-05, 5.7313e-04,\n",
      "         7.7305e-04, 1.0464e-04, 9.2250e-05, 1.8495e-04, 9.6642e-04, 6.5645e-03,\n",
      "         1.4847e-02, 7.2444e-04, 1.7678e-02, 1.1078e-04, 3.5591e-04, 1.0298e-04,\n",
      "         7.9667e-05, 3.7379e-04, 6.0870e-03, 2.3019e-03, 5.8701e-04, 6.6893e-04,\n",
      "         1.1238e-04, 1.3150e-04, 7.1440e-04, 1.1645e-04, 2.0226e-04, 1.4091e-03,\n",
      "         7.9140e-04, 1.5631e-03, 4.4642e-04, 2.7754e-04, 4.8233e-04, 6.4062e-04,\n",
      "         4.2063e-04, 5.6923e-04, 6.2323e-06, 9.5952e-06, 1.4697e-04, 6.2266e-05,\n",
      "         9.2542e-04, 4.5426e-05, 1.2508e-03, 2.6994e-04, 2.5094e-05, 1.3129e-02,\n",
      "         7.0722e-01, 2.4452e-03, 1.0118e-04, 5.2856e-05, 6.6586e-04, 3.6857e-02,\n",
      "         1.8823e-04, 4.7386e-03, 1.3757e-03, 2.2036e-04, 2.0671e-03, 8.7088e-05,\n",
      "         1.4390e-04, 3.7851e-04, 1.2681e-04, 9.3765e-02, 6.8134e-05, 1.4196e-05,\n",
      "         1.0045e-04, 6.6866e-05, 3.4894e-04, 8.3519e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Prediction using MEMO with SGD\n",
    "clip_model.use_SGD()\n",
    "prediction2, probs2, entropy2 = clip_model.MEMO(image, num_augmentations=10)\n",
    "print(prediction2)\n",
    "print(entropy2)\n",
    "print(probs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mlTDDcr_IRYf",
    "outputId": "8a58029d-da53-4ed3-96a2-5a039d91d2ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "0.3890056312084198\n",
      "tensor([[7.0291e-05, 1.2202e-03, 2.0966e-03, 2.8387e-05, 4.4688e-06, 2.8498e-05,\n",
      "         3.2856e-05, 6.2306e-05, 1.0148e-05, 4.9512e-05, 2.7819e-04, 5.3239e-04,\n",
      "         6.4909e-06, 6.8183e-06, 9.8106e-05, 1.4464e-06, 4.3893e-04, 2.5695e-06,\n",
      "         4.9707e-04, 5.9817e-05, 5.5518e-06, 6.2155e-05, 4.5031e-06, 1.0935e-05,\n",
      "         5.5434e-06, 4.1206e-06, 2.6603e-05, 4.4461e-04, 3.8518e-04, 2.5397e-04,\n",
      "         1.2974e-06, 1.2624e-05, 4.2113e-05, 4.5163e-05, 2.7994e-06, 4.2294e-04,\n",
      "         7.2896e-05, 4.4254e-05, 1.0838e-05, 1.8192e-06, 3.2608e-05, 2.0624e-03,\n",
      "         5.1893e-04, 3.0729e-05, 3.5040e-03, 1.8276e-06, 1.2709e-04, 2.3962e-05,\n",
      "         5.4501e-06, 1.4678e-05, 1.1937e-03, 3.1151e-05, 1.7511e-04, 5.6660e-05,\n",
      "         2.8462e-05, 1.3722e-05, 4.6307e-05, 2.8664e-06, 9.7663e-06, 8.9210e-04,\n",
      "         2.8619e-04, 3.3893e-04, 2.8193e-06, 5.9115e-06, 1.3063e-04, 8.1566e-05,\n",
      "         3.1865e-05, 1.4796e-05, 5.0939e-07, 1.6153e-06, 1.7927e-05, 1.1527e-05,\n",
      "         8.4458e-06, 4.9185e-06, 1.6388e-04, 6.4259e-04, 8.2300e-06, 1.8448e-04,\n",
      "         9.5956e-01, 1.1634e-04, 1.8333e-05, 6.7579e-06, 2.8254e-05, 1.6869e-02,\n",
      "         4.2036e-05, 1.1157e-04, 7.6165e-05, 1.6402e-05, 2.6695e-04, 5.7434e-06,\n",
      "         6.3032e-06, 7.7760e-05, 5.5851e-06, 1.5060e-04, 1.2123e-05, 7.9850e-07,\n",
      "         7.1923e-05, 1.1627e-05, 1.2605e-04, 4.3244e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Prediction using MEMO with ADAM\n",
    "clip_model.use_ADAM()\n",
    "clip_model.tokenize_labels(cifar100.classes) #You have to tokenize the labels again for the change in precision\n",
    "prediction3, probs3, entropy3 = clip_model.MEMO(image, num_augmentations=10)\n",
    "print(prediction3)\n",
    "print(entropy3)\n",
    "print(probs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nrxX4QrwsCbv",
    "outputId": "828b2345-5334-418b-b40b-444910986132"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "5.611426830291748\n",
      "tensor([0.0064, 0.0020, 0.0094, 0.0159, 0.0024, 0.0015, 0.0036, 0.0047, 0.0037,\n",
      "        0.0079, 0.0029, 0.0115, 0.0021, 0.0032, 0.0041, 0.0024, 0.0094, 0.0036,\n",
      "        0.0035, 0.0114, 0.0044, 0.0107, 0.0029, 0.0023, 0.0009, 0.0038, 0.0035,\n",
      "        0.0242, 0.0036, 0.0294, 0.0014, 0.0065, 0.0031, 0.0114, 0.0015, 0.0182,\n",
      "        0.0032, 0.0038, 0.0027, 0.0075, 0.0080, 0.0273, 0.0121, 0.0070, 0.0103,\n",
      "        0.0014, 0.0164, 0.0213, 0.0034, 0.0102, 0.0039, 0.0064, 0.0574, 0.0015,\n",
      "        0.0009, 0.0005, 0.0391, 0.0023, 0.0041, 0.1248, 0.0108, 0.0077, 0.0039,\n",
      "        0.0030, 0.0048, 0.0042, 0.0057, 0.0013, 0.0009, 0.0124, 0.0019, 0.0014,\n",
      "        0.0031, 0.0034, 0.0014, 0.0026, 0.0052, 0.0044, 0.1003, 0.0022, 0.0020,\n",
      "        0.0020, 0.0149, 0.0114, 0.0022, 0.0200, 0.0051, 0.0162, 0.0090, 0.0066,\n",
      "        0.0070, 0.0077, 0.0038, 0.0256, 0.0022, 0.0030, 0.0308, 0.0060, 0.0206,\n",
      "        0.0081])\n"
     ]
    }
   ],
   "source": [
    "# Prediction using TPT\n",
    "prediction4,prob_avg, entropy4 = clip_model.TPT(image, num_augmentations=10)\n",
    "print(prediction4)\n",
    "print(entropy4)\n",
    "print(prob_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory (just to put it somewhere, later we can move this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that for now we can start writting the explanations for the methods here and then we can reorganize it for the final version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-Time Prompt Tuning (TPT) for Image Classification with CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-Time Prompt Tuning (TPT) is a method designed to enhance the performance of vision-language models like CLIP by adapting the text prompts dynamically at test time. This approach is particularly useful in zero-shot learning scenarios where the model needs to generalize to new tasks without additional training.\n",
    "\n",
    "### Concept Overview\n",
    "\n",
    "TPT optimizes the text prompt to improve consistency and confidence in the model's predictions across various augmented views of a test image. By minimizing the entropy of the prediction distribution average across low-entropy augmentations, TPT attempts to make the model more reliable and robust, even under slight variations in input data.\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "#### Step 1: Initial Setup\n",
    "- **Model Preparation**: Initialize the pre-trained CLIP model using `CLIPModel(model_name='ViT-B/32')`.\n",
    "- **Prompt Initialization**: Start with a basic prompt template like \"a photo of a {object}\", processed using the `tokenize_labels` method.\n",
    "\n",
    "#### Step 2: Augmentation and Prediction\n",
    "- **Generate Augmented Views**: Create multiple augmented versions of the test image using the `augment_image` method, which might include crops, rotations, and color adjustments, etc.\n",
    "\n",
    "#### Step 3: Define the Optimization Objective\n",
    "The goal is to minimize the entropy of the predicted probability distribution across different classes for the test image, computed using the `entropy_loss_TPT` method.\n",
    "\n",
    "##### Equation for Entropy Minimization:\n",
    "\\\\[ p^* = \\arg\\min_p -\\sum_{i=1}^K \\tilde{p}_p(y_i | X_{\\text{test}}) \\log \\tilde{p}_p(y_i | X_{\\text{test}}) \\\\]\n",
    "Where:\n",
    "- \\\\( p^* \\\\) is the optimized prompt.\n",
    "- \\\\( K \\\\) is the number of classes.\n",
    "- \\\\( \\tilde{p}_p(y_i | X_{\\text{test}}) \\\\) is the averaged predicted probability for class \\( y_i \\) also returned in `entropy_loss_TPT`.\n",
    "\n",
    "With the Averaged Prediction Probability defined as:\n",
    "\\\\[ \\tilde{p}_p(y_i | X_{\\text{test}}) = \\frac{1}{N} \\sum_{n=1}^N p_p(y_i | A_i(X_{\\text{test}})) \\\\]\n",
    "- \\\\( N \\\\) is the number of augmented views.\n",
    "- \\\\( A_i(X_{\\text{test}}) \\\\) denotes the \\( i \\)-th augmented view of the test image.\n",
    "\n",
    "#### Step 4: Optimize the Prompt\n",
    "In the paper, they use gradient-based optimization to adjust the prompt parameters (provided by CoOp or CoCoOp) by minimizing the entropy. In our case, we decided that training CoOp or CoCoOp is not what we want to focus on for the project, therefor, for now what `TPT` method gives us is a evaluation of how good the prompts are that we will use to guide a wider classification method.\n",
    "\n",
    "#### Step 5: Confidence Selection\n",
    "Filter out noisy predictions by applying a confidence selection mechanism (`confidence_selection`) that discards the augmentations with the highest entropy (over a threshold value).\n",
    "\n",
    "##### Confidence-Based Averaging:\n",
    "\\\\[ \\tilde{p}_p(y | X_{\\text{test}}) = \\frac{1}{\\rho N} \\sum_{i=1}^N 1[H(p_i) \\leq \\tau] p_p(y | A_i(X_{\\text{test}})) \\\\]\n",
    "- \\\\(\\rho\\\\) is the cutoff percentile on the N augmented views (set to 0.8 by default in our implementation).\n",
    "- \\\\( 1[H(p_i) \\leq \\tau] \\\\) is an indicator function that filters predictions based on their entropy \\\\( H \\\\), with \\\\( \\tau \\\\) being a threshold set in the `confidence_selection` method as the entropy of the \\\\(\\rho\\\\) percentile. We are removing the augmentations with the highest entropy in the top 20%.\n",
    "\n",
    "\n",
    "#### Step 6: Final Prediction\n",
    "Apply the optimized prompt to make the final prediction on the original test image using the `TPT` method, which performs the entire sequence described above except for the prompt optimization which we will implement in other method without using CoOp or CoCoOp.\n",
    "\n",
    "### Note:\n",
    "Apart from what the paper says, I TPT alone without prompt optimization returns the averaged probability over low-entropy augmentations without doing any training. This is basically saying that all the top 80% lowest entropy augmentations are equally valid and averages the predictions across them, so the original image does not have more weight in the output than the augmentations. This is not the use proposed bythe paper but I am runing it as well to see how it does. I mean, the idea is not that crazy actually, although it is quite sensitive to the kind of augmentations we do (if we have bad augmentations in the low entropy 80%, we will be saying that those are equally valid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS in AWS Bucket DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to inspect the file directories we have access to with the datasets.\n",
    "\n",
    "With the following function we can print the folders and the first 3 subfolders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to list the s3_bucket folders and the first 3 subfolders\n",
    "def list_datasets_with_subfolders(s3_bucket, s3_region):\n",
    "    s3_client = boto3.client(\"s3\", region_name=s3_region, verify=True)\n",
    "    response = s3_client.list_objects_v2(Bucket=s3_bucket, Delimiter='/')\n",
    "    \n",
    "    # Use a set to keep track of unique folder names\n",
    "    folders = set()\n",
    "    \n",
    "    # Append the common prefixes which are the folder names\n",
    "    for prefix in response.get('CommonPrefixes', []):\n",
    "        folders.add(prefix.get('Prefix'))\n",
    "    \n",
    "    # Print out the available folders and their first 3 subfolders in the bucket\n",
    "    for folder in sorted(folders):\n",
    "        print(f\"Folder: {folder}\")\n",
    "        \n",
    "        # List objects within the folder to find subfolders\n",
    "        sub_response = s3_client.list_objects_v2(Bucket=s3_bucket, Prefix=folder, Delimiter='/')\n",
    "        subfolders = []\n",
    "        \n",
    "        for sub_prefix in sub_response.get('CommonPrefixes', []):\n",
    "            subfolders.append(sub_prefix.get('Prefix'))\n",
    "        \n",
    "        # Print the first 3 subfolders\n",
    "        for subfolder in sorted(subfolders)[:3]:\n",
    "            print(f\"  Subfolder: {subfolder}\")\n",
    "        if len(subfolders) > 3:\n",
    "            print(\"  ... and more subfolders\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder: OfficeHomeDataset_10072016/\n",
      "  Subfolder: OfficeHomeDataset_10072016/Art/\n",
      "  Subfolder: OfficeHomeDataset_10072016/Clipart/\n",
      "  Subfolder: OfficeHomeDataset_10072016/Product/\n",
      "  ... and more subfolders\n",
      "\n",
      "Folder: imagenet-a/\n",
      "  Subfolder: imagenet-a/n01498041/\n",
      "  Subfolder: imagenet-a/n01531178/\n",
      "  Subfolder: imagenet-a/n01534433/\n",
      "  ... and more subfolders\n",
      "\n",
      "Folder: imagenetv2-matched-frequency-format-val/\n",
      "  Subfolder: imagenetv2-matched-frequency-format-val/0/\n",
      "  Subfolder: imagenetv2-matched-frequency-format-val/1/\n",
      "  Subfolder: imagenetv2-matched-frequency-format-val/10/\n",
      "  ... and more subfolders\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the folders and first 3 subfolders of the AWS bucket\n",
    "s3_bucket = \"deeplearning2024-datasets\"\n",
    "s3_region = \"eu-west-1\"\n",
    "list_datasets_with_subfolders(s3_bucket, s3_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SERIALIZING OBJECTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store all the relevant information of each prediction, we are going to use the PredictionResult class. This dataclass stores for each image (from which we store the file name as well) we save the indeces and probabilities of the top 5 predicted classes, the true label index and probability and the Shanon entropy of the whole probability vector. So for each image we get a dictionary with all those values.\n",
    "\n",
    "For saving and loading this results we will use .json files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an object to store the result of each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PredictionResult:\n",
    "    image_id: str  # Name of the image file\n",
    "    top_indices: List[int]\n",
    "    top_probabilities: List[float]\n",
    "    true_label_index: int\n",
    "    true_label_probability: float\n",
    "    prediction_entropy: float\n",
    "\n",
    "    def __init__(self, name_img: str, prediction: int, probs: torch.Tensor, entropy: float, num_top: int = 5, decimal_places: int = 6):\n",
    "        self.image_id = name_img\n",
    "        # Get the top 'num_top' indices and probabilities\n",
    "        top_probs, top_indices = torch.topk(probs, num_top)\n",
    "        self.top_indices = top_indices.tolist()\n",
    "        self.top_probabilities = [round(prob.item(), decimal_places) for prob in top_probs]\n",
    "        # True label information\n",
    "        self.true_label_index = prediction\n",
    "        self.true_label_probability = round(probs[prediction].item(), decimal_places)\n",
    "        # Prediction entropy\n",
    "        self.prediction_entropy = entropy\n",
    "\n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, data: dict):\n",
    "        obj = cls.__new__(cls)  # Create a new instance without calling __init__\n",
    "        # Directly assign the values\n",
    "        obj.image_id = data['image_id']\n",
    "        obj.top_indices = data['top_indices']\n",
    "        obj.top_probabilities = data['top_probabilities']\n",
    "        obj.true_label_index = data['true_label_index']\n",
    "        obj.true_label_probability = data['true_label_probability']\n",
    "        obj.prediction_entropy = data['prediction_entropy']\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 'example_image.jpg', 'top_indices': [2, 1, 3], 'top_probabilities': [0.4, 0.3, 0.2], 'true_label_index': 2, 'true_label_probability': 0.4, 'prediction_entropy': 0.97}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "name_img = 'example_image.jpg'\n",
    "prediction = 2\n",
    "probs = torch.tensor([0.1, 0.3, 0.4, 0.2])  # example tensor\n",
    "entropy = 0.97\n",
    "num_top = 3\n",
    "\n",
    "pred_result = PredictionResult(name_img, prediction, probs, entropy, num_top)\n",
    "print(pred_result.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a function to serialize a list of prediction objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serializing a list of Prediction objects\n",
    "\n",
    "# Function to serialize a list of PredictionResult objects\n",
    "def serialize_results(prediction_results: List[PredictionResult], file_path: str):\n",
    "    # Convert each PredictionResult object to a dictionary\n",
    "    results_as_dicts = [result.to_dict() for result in prediction_results]\n",
    "    \n",
    "    # Serialize the list of dictionaries to a JSON string\n",
    "    json_string = json.dumps(results_as_dicts, indent=4)\n",
    "    \n",
    "    # Write the JSON string to a file\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "prediction_results = [\n",
    "    PredictionResult('example_image1.jpg', 2, torch.tensor([0.1, 0.3, 0.4, 0.2]), 0.97, 3),\n",
    "    PredictionResult('example_image2.jpg', 0, torch.tensor([0.6, 0.2, 0.1, 0.1]), 0.85, 3)\n",
    "]\n",
    "\n",
    "serialize_results(prediction_results, 'prediction_results.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a function to load the serialized list of predictions from the .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to deserialize a list of PredictionResult objects from a file\n",
    "# Function to deserialize a list of PredictionResult objects from a file\n",
    "def deserialize_results(file_path: str) -> List[PredictionResult]:\n",
    "    # Read the JSON string from the file\n",
    "    with open(file_path, 'r') as f:\n",
    "        json_string = f.read()\n",
    "    \n",
    "    # Deserialize the JSON string to a list of dictionaries\n",
    "    results_as_dicts = json.loads(json_string)\n",
    "    \n",
    "    # Convert each dictionary back to a PredictionResult object using from_dict\n",
    "    prediction_results = [PredictionResult.from_dict(d) for d in results_as_dicts]\n",
    "    \n",
    "    return prediction_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_id': 'example_image1.jpg', 'top_indices': [2, 1, 3], 'top_probabilities': [0.4, 0.3, 0.2], 'true_label_index': 2, 'true_label_probability': 0.4, 'prediction_entropy': 0.97}\n",
      "{'image_id': 'example_image2.jpg', 'top_indices': [0, 1, 3], 'top_probabilities': [0.6, 0.2, 0.1], 'true_label_index': 0, 'true_label_probability': 0.6, 'prediction_entropy': 0.85}\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "deserialized_results = deserialize_results('prediction_results.json')\n",
    "for result in deserialized_results:\n",
    "    print(result.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing on testing function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to inherit the dataset class and define our own version to access the AWS bucket. \n",
    "\n",
    "Note: for the imageneta we have some class identifiers which are of the form n23423523... So since for creating the class labels we have to use the class names, we get them from a map defined using the .txt file that I found in HuggingFace. Also, the part were it sorts those class identifiers... I just copied it from the notebook from moodle but for this dataset, doesn't make much sense. I still have to implement the classnames for imagenetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redefine for getting the imagefile name as well\n",
    "class S3ImageFolder(Dataset):\n",
    "    def __init__(self, root, transform=None, imageneta=True):\n",
    "        self.s3_bucket = \"deeplearning2024-datasets\"\n",
    "        self.s3_region = \"eu-west-1\"\n",
    "        self.s3_client = boto3.client(\"s3\", region_name=self.s3_region, verify=True)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Get list of objects in the bucket\n",
    "        response = self.s3_client.list_objects_v2(Bucket=self.s3_bucket, Prefix=root)\n",
    "        objects = response.get(\"Contents\", [])\n",
    "        while response.get(\"NextContinuationToken\"):\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.s3_bucket,\n",
    "                Prefix=root,\n",
    "                ContinuationToken=response[\"NextContinuationToken\"]\n",
    "            )\n",
    "            objects.extend(response.get(\"Contents\", []))\n",
    "\n",
    "        # Iterate and keep valid files only\n",
    "        self.instances = []\n",
    "        self.samples = []  # Add samples attribute\n",
    "        for item in objects:\n",
    "            key = item[\"Key\"]\n",
    "            path = Path(key)\n",
    "            \n",
    "            # Check if file is valid\n",
    "            if path.suffix.lower() not in (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\"):\n",
    "                continue\n",
    "\n",
    "            # Get label\n",
    "            label = path.parent.name\n",
    "\n",
    "            # Keep track of valid instances\n",
    "            self.instances.append((label, key))\n",
    "            self.samples.append((key, label))  # Add to samples\n",
    "\n",
    "        # Sort classes in alphabetical order (as in ImageFolder)\n",
    "        self.classes = sorted(set(label for label, _ in self.instances)) #This contains keys of the form n0349583...\n",
    "        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n",
    "        # Get the labels that will be used for the CLIP text_features\n",
    "        if imageneta:\n",
    "            self.class_to_name = self.map_classnames_imagenetA()\n",
    "            self.class_names = [self.class_to_name[cls] for cls in self.classes if cls in self.class_to_name]\n",
    "\n",
    "    \n",
    "    def map_classnames_imagenetA(self):\n",
    "        # Define the path to the words file\n",
    "        file_path = 'List_ClassNames_imageneta.txt'\n",
    "    \n",
    "        # Initialize an empty dictionary to store the class names with their corresponding IDs\n",
    "        class_dict = {}\n",
    "    \n",
    "        # Open and read the file line by line\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                # Split each line into WordNet ID and class name, and strip to remove any leading/trailing whitespace\n",
    "                parts = line.strip().split(' ', 1)\n",
    "                if len(parts) > 1:\n",
    "                    # Use the WordNet ID as the key and the class name as the value in the dictionary\n",
    "                    class_dict[parts[0]] = parts[1]\n",
    "    \n",
    "        return class_dict\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label, key = self.instances[idx]\n",
    "        response = self.s3_client.get_object(Bucket=self.s3_bucket, Key=key)\n",
    "        img = Image.open(response['Body'])\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        filename = os.path.basename(key)  # Extract the filename from the key\n",
    "\n",
    "        return img, self.class_to_idx[label], filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is to create a subset with the same amount of images per each class. The class inherited is because I neede to do the initialization of the datasetclass to avoid some errors. Don't worry much about it. \n",
    "\n",
    "The subset that the function returns is just the same object as the full dataset but with a fixed amount of images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a subclass of Subset to include additional attributes\n",
    "class SubsetWithAttributes(Subset):\n",
    "    def __init__(self, dataset, indices):\n",
    "        super().__init__(dataset, indices)\n",
    "        self.classes = dataset.classes\n",
    "        self.class_to_idx = dataset.class_to_idx\n",
    "\n",
    "#Function for getting the subset\n",
    "def create_stratified_subset(dataset, num_samples_per_class=5):\n",
    "    # Fix the random seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Determine class indices\n",
    "    targets = np.array([dataset.class_to_idx[s[1]] for s in dataset.samples])\n",
    "    classes, class_indices = np.unique(targets, return_inverse=True)\n",
    "\n",
    "    # Select samples from each class\n",
    "    indices = []\n",
    "    for c in classes:\n",
    "        class_idx = np.where(class_indices == c)[0]\n",
    "        if len(class_idx) >= num_samples_per_class:\n",
    "            selected_indices = np.random.choice(class_idx, num_samples_per_class, replace=False)\n",
    "            indices.extend(selected_indices)\n",
    "        else:\n",
    "            # If a class has fewer than the desired number, take all\n",
    "            indices.extend(class_idx)\n",
    "\n",
    "    # Create subset\n",
    "    subset = SubsetWithAttributes(dataset, indices)\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of a subset of Imagenet-A with 5 samples per class\n",
    "subset = create_stratified_subset(imageneta, num_samples_per_class=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subclass of Subset to include additional attributes\n",
    "class SubsetWithAttributes(Subset):\n",
    "    def __init__(self, dataset, indices):\n",
    "        super().__init__(dataset, indices)\n",
    "        self.classes = dataset.classes\n",
    "        self.class_to_idx = dataset.class_to_idx\n",
    "\n",
    "#Function for getting the subset\n",
    "def create_stratified_subset(dataset, num_samples_per_class=5):\n",
    "    # Fix the random seed for reproducibility\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Determine class indices\n",
    "    targets = np.array([dataset.class_to_idx[s[1]] for s in dataset.samples])\n",
    "    classes, class_indices = np.unique(targets, return_inverse=True)\n",
    "\n",
    "    # Select samples from each class\n",
    "    indices = []\n",
    "    for c in classes:\n",
    "        class_idx = np.where(class_indices == c)[0]\n",
    "        if len(class_idx) >= num_samples_per_class:\n",
    "            selected_indices = np.random.choice(class_idx, num_samples_per_class, replace=False)\n",
    "            indices.extend(selected_indices)\n",
    "        else:\n",
    "            # If a class has fewer than the desired number, take all\n",
    "            indices.extend(class_idx)\n",
    "\n",
    "    # Create subset\n",
    "    subset = SubsetWithAttributes(dataset, indices)\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of a subset of Imagenet-A with 5 samples per class (Remember to define imagenetA before creating a subset)\n",
    "subset = create_stratified_subset(imageneta, num_samples_per_class=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define the testing function. It tokenizes the class labels of the dataset using the tokenize_labels. So if we want to use different labels, we should probably create another method of CLIP and put it here or modify the existing tokenize_labels to do something else than \"A photo of a {class}\".\n",
    "\n",
    "Note: In the implementation, inside the loop, I declaring the resultfilename in every single iteration. Which does not make sense from a performance point of view. But I did not want to put it outside the loop and put another 4 conditionals because I thought it would be less readable and the performance, would not be that much worse.\n",
    "\n",
    "Note 2: Also the average calculations, could be done with the data from the .json files, so there is really no point on doing it here but I left it since I had it from a previous implementation of the tests. But I think, we should leave the testing just to create the .json files and then use them to do all the computations afterwards, without having CLIP computing stuff around. Just used the already collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the testing function\n",
    "def testing(dataset, model, method='CLIP', batch_size=32, num_aug=20):\n",
    "    model.tokenize_labels(dataset.class_names)\n",
    "\n",
    "    def custom_collate_fn(batch):\n",
    "        images = [item[0] for item in batch]  # PIL images\n",
    "        labels = [item[1] for item in batch]  # Corresponding labels\n",
    "        filenames = [item[2] for item in batch]    # Corresponding keys/IDs\n",
    "        return images, labels, filenames\n",
    "    \n",
    "    #We use num_workers=4 but this only works well when using the gpu, not the cpu.\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn, num_workers=4)\n",
    "\n",
    "    # Initialize lists to store results\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    entropies = []\n",
    "    confidences = []\n",
    "    results = []\n",
    "\n",
    "    # Evaluation loop\n",
    "    for images, labels, filenames in tqdm(dataloader):  \n",
    "        for image, label, filename in zip(images, labels, filenames): \n",
    "            try:\n",
    "                # We choose how the test time predictions are made\n",
    "                if method == 'CLIP':\n",
    "                    prediction, probs, entropy = model.predict(image)\n",
    "                    resultsfilename = 'CLIP_Base_results.json'\n",
    "                elif method == 'MEMO':\n",
    "                    prediction, probs, entropy = model.MEMO(image, num_augmentations=num_aug)\n",
    "                    resultsfilename = 'CLIP_MEMO_results.json'\n",
    "                elif method == 'TPT':\n",
    "                    prediction, probs, entropy = model.TPT(image, num_augmentations=num_aug)\n",
    "                    resultsfilename = 'CLIP_TPT_results.json'\n",
    "                elif method == 'EB':\n",
    "                    prediction, probs, entropy = model.entropyboosting(image, num_augmentations=num_aug)\n",
    "                    resultsfilename = 'CLIP_EB_results.json'\n",
    "                else:\n",
    "                    print('Enter a valid method for testing.')\n",
    "\n",
    "                #Store the results of each image:\n",
    "                results.append(PredictionResult(filename, prediction, probs, entropy))\n",
    "\n",
    "                #Some computations for getting average results at the end\n",
    "                if int(prediction) == int(label):\n",
    "                    correct_predictions += 1\n",
    "                total_predictions += 1\n",
    "                entropies.append(entropy)\n",
    "                confidences.append(torch.max(probs).item())\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "\n",
    "    #Serializing results for each image:\n",
    "    serialize_results(results, resultsfilename)\n",
    "    \n",
    "    # Post evaluation general statistics\n",
    "    accuracy = (correct_predictions / total_predictions) * 100\n",
    "    average_entropy = sum(entropies) / len(entropies)\n",
    "    average_confidence = sum(confidences) / len(confidences)\n",
    "    print(f'Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Average entropy across all predictions: {average_entropy:.2f}')\n",
    "    print(f'Average confidence across all predictions: {average_confidence:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After all the definitions here we create the dataset object and use the testing function with the different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the imagenet-a\n",
    "s3_rootA = 'imagenet-a'\n",
    "imageneta = S3ImageFolder(root=s3_rootA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the imagenet-v2\n",
    "s3_rootV2 = 'imagenetv2-matched-frequency-format-val'\n",
    "imagenetv2 = S3ImageFolder(root=s3_rootV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stingray', 'goldfinch', 'junco', 'American robin', 'jay', 'bald eagle', 'vulture', 'newt', 'American bullfrog', 'box turtle', 'green iguana', 'agama', 'chameleon', 'American alligator', 'garter snake', 'harvestman', 'scorpion', 'tarantula', 'centipede', 'sulphur-crested cockatoo', 'lorikeet', 'hummingbird', 'toucan', 'duck', 'goose', 'koala', 'jellyfish', 'sea anemone', 'flatworm', 'snail', 'crayfish', 'hermit crab', 'flamingo', 'great egret', 'oystercatcher', 'pelican', 'sea lion', 'Chihuahua', 'Golden Retriever', 'Rottweiler', 'German Shepherd Dog', 'pug', 'red fox', 'Persian cat', 'lynx', 'lion', 'American black bear', 'mongoose', 'ladybug', 'rhinoceros beetle', 'weevil', 'fly', 'bee', 'ant', 'grasshopper', 'stick insect', 'cockroach', 'mantis', 'leafhopper', 'dragonfly', 'monarch butterfly', 'small white', 'gossamer-winged butterfly', 'starfish', 'cottontail rabbit', 'porcupine', 'fox squirrel', 'marmot', 'bison', 'skunk', 'armadillo', 'baboon', 'white-headed capuchin', 'African bush elephant', 'pufferfish', 'academic gown', 'accordion', 'acoustic guitar', 'airliner', 'ambulance', 'apron', 'balance beam', 'balloon', 'banjo', 'barn', 'wheelbarrow', 'basketball', 'lighthouse', 'beaker', 'bikini', 'bow', 'bow tie', 'breastplate', 'broom', 'candle', 'canoe', 'castle', 'cello', 'chain', 'chest', 'Christmas stocking', 'cowboy boot', 'cradle', 'rotary dial telephone', 'digital clock', 'doormat', 'drumstick', 'dumbbell', 'envelope', 'feather boa', 'flagpole', 'forklift', 'fountain', 'garbage truck', 'goblet', 'go-kart', 'golf cart', 'grand piano', 'hair dryer', 'clothes iron', \"jack-o'-lantern\", 'jeep', 'kimono', 'lighter', 'limousine', 'manhole cover', 'maraca', 'marimba', 'mask', 'mitten', 'mosque', 'nail', 'obelisk', 'ocarina', 'organ', 'parachute', 'parking meter', 'piggy bank', 'billiard table', 'hockey puck', 'quill', 'racket', 'reel', 'revolver', 'rocking chair', 'rugby ball', 'salt shaker', 'sandal', 'saxophone', 'school bus', 'schooner', 'sewing machine', 'shovel', 'sleeping bag', 'snowmobile', 'snowplow', 'soap dispenser', 'spatula', 'spider web', 'steam locomotive', 'stethoscope', 'couch', 'submarine', 'sundial', 'suspension bridge', 'syringe', 'tank', 'teddy bear', 'toaster', 'torch', 'tricycle', 'umbrella', 'unicycle', 'viaduct', 'volleyball', 'washing machine', 'water tower', 'wine bottle', 'shipwreck', 'guacamole', 'pretzel', 'cheeseburger', 'hot dog', 'broccoli', 'cucumber', 'bell pepper', 'mushroom', 'lemon', 'banana', 'custard apple', 'pomegranate', 'carbonara', 'bubble', 'cliff', 'volcano', 'baseball player', 'rapeseed', \"yellow lady's slipper\", 'corn', 'acorn']\n"
     ]
    }
   ],
   "source": [
    "# Class names from ImagenetA\n",
    "print(imageneta.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 215/215 [01:47<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 29.85%\n",
      "Average entropy across all predictions: 3.16\n",
      "Average confidence across all predictions: 0.46\n"
     ]
    }
   ],
   "source": [
    "#subset = create_stratified_subset(imageneta, num_samples_per_class=1)\n",
    "testing(imageneta, clip_model, method='CLIP', batch_size=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 215/215 [1:23:58<00:00, 23.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 30.81%\n",
      "Average entropy across all predictions: 2.69\n",
      "Average confidence across all predictions: 0.45\n"
     ]
    }
   ],
   "source": [
    "testing(imageneta, clip_model, method='EB', batch_size=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 215/215 [1:16:49<00:00, 21.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.93%\n",
      "Average entropy across all predictions: 2.67\n",
      "Average confidence across all predictions: 0.53\n"
     ]
    }
   ],
   "source": [
    "testing(imageneta, clip_model, method='MEMO', batch_size=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 215/215 [1:02:46<00:00, 17.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 27.60%\n",
      "Average entropy across all predictions: 4.70\n",
      "Average confidence across all predictions: 0.25\n"
     ]
    }
   ],
   "source": [
    "testing(imageneta, clip_model, method='TPT', batch_size=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is to compute some meaningful values from the data logged into the .json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "from typing import List, Dict\n",
    "\n",
    "def analyze_results(results_list: List[PredictionResult]) -> Dict[str, List[float]]:\n",
    "    # Dictionary to hold statistics for each class\n",
    "    class_stats = {}\n",
    "    # Global count of cases where the true label was among the top 5 predictions\n",
    "    global_top5 = 0\n",
    "    # Lists to hold entropies for all correctly and incorrectly predicted cases globally\n",
    "    correct_global_entropy = []\n",
    "    incorrect_global_entropy = []\n",
    "\n",
    "    # Process each prediction result in the list\n",
    "    for result in results_list:\n",
    "        class_index = result.true_label_index  # Extract the class index from the result\n",
    "\n",
    "        # Initialize statistics for the class if it hasn't been done yet\n",
    "        if class_index not in class_stats:\n",
    "            class_stats[class_index] = {\n",
    "                'total': 0,  # Total predictions for this class\n",
    "                'correct': 0,  # Count of correct predictions\n",
    "                'true_in_top5': 0,  # Count of times true label was in the top 5 predictions\n",
    "                'entropies': [],  # List of entropies for all predictions\n",
    "                'correct_entropies': [],  # List of entropies for correct predictions\n",
    "                'incorrect_entropies': []  # List of entropies for incorrect predictions\n",
    "            }\n",
    "\n",
    "        # Update the total number of predictions for this class\n",
    "        class_stats[class_index]['total'] += 1\n",
    "\n",
    "        # Check if the true label is in the top 5 predictions\n",
    "        if class_index in result.top_indices:\n",
    "            class_stats[class_index]['true_in_top5'] += 1\n",
    "            global_top5 += 1  # Increment global top-5 counter\n",
    "\n",
    "        # Check if the prediction was correct (true label is the top prediction)\n",
    "        if class_index == result.top_indices[0]:\n",
    "            class_stats[class_index]['correct'] += 1\n",
    "            class_stats[class_index]['correct_entropies'].append(result.prediction_entropy)\n",
    "            correct_global_entropy.append(result.prediction_entropy)  # Add entropy to global correct list\n",
    "        else:\n",
    "            class_stats[class_index]['incorrect_entropies'].append(result.prediction_entropy)\n",
    "            incorrect_global_entropy.append(result.prediction_entropy)  # Add entropy to global incorrect list\n",
    "\n",
    "        # Record the entropy for this prediction to calculate the average later\n",
    "        class_stats[class_index]['entropies'].append(result.prediction_entropy)\n",
    "\n",
    "    # Create a dictionary to store computed insights\n",
    "    # Here we store the global insights\n",
    "    result_insights = {\n",
    "    'accuracy_per_class': {}, # Accuracy for each class\n",
    "    'top5_per_class': {}, # Top-5 accuracy for each class\n",
    "    'global_top5': global_top5 / len(results_list),  # Global top-5 accuracy\n",
    "    'avg_entropy_per_class': {}, # Average entropy for each class\n",
    "    'avg_correct_entropy_per_class': {}, # Average entropy for correct predictions per class\n",
    "    'global_avg_correct_entropy': sum(correct_global_entropy) / len(correct_global_entropy) if correct_global_entropy else 0,\n",
    "    'avg_incorrect_entropy_per_class': {}, # Average entropy for incorrect predictions per class\n",
    "    'global_avg_incorrect_entropy': sum(incorrect_global_entropy) / len(incorrect_global_entropy) if incorrect_global_entropy else 0\n",
    "}\n",
    "\n",
    "    # Here we compute and store also the insights per class into the result_insights\n",
    "    for idx, stats in class_stats.items():\n",
    "        total = stats['total']\n",
    "        correct = stats['correct']\n",
    "        top5 = stats['true_in_top5']\n",
    "        entropies = stats['entropies']\n",
    "        correct_entropies = stats['correct_entropies']\n",
    "        incorrect_entropies = stats['incorrect_entropies']\n",
    "\n",
    "        # Compute and store class-specific statistics\n",
    "        result_insights['accuracy_per_class'][idx] = correct / total if total > 0 else 0\n",
    "        result_insights['top5_per_class'][idx] = top5 / total if total > 0 else 0\n",
    "        result_insights['avg_entropy_per_class'][idx] = sum(entropies) / total if total > 0 else 0\n",
    "        if correct_entropies:\n",
    "            result_insights['avg_correct_entropy_per_class'][idx] = sum(correct_entropies) / len(correct_entropies)\n",
    "        else:\n",
    "            result_insights['avg_correct_entropy_per_class'][idx] = 0  # No correct predictions in this class\n",
    "        if incorrect_entropies:\n",
    "            result_insights['avg_incorrect_entropy_per_class'][idx] = sum(incorrect_entropies) / len(incorrect_entropies)\n",
    "        else:\n",
    "            result_insights['avg_incorrect_entropy_per_class'][idx] = 0  # No incorrect predictions in this class\n",
    "    \n",
    "    return result_insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example result_insights structure\n",
    "list_results= deserialize_results('CLIP_Base_results.json')\n",
    "result_insights = analyze_results(list_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500\n",
      "PredictionResult(image_id='0.000116_digital clock _ digital clock_0.865662.jpg', top_indices=[178, 26, 193, 35, 0], top_probabilities=[0.459217, 0.052576, 0.050009, 0.041071, 0.038435], true_label_index=178, true_label_probability=0.459217, prediction_entropy=3.9333319664001465)\n"
     ]
    }
   ],
   "source": [
    "print(len(list_results))\n",
    "print(list_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy_per_class': {178: 1.0, 31: 1.0, 36: 1.0, 125: 1.0, 0: 1.0, 8: 1.0, 30: 1.0, 7: 1.0, 152: 1.0, 27: 1.0, 82: 1.0, 147: 1.0, 162: 1.0, 141: 1.0, 34: 1.0, 74: 1.0, 169: 1.0, 90: 1.0, 26: 1.0, 166: 1.0, 92: 1.0, 135: 1.0, 28: 1.0, 21: 1.0, 171: 1.0, 163: 1.0, 132: 1.0, 110: 1.0, 111: 1.0, 11: 1.0, 4: 1.0, 54: 1.0, 1: 1.0, 2: 1.0, 46: 1.0, 136: 1.0, 23: 1.0, 86: 1.0, 3: 1.0, 66: 1.0, 19: 1.0, 192: 1.0, 107: 1.0, 80: 1.0, 61: 1.0, 186: 1.0, 24: 1.0, 153: 1.0, 124: 1.0, 81: 1.0, 67: 1.0, 47: 1.0, 161: 1.0, 112: 1.0, 129: 1.0, 72: 1.0, 37: 1.0, 140: 1.0, 35: 1.0, 164: 1.0, 44: 1.0, 168: 1.0, 64: 1.0, 14: 1.0, 69: 1.0, 5: 1.0, 20: 1.0, 154: 1.0, 184: 1.0, 144: 1.0, 84: 1.0, 65: 1.0, 114: 1.0, 42: 1.0, 199: 1.0, 113: 1.0, 56: 1.0, 172: 1.0, 71: 1.0, 45: 1.0, 51: 1.0, 98: 1.0, 118: 1.0, 158: 1.0, 6: 1.0, 133: 1.0, 193: 1.0, 173: 1.0, 95: 1.0, 85: 1.0, 87: 1.0, 13: 1.0, 177: 1.0, 32: 1.0, 22: 1.0, 78: 1.0, 101: 1.0, 16: 1.0, 63: 1.0, 180: 1.0, 49: 1.0, 116: 1.0, 9: 1.0, 55: 1.0, 106: 1.0, 70: 1.0, 52: 1.0, 10: 1.0, 179: 1.0, 73: 1.0, 17: 1.0, 58: 1.0, 29: 1.0, 48: 1.0, 145: 1.0, 18: 1.0, 59: 1.0, 25: 1.0, 12: 1.0, 57: 1.0, 189: 1.0, 109: 1.0, 182: 1.0, 60: 1.0, 40: 1.0, 76: 1.0, 131: 1.0, 93: 1.0, 50: 1.0, 102: 1.0, 155: 1.0, 198: 1.0, 183: 1.0, 119: 1.0, 99: 1.0, 160: 1.0, 143: 1.0, 150: 1.0, 15: 1.0, 165: 1.0, 94: 1.0, 139: 1.0, 123: 1.0, 97: 1.0, 103: 1.0, 53: 1.0, 121: 1.0, 196: 1.0, 188: 1.0, 146: 1.0, 151: 1.0, 130: 1.0, 33: 1.0, 176: 1.0, 187: 1.0, 148: 1.0, 68: 1.0, 39: 1.0, 62: 1.0, 88: 1.0, 194: 1.0, 185: 1.0, 190: 1.0, 137: 1.0, 126: 1.0, 138: 1.0, 122: 1.0, 96: 1.0, 174: 1.0, 149: 1.0, 115: 1.0, 41: 1.0, 38: 1.0, 79: 1.0, 100: 1.0, 105: 1.0, 108: 1.0, 43: 1.0, 175: 1.0, 75: 1.0, 128: 1.0, 167: 1.0, 195: 1.0, 156: 1.0, 127: 1.0, 170: 1.0, 120: 1.0, 142: 1.0, 181: 1.0, 157: 1.0, 104: 1.0, 83: 1.0, 77: 1.0, 89: 1.0, 117: 1.0, 91: 1.0, 134: 1.0, 159: 1.0, 191: 1.0}, 'top5_per_class': {178: 1.0, 31: 1.0, 36: 1.0, 125: 1.0, 0: 1.0, 8: 1.0, 30: 1.0, 7: 1.0, 152: 1.0, 27: 1.0, 82: 1.0, 147: 1.0, 162: 1.0, 141: 1.0, 34: 1.0, 74: 1.0, 169: 1.0, 90: 1.0, 26: 1.0, 166: 1.0, 92: 1.0, 135: 1.0, 28: 1.0, 21: 1.0, 171: 1.0, 163: 1.0, 132: 1.0, 110: 1.0, 111: 1.0, 11: 1.0, 4: 1.0, 54: 1.0, 1: 1.0, 2: 1.0, 46: 1.0, 136: 1.0, 23: 1.0, 86: 1.0, 3: 1.0, 66: 1.0, 19: 1.0, 192: 1.0, 107: 1.0, 80: 1.0, 61: 1.0, 186: 1.0, 24: 1.0, 153: 1.0, 124: 1.0, 81: 1.0, 67: 1.0, 47: 1.0, 161: 1.0, 112: 1.0, 129: 1.0, 72: 1.0, 37: 1.0, 140: 1.0, 35: 1.0, 164: 1.0, 44: 1.0, 168: 1.0, 64: 1.0, 14: 1.0, 69: 1.0, 5: 1.0, 20: 1.0, 154: 1.0, 184: 1.0, 144: 1.0, 84: 1.0, 65: 1.0, 114: 1.0, 42: 1.0, 199: 1.0, 113: 1.0, 56: 1.0, 172: 1.0, 71: 1.0, 45: 1.0, 51: 1.0, 98: 1.0, 118: 1.0, 158: 1.0, 6: 1.0, 133: 1.0, 193: 1.0, 173: 1.0, 95: 1.0, 85: 1.0, 87: 1.0, 13: 1.0, 177: 1.0, 32: 1.0, 22: 1.0, 78: 1.0, 101: 1.0, 16: 1.0, 63: 1.0, 180: 1.0, 49: 1.0, 116: 1.0, 9: 1.0, 55: 1.0, 106: 1.0, 70: 1.0, 52: 1.0, 10: 1.0, 179: 1.0, 73: 1.0, 17: 1.0, 58: 1.0, 29: 1.0, 48: 1.0, 145: 1.0, 18: 1.0, 59: 1.0, 25: 1.0, 12: 1.0, 57: 1.0, 189: 1.0, 109: 1.0, 182: 1.0, 60: 1.0, 40: 1.0, 76: 1.0, 131: 1.0, 93: 1.0, 50: 1.0, 102: 1.0, 155: 1.0, 198: 1.0, 183: 1.0, 119: 1.0, 99: 1.0, 160: 1.0, 143: 1.0, 150: 1.0, 15: 1.0, 165: 1.0, 94: 1.0, 139: 1.0, 123: 1.0, 97: 1.0, 103: 1.0, 53: 1.0, 121: 1.0, 196: 1.0, 188: 1.0, 146: 1.0, 151: 1.0, 130: 1.0, 33: 1.0, 176: 1.0, 187: 1.0, 148: 1.0, 68: 1.0, 39: 1.0, 62: 1.0, 88: 1.0, 194: 1.0, 185: 1.0, 190: 1.0, 137: 1.0, 126: 1.0, 138: 1.0, 122: 1.0, 96: 1.0, 174: 1.0, 149: 1.0, 115: 1.0, 41: 1.0, 38: 1.0, 79: 1.0, 100: 1.0, 105: 1.0, 108: 1.0, 43: 1.0, 175: 1.0, 75: 1.0, 128: 1.0, 167: 1.0, 195: 1.0, 156: 1.0, 127: 1.0, 170: 1.0, 120: 1.0, 142: 1.0, 181: 1.0, 157: 1.0, 104: 1.0, 83: 1.0, 77: 1.0, 89: 1.0, 117: 1.0, 91: 1.0, 134: 1.0, 159: 1.0, 191: 1.0}, 'global_top5': 1.0, 'avg_entropy_per_class': {178: 3.521213644567658, 31: 3.196147482842207, 36: 2.7395280857125055, 125: 3.645500086247921, 0: 2.9054761953810426, 8: 2.851901265291067, 30: 3.000034122512891, 7: 2.7584526350303573, 152: 3.8846816771170674, 27: 2.93650952898539, 82: 3.78251771365895, 147: 3.9547661779418823, 162: 3.2562281787395477, 141: 4.200889833737165, 34: 3.0548064896175937, 74: 2.6208936110619576, 169: 3.8050793409347534, 90: 4.413734933163258, 26: 2.6136729030898125, 166: 3.3813728793309283, 92: 3.91164871524362, 135: 3.726518328596906, 28: 3.645389825105667, 21: 3.2752331596087005, 171: 4.304383020401001, 163: 3.6399524384423305, 132: 3.485648729862311, 110: 3.7555361380652776, 111: 3.066551089286804, 11: 3.656692303863226, 4: 2.8325530290603638, 54: 3.432241381942362, 1: 2.454720929745705, 2: 2.691191396698719, 46: 2.3401011979726496, 136: 3.9792351284447838, 23: 2.6341720101345016, 86: 1.2308592245374854, 3: 2.6375430686042662, 66: 2.951158669111984, 19: 2.757967810915864, 192: 3.662972960716639, 107: 3.672119432971591, 80: 3.6401852204843803, 61: 3.211464626921548, 186: 3.209497022628784, 24: 3.1451215283963285, 153: 3.1943648643791676, 124: 3.027890887971108, 81: 3.717015968358263, 67: 3.1441350774518373, 47: 2.650700239633972, 161: 3.22710515991334, 112: 3.8382510731859907, 129: 4.227013475754681, 72: 2.67894434862371, 37: 3.877468629375748, 140: 3.8558714389801025, 35: 2.842851882370619, 164: 3.3202150942463624, 44: 3.000223573885466, 168: 4.801508188247681, 64: 3.2426154302514116, 14: 3.0180549841860067, 69: 3.09196720050218, 5: 2.3879635504939976, 20: 2.0546166401457144, 154: 3.445688493324049, 184: 3.626792405332838, 144: 3.5115154239264403, 84: 3.3335297572612763, 65: 2.852870828226993, 114: 3.4836140275001526, 42: 2.3610288198464193, 199: 3.5079523465212654, 113: 3.2053931172077474, 56: 3.1726305230305747, 172: 3.0398836361639425, 71: 2.5343107377227985, 45: 2.6687452701421885, 51: 3.5457018594588003, 98: 3.8923545561053534, 118: 4.096153778689248, 158: 3.504336329549551, 6: 3.3522706456103566, 133: 3.9897561344233425, 193: 3.959837230769071, 173: 2.581359123190244, 95: 3.639977909269787, 85: 3.4636897960814035, 87: 2.5785896338167644, 13: 2.8003692225153958, 177: 3.4624006872375808, 32: 2.1391274652037864, 22: 2.804467792312304, 78: 2.3735730604014615, 101: 3.5069339102820347, 16: 3.420850590519283, 63: 3.0094861717273793, 180: 3.351669112841288, 49: 3.682975607258933, 116: 3.5102232044393364, 9: 2.8739633343873487, 55: 2.971692174558456, 106: 4.185352245966594, 70: 3.2565893165526854, 52: 2.958831299940745, 10: 3.1886212757000556, 179: 2.518625372983727, 73: 1.9020358293571256, 17: 2.9412252351641657, 58: 2.936863742376629, 29: 3.5468304250389338, 48: 2.8735251993473088, 145: 2.215245462126202, 18: 3.251345191751757, 59: 3.095617243197075, 25: 2.3171029045350022, 12: 3.640248857695481, 57: 3.4994992875471347, 189: 3.474938601255417, 109: 2.9747783106106978, 182: 3.922896275153527, 60: 2.137337177164025, 40: 2.731779992580414, 76: 3.817124241276791, 131: 4.013441103607861, 93: 4.478989177628567, 50: 2.971360649381365, 102: 3.843848155994041, 155: 2.566177389136067, 198: 3.1924879683388605, 183: 3.8439643681049347, 119: 3.790355920791626, 99: 3.394692619641622, 160: 3.278409020975232, 143: 3.575064569711685, 150: 3.440928958080433, 15: 3.026715860173509, 165: 2.5730865268330825, 94: 2.8399551371733347, 139: 3.3754281252622604, 123: 3.5273269641967047, 97: 3.0787370204925537, 103: 3.2842826253988524, 53: 3.7119555244079003, 121: 1.85266047503267, 196: 3.2918751219897815, 188: 3.4348751836352878, 146: 4.349823318421841, 151: 3.431221138238907, 130: 3.2259537661075592, 33: 2.0670335421035455, 176: 2.6437296999825373, 187: 3.0572752177715303, 148: 4.037090241909027, 68: 2.435260048508644, 39: 3.172727558016777, 62: 2.781964318619834, 88: 3.7228493032784296, 194: 3.749506525695324, 185: 2.8235637971333096, 190: 3.115241934855779, 137: 3.649742705481393, 126: 4.036746937733192, 138: 2.637951036817149, 122: 2.429636395183103, 96: 3.5087774708157493, 174: 2.3456416285022144, 149: 2.620327595472336, 115: 2.2437544942778698, 41: 3.4002240796883902, 38: 3.9484544727537365, 79: 2.6261951764424643, 100: 2.477477135253139, 105: 4.12118296731602, 108: 3.694473549723625, 43: 2.2919412158105685, 175: 3.3908977111180625, 75: 2.7268559387004054, 128: 3.8905527978031724, 167: 3.8409488846858344, 195: 2.9525681060552595, 156: 2.775459978315565, 127: 2.989511491923497, 170: 3.2313944165771074, 120: 2.5548160054853986, 142: 4.64974425055764, 181: 2.8940791289011636, 157: 3.8753354867299397, 104: 2.9736748173832894, 83: 4.154961128234863, 77: 2.919936239719391, 89: 4.035316543919699, 117: 2.392778294651132, 91: 3.898803279868194, 134: 2.9629284342130027, 159: 1.3229588780086488, 191: 2.899983181107429}, 'avg_correct_entropy_per_class': {178: 3.521213644567658, 31: 3.196147482842207, 36: 2.7395280857125055, 125: 3.645500086247921, 0: 2.9054761953810426, 8: 2.851901265291067, 30: 3.000034122512891, 7: 2.7584526350303573, 152: 3.8846816771170674, 27: 2.93650952898539, 82: 3.78251771365895, 147: 3.9547661779418823, 162: 3.2562281787395477, 141: 4.200889833737165, 34: 3.0548064896175937, 74: 2.6208936110619576, 169: 3.8050793409347534, 90: 4.413734933163258, 26: 2.6136729030898125, 166: 3.3813728793309283, 92: 3.91164871524362, 135: 3.726518328596906, 28: 3.645389825105667, 21: 3.2752331596087005, 171: 4.304383020401001, 163: 3.6399524384423305, 132: 3.485648729862311, 110: 3.7555361380652776, 111: 3.066551089286804, 11: 3.656692303863226, 4: 2.8325530290603638, 54: 3.432241381942362, 1: 2.454720929745705, 2: 2.691191396698719, 46: 2.3401011979726496, 136: 3.9792351284447838, 23: 2.6341720101345016, 86: 1.2308592245374854, 3: 2.6375430686042662, 66: 2.951158669111984, 19: 2.757967810915864, 192: 3.662972960716639, 107: 3.672119432971591, 80: 3.6401852204843803, 61: 3.211464626921548, 186: 3.209497022628784, 24: 3.1451215283963285, 153: 3.1943648643791676, 124: 3.027890887971108, 81: 3.717015968358263, 67: 3.1441350774518373, 47: 2.650700239633972, 161: 3.22710515991334, 112: 3.8382510731859907, 129: 4.227013475754681, 72: 2.67894434862371, 37: 3.877468629375748, 140: 3.8558714389801025, 35: 2.842851882370619, 164: 3.3202150942463624, 44: 3.000223573885466, 168: 4.801508188247681, 64: 3.2426154302514116, 14: 3.0180549841860067, 69: 3.09196720050218, 5: 2.3879635504939976, 20: 2.0546166401457144, 154: 3.445688493324049, 184: 3.626792405332838, 144: 3.5115154239264403, 84: 3.3335297572612763, 65: 2.852870828226993, 114: 3.4836140275001526, 42: 2.3610288198464193, 199: 3.5079523465212654, 113: 3.2053931172077474, 56: 3.1726305230305747, 172: 3.0398836361639425, 71: 2.5343107377227985, 45: 2.6687452701421885, 51: 3.5457018594588003, 98: 3.8923545561053534, 118: 4.096153778689248, 158: 3.504336329549551, 6: 3.3522706456103566, 133: 3.9897561344233425, 193: 3.959837230769071, 173: 2.581359123190244, 95: 3.639977909269787, 85: 3.4636897960814035, 87: 2.5785896338167644, 13: 2.8003692225153958, 177: 3.4624006872375808, 32: 2.1391274652037864, 22: 2.804467792312304, 78: 2.3735730604014615, 101: 3.5069339102820347, 16: 3.420850590519283, 63: 3.0094861717273793, 180: 3.351669112841288, 49: 3.682975607258933, 116: 3.5102232044393364, 9: 2.8739633343873487, 55: 2.971692174558456, 106: 4.185352245966594, 70: 3.2565893165526854, 52: 2.958831299940745, 10: 3.1886212757000556, 179: 2.518625372983727, 73: 1.9020358293571256, 17: 2.9412252351641657, 58: 2.936863742376629, 29: 3.5468304250389338, 48: 2.8735251993473088, 145: 2.215245462126202, 18: 3.251345191751757, 59: 3.095617243197075, 25: 2.3171029045350022, 12: 3.640248857695481, 57: 3.4994992875471347, 189: 3.474938601255417, 109: 2.9747783106106978, 182: 3.922896275153527, 60: 2.137337177164025, 40: 2.731779992580414, 76: 3.817124241276791, 131: 4.013441103607861, 93: 4.478989177628567, 50: 2.971360649381365, 102: 3.843848155994041, 155: 2.566177389136067, 198: 3.1924879683388605, 183: 3.8439643681049347, 119: 3.790355920791626, 99: 3.394692619641622, 160: 3.278409020975232, 143: 3.575064569711685, 150: 3.440928958080433, 15: 3.026715860173509, 165: 2.5730865268330825, 94: 2.8399551371733347, 139: 3.3754281252622604, 123: 3.5273269641967047, 97: 3.0787370204925537, 103: 3.2842826253988524, 53: 3.7119555244079003, 121: 1.85266047503267, 196: 3.2918751219897815, 188: 3.4348751836352878, 146: 4.349823318421841, 151: 3.431221138238907, 130: 3.2259537661075592, 33: 2.0670335421035455, 176: 2.6437296999825373, 187: 3.0572752177715303, 148: 4.037090241909027, 68: 2.435260048508644, 39: 3.172727558016777, 62: 2.781964318619834, 88: 3.7228493032784296, 194: 3.749506525695324, 185: 2.8235637971333096, 190: 3.115241934855779, 137: 3.649742705481393, 126: 4.036746937733192, 138: 2.637951036817149, 122: 2.429636395183103, 96: 3.5087774708157493, 174: 2.3456416285022144, 149: 2.620327595472336, 115: 2.2437544942778698, 41: 3.4002240796883902, 38: 3.9484544727537365, 79: 2.6261951764424643, 100: 2.477477135253139, 105: 4.12118296731602, 108: 3.694473549723625, 43: 2.2919412158105685, 175: 3.3908977111180625, 75: 2.7268559387004054, 128: 3.8905527978031724, 167: 3.8409488846858344, 195: 2.9525681060552595, 156: 2.775459978315565, 127: 2.989511491923497, 170: 3.2313944165771074, 120: 2.5548160054853986, 142: 4.64974425055764, 181: 2.8940791289011636, 157: 3.8753354867299397, 104: 2.9736748173832894, 83: 4.154961128234863, 77: 2.919936239719391, 89: 4.035316543919699, 117: 2.392778294651132, 91: 3.898803279868194, 134: 2.9629284342130027, 159: 1.3229588780086488, 191: 2.899983181107429}, 'global_avg_correct_entropy': 3.1632017979900895, 'avg_incorrect_entropy_per_class': {178: 0, 31: 0, 36: 0, 125: 0, 0: 0, 8: 0, 30: 0, 7: 0, 152: 0, 27: 0, 82: 0, 147: 0, 162: 0, 141: 0, 34: 0, 74: 0, 169: 0, 90: 0, 26: 0, 166: 0, 92: 0, 135: 0, 28: 0, 21: 0, 171: 0, 163: 0, 132: 0, 110: 0, 111: 0, 11: 0, 4: 0, 54: 0, 1: 0, 2: 0, 46: 0, 136: 0, 23: 0, 86: 0, 3: 0, 66: 0, 19: 0, 192: 0, 107: 0, 80: 0, 61: 0, 186: 0, 24: 0, 153: 0, 124: 0, 81: 0, 67: 0, 47: 0, 161: 0, 112: 0, 129: 0, 72: 0, 37: 0, 140: 0, 35: 0, 164: 0, 44: 0, 168: 0, 64: 0, 14: 0, 69: 0, 5: 0, 20: 0, 154: 0, 184: 0, 144: 0, 84: 0, 65: 0, 114: 0, 42: 0, 199: 0, 113: 0, 56: 0, 172: 0, 71: 0, 45: 0, 51: 0, 98: 0, 118: 0, 158: 0, 6: 0, 133: 0, 193: 0, 173: 0, 95: 0, 85: 0, 87: 0, 13: 0, 177: 0, 32: 0, 22: 0, 78: 0, 101: 0, 16: 0, 63: 0, 180: 0, 49: 0, 116: 0, 9: 0, 55: 0, 106: 0, 70: 0, 52: 0, 10: 0, 179: 0, 73: 0, 17: 0, 58: 0, 29: 0, 48: 0, 145: 0, 18: 0, 59: 0, 25: 0, 12: 0, 57: 0, 189: 0, 109: 0, 182: 0, 60: 0, 40: 0, 76: 0, 131: 0, 93: 0, 50: 0, 102: 0, 155: 0, 198: 0, 183: 0, 119: 0, 99: 0, 160: 0, 143: 0, 150: 0, 15: 0, 165: 0, 94: 0, 139: 0, 123: 0, 97: 0, 103: 0, 53: 0, 121: 0, 196: 0, 188: 0, 146: 0, 151: 0, 130: 0, 33: 0, 176: 0, 187: 0, 148: 0, 68: 0, 39: 0, 62: 0, 88: 0, 194: 0, 185: 0, 190: 0, 137: 0, 126: 0, 138: 0, 122: 0, 96: 0, 174: 0, 149: 0, 115: 0, 41: 0, 38: 0, 79: 0, 100: 0, 105: 0, 108: 0, 43: 0, 175: 0, 75: 0, 128: 0, 167: 0, 195: 0, 156: 0, 127: 0, 170: 0, 120: 0, 142: 0, 181: 0, 157: 0, 104: 0, 83: 0, 77: 0, 89: 0, 117: 0, 91: 0, 134: 0, 159: 0, 191: 0}, 'global_avg_incorrect_entropy': 0}\n"
     ]
    }
   ],
   "source": [
    "print(result_insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to visualize this results we will define some other functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_insights(result_insights):\n",
    "    # Convert class-specific insights to a DataFrame for easy manipulation and plotting\n",
    "    df = pd.DataFrame({\n",
    "        'Accuracy': result_insights['accuracy_per_class'],\n",
    "        'Top-5 Accuracy': result_insights['top5_per_class'],\n",
    "        'Average Entropy': result_insights['avg_entropy_per_class'],\n",
    "        'Average Correct Entropy': result_insights['avg_correct_entropy_per_class'],\n",
    "        'Average Incorrect Entropy': result_insights['avg_incorrect_entropy_per_class']\n",
    "    })\n",
    "    \n",
    "    # Create dropdown menu for selecting the metric to plot\n",
    "    dropdown = widgets.Dropdown(\n",
    "        options=list(df.columns),\n",
    "        value='Accuracy',\n",
    "        description='Metric:',\n",
    "        disabled=False,\n",
    "    )\n",
    "    \n",
    "    # Plot function to update plots based on dropdown selection\n",
    "    def update_plot(metric):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        df[metric].sort_values().plot(kind='bar', color='skyblue')\n",
    "        plt.title(f'{metric} per Class')\n",
    "        plt.ylabel(metric)\n",
    "        plt.xlabel('Class Index')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Widget to interact with the dropdown to change plots\n",
    "    widgets.interactive(update_plot, metric=dropdown)\n",
    "\n",
    "# Example usage:\n",
    "# plot_class_insights(result_insights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_global_insights(result_insights):\n",
    "    # Display global insights\n",
    "    print(\"Global Insights:\")\n",
    "    print(f\"Global Top-5 Accuracy: {result_insights['global_top5']:.2%}\")\n",
    "    print(f\"Global Average Correct Entropy: {result_insights['global_avg_correct_entropy']}\")\n",
    "    print(f\"Global Average Incorrect Entropy: {result_insights['global_avg_incorrect_entropy']}\")\n",
    "    \n",
    "    # Display the extremes (highest and lowest values)\n",
    "    df = pd.DataFrame({\n",
    "        'Accuracy': result_insights['accuracy_per_class'],\n",
    "        'Top-5 Accuracy': result_insights['top5_per_class']\n",
    "    })\n",
    "    print(\"\\nHighest 5 Accuracies:\")\n",
    "    print(df['Accuracy'].nlargest(5))\n",
    "    print(\"\\nLowest 5 Accuracies:\")\n",
    "    print(df['Accuracy'].nsmallest(5))\n",
    "\n",
    "# Example usage:\n",
    "# display_global_insights(result_insights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .json file and compute the insights\n",
    "results_list= deserialize_results('CLIP_Base_results.json')\n",
    "result_insights = analyze_results(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display per class insights\n",
    "plot_class_insights(result_insights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Insights:\n",
      "Global Top-5 Accuracy: 100.00%\n",
      "Global Average Correct Entropy: 3.1632017979900895\n",
      "Global Average Incorrect Entropy: 0\n",
      "\n",
      "Highest 5 Accuracies:\n",
      "178    1.0\n",
      "31     1.0\n",
      "36     1.0\n",
      "125    1.0\n",
      "0      1.0\n",
      "Name: Accuracy, dtype: float64\n",
      "\n",
      "Lowest 5 Accuracies:\n",
      "178    1.0\n",
      "31     1.0\n",
      "36     1.0\n",
      "125    1.0\n",
      "0      1.0\n",
      "Name: Accuracy, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Display global insights\n",
    "display_global_insights(result_insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we integrate all the summary display functionality into a single function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def print_summary(result_insights):\n",
    "    \"\"\"Prints a comprehensive summary of all results, including histograms and numeric summaries.\"\"\"\n",
    "\n",
    "    for key, values in result_insights.items():\n",
    "        # Check if the value is a list and plot histogram\n",
    "        if isinstance(values, list):\n",
    "            plot_histogram(values, f\"Histogram of {key}\", key)\n",
    "            series = pd.Series(values)\n",
    "            display(Markdown(f\"**Top 5 values for {key}:**\"))\n",
    "            display(series.nlargest(5).to_frame(name=key))\n",
    "            display(Markdown(f\"**Bottom 5 values for {key}:**\"))\n",
    "            display(series.nsmallest(5).to_frame(name=key))\n",
    "        else:\n",
    "            # Display non-list items neatly\n",
    "            display(Markdown(f\"**{key}:** {values:.4f}\"))\n",
    "\n",
    "def plot_histogram(data, title, xlabel):\n",
    "    \"\"\"Plot a histogram for the given data.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(data, bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example data for demonstration\n",
    "result_insights = {\n",
    "    'accuracy_per_class': [0.95, 0.88, 0.90, 0.85, 0.95],\n",
    "    'top5_per_class': [1.0, 0.98, 0.97, 0.99, 1.0],\n",
    "    'global_top5': 0.99,\n",
    "    'avg_entropy_per_class': [0.1, 0.2, 0.15, 0.18, 0.1],\n",
    "    'avg_correct_entropy_per_class': [0.05, 0.07, 0.06, 0.08, 0.04],\n",
    "    'global_avg_correct_entropy': 0.06,\n",
    "    'avg_incorrect_entropy_per_class': [0.2, 0.22, 0.25, 0.21, 0.23],\n",
    "    'global_avg_incorrect_entropy': 0.22\n",
    "}\n",
    "\n",
    "# Call the function\n",
    "print_summary(result_insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLIP Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
